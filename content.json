{"pages":[{"title":"About Me","text":"https://liyuliang.cc/sitemap.xmlhttps://liyuliang.cc/baidusitemap.xml","link":"/about/index.html"},{"title":"Categories","text":"https://liyuliang.cc/sitemap.xmlhttps://liyuliang.cc/baidusitemap.xml","link":"/categories/index.html"},{"title":"donate","text":"If you think my blog is helpful to you, you can donate to my blog.","link":"/donate/index.html"},{"title":"Copyrights","text":"感谢你的阅读，本博客中所有文章由Liyuliang’s Blog 版权所有。 Thank you for reading. All posts in this blog are copyrighted by Liyuliang’s Blog. 如若转载，请注明出处：Liyuliang’s Blog If reproduced, please indicate the source: Liyuliang’s Blog 本博客采用知识共享署名-非商业性使用-相同方式共享 4.0 国际许可协议进行许可。 This blog uses Creative Commons Attribution-NonCommercial-Share-Sharing 4.0 International License Agreement to license.","link":"/copyright/index.html"},{"title":"Tags","text":"","link":"/tags/index.html"}],"posts":[{"title":"About Glide update","text":"My computer develop environment12345678910$ sw_vers ProductName: Mac OS XProductVersion: 10.12.6BuildVersion: 16G29$ go versiongo version go1.8.3 darwin/amd64$ glide -vglide version 0.12.3 Use glide to import dependence packages1$ glide get &quot;github.com/sirupsen/logrus&quot; Because the package github.com/sirupsen/logrus has imported the golang office package golang.org/x/crypto,which has been moved to github anymore When updating the project ‘s dependence packages, it always notice that the crypto package cannot been found then interrupted update The old way is to download the package below the $GOPATH directory,then running update to skip this package 12345$ mkdir -p $GOPATH/src/golang.org/x/$ git clone https://github.com/golang/crypto.git $GOPATH/src/golang.org/x/crypto$ git clone https://github.com/golang/sys.git $GOPATH/src/golang.org/x/sys$ glide up This method is quiet convenient, but each deployment of a server, the workload will increase, so there is a new way. Fork the project github.com/sirupsen/logrus to my own github, update the github repo address in my project Replace the github repo in project glide config file Update the dependency package glide up But in the process of updating ,package golang.org/x/crypto still be prompted to import in a new git warehouse In order to exclude the influence of other third party packages, so I created a new project 1234$ cd $GOPATH/src/$ mkdir test;cd test$ glide init$ glide get https://github.com/liyuliang2013/logrus.git After check the code in the new repository, I found the code does not been modify !!But git clone this new warehouse, the code is indeed modified, why is this?? Check glide’s explanation:issue 330issue 726 ok,we changed the glide configuration file 12import:- package: github.com/sirupsen/logrus to12345import:- package: github.com/sirupsen/logrus repo: https://github.com/liyuliang2013/logrus.git vcs: git version: master Then the glide will download the package basing on the repo actual address","link":"/2017/09/17/About-Glide-update/"},{"title":"About include and require in PHP","text":"My computer develop environment12$ php -vPHP 7.0.26 Difference between “include” and “require”Different ways to deal with failure: require is identical to include except upon failure it will also produce a fatal E_COMPILE_ERROR level error. In other words,it will halt the script whereas include only emits a warning (E_WARNING) which allows the script to continue. Require: It will produce a fatal level error and halt the script File is just processed only once which is be included, because the file ‘s content will be replaced into the script. Conditional include function Somebody think the ‘a.php’ will be include whether $some is true or not.123if ($come) { require 'a.php';} But i have try this is not work Include: It Just emits a warning then it will allow the script to continue. File will be read and processed each time when file is been included. Unconditional include function Difference between “include / require” and “include_once / require_once”“_once” suffix means that file will not be loaded again if it is loaded","link":"/2017/05/06/About-include-and-require-in-PHP/"},{"title":"Automatically replenish when the number of ID pools is reduced to half","text":"My computer develop environment1234567$ sw_vers ProductName: Mac OS XProductVersion: 10.12.6BuildVersion: 16G29$ go versiongo version go1.8.3 darwin/amd64 I’m using the [twitter Snowflake] (https://github.com/twitter/snowflake) to build a uuid service. I want to maintain a id pool to provide this service which can be auto filled id up when the number of id is half the reset 12345678910111213141516171819202122232425262728293031323334353637package mainimport ( \"fmt\" \"time\")var ids []intvar idsLen = 10var needFill = make(chan bool)func main() { go fillIds() for { if len(ids) &lt; idsLen/2 { needFill &lt;- true } else { needFill &lt;- false } ids = ids[1:] fmt.Printf(\"%v\\n\", ids) time.Sleep(time.Second * 2) }}func fillIds() { for { if &lt;-needFill { ids = append(ids, 1, 2, 3, 4, 5) } }}","link":"/2017/04/19/Automatically-replenish-when-the-number-of-ID-pools-is-reduced-to-half/"},{"title":"Be care of the expose ports when docker running in your server","text":"Server environment12345$ cat /etc/issue Ubuntu 14.04.5 LTS$ docker -v Docker version 1.6.2 Before using docker in ubuntu, open ports are usually be limited by iptable and it’s safe and reliable.I’m also very confident to use this tool to configure the firewall.Like Mysql,Redis,Elasticsearch, service machines are usually been deployed in internal network. So i have never consider about the problem that ports will exposed without authentication. One day after deploying a picture storage service by docker container.I ban its port in iptable list on test server machine which can be visit in external network. It looks like this: Running redis docker container as service1$ docker run --name=redis-app -d -p 6379:6379 redis:3.2.1 Docker will start a single redis container and binds its port to the host.At this time, redis can be connected anywhere. Try to ban the 6379 port by iptable12# Reject all request to the 6379 port $ sudo iptables -I INPUT -p tcp --dport 6379 -j DROP 12# Just allow local access$ sudo iptables -I INPUT -s 127.0.0.1 -p tcp --dport 6379 -j ACCEPT 12# Save the change$ sudo iptables-save You will find that iptable doesn’t work for this port !!Anyone can still connect to this redis !!Let’s check the iptable.123$ sudo iptables -L DOCKERtarget prot opt source destination ACCEPT tcp -- anywhere 172.17.0.2 tcp dpt:6379 Go ahead, How to fix itCancel the docker to modify iptables permissions1$ sudo vim /etc/default/docker Add this option at the bottom1DOCKER_OPTS=&quot;--iptables=false&quot; Restart docker1$ sudo service docker restart","link":"/2017/03/25/Be-care-of-the-expose-ports-when-docker-running-in-your-server/"},{"title":"Build Google Mirror By Nginx","text":"Server Environment1234$ cat /etc/issue Ubuntu 14.04.4 LTS$ nginx -v nginx version: nginx/1.4.6 (Ubuntu) A simple demo to build a google mirror. 1234567891011121314151617181920$ cat /etc/nginx/sites-enabled/google.conf server { listen 80; server_name _; location / { proxy_pass https://www.google.com.hk; proxy_connect_timeout 120; proxy_read_timeout 600; proxy_send_timeout 600; send_timeout 600; proxy_redirect off; proxy_set_header X-Real-IP $remote_addr; # proxy_set_header Host $host; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; }}","link":"/2017/12/15/Build-Google-Mirror-By-Nginx/"},{"title":"Build new mysql slave without stop service or locked table by Xtrabackup","text":"Server environment12345$ cat /etc/issue Ubuntu 14.04.5 LTS$ php -v PHP 5.6.32-1+ubuntu14.04.1+deb.sury.org+2 (cli) Here，I have 2 online running mysql machine, master Mysql A and slave B, they are responsible for read and write respectively.But today there is a new business with a lot of reading operate, i hope these queries that it will not affect the performance of origin mysql slave B.So i want to move this part of query operation to a new slave C and the C will sync data from A. Host HostName Father Master-A mysql-dev – Slave-B mysql-dev-slave Master-A Slave-C (new) mysql-dev-slave2 Master-A I build three docker container to mock this scene.12345$ docker psCONTAINER ID IMAGE ... PORTS NAMES83c29d41ad9a mysql:5.6 ... 127.0.0.1:3306-&gt;3306/tcp mysql-dev134ba07f70d7 mysql:5.6 ... 0.0.0.0:4306-&gt;3306/tcp mysql-dev-slave59ba368743bb mysql:5.6 ... 0.0.0.0:4307-&gt;3306/tcp mysql-dev-slave2 Install Xtrabackup In Master Machine And New Slave1$ sudo apt-get install -y Xtrabackup Backup Mysql Master Server By Xtrabackup123$ mkdir /mysql-backup$ cd /mysql-backup/$ innobackupex --user=root --password=123456 ./ 12345678910111213141516171819202122InnoDB Backup Utility v1.5.1-xtrabackup; Copyright 2003, 2009 Innobase Oyand Percona LLC and/or its affiliates 2009-2013. All Rights Reserved.This software is published underthe GNU GENERAL PUBLIC LICENSE Version 2, June 1991.170830 20:31:20 innobackupex: Connecting to MySQL server with DSN &apos;dbi:mysql:;mysql_read_default_group=xtrabackup&apos; as &apos;root&apos; (using password: YES).......xtrabackup: The latest check point (for incremental): &apos;235358683&apos;xtrabackup: Stopping log copying thread..&gt;&gt; log scanned up to (235358683)xtrabackup: Creating suspend file &apos;/mysql-backup/2017-08-30_20-31-20/xtrabackup_log_copied&apos; with pid &apos;194&apos;xtrabackup: Transaction log of lsn (235358683) to (235358683) was copied.170830 20:31:20 innobackupex: All tables unlockedinnobackupex: Backup created in directory &apos;/mysql-backup/2017-08-30_20-31-20&apos;innobackupex: MySQL binlog position: filename &apos;binlog.000003&apos;, position 120170830 20:31:20 innobackupex: Connection to database server closed170830 20:31:20 innobackupex: completed OK! We can see the backup data directory structure1234567891011$ tree -L 2 /mysql-backup/ /mysql-backup/ └── 2017-08-30_20-31-20 ├── backup-my.cnf ├── ibdata1 ├── mysql ├── performance_schema ├── symfony ├── xtrabackup_binary ├── xtrabackup_checkpoints └── xtrabackup_logfile Copy The Backup Directory From Master To New SlaveYou can use whatever tools you want to do this. Such as scp1$ scp -r /mysql-backup username@new-mysql-slave-ip:~/ I’m using docker container mock the mysql.By means of docker cp to move data directory to new slave machine, you can ignore these operation.123$ cd ~/backup/$ docker cp mysql-dev:/mysql-backup/2017-08-30_20-31-20 .$ docker cp 2017-08-30_20-31-20/ mysql-dev-slave2:/mysql-backup/ Stop New Mysql Slave Server1$ sudo service mysql stop Replace The Data Directory By Backup Directory123$ sudo rm -rf /var/lib/mysql/$ sudo mv 2017-08-30_20-33-10/ /var/lib/mysql$ sudo chown mysql.mysql -R /var/lib/mysql Start slave Mysql1$ sudo service mysql start Check Master Binlog And Pos Position At New Slave Mysql1$ mysql -h127.0.0.1 -uroop -puman73 -P4307 1234567mysql&gt; show master status;+---------------+----------+--------------+------------------+-------------------+| File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |+---------------+----------+--------------+------------------+-------------------+| binlog.000001 | 120 | | | |+---------------+----------+--------------+------------------+-------------------+1 row in set (0.00 sec) Set Synchronization From Log Point To Master At New Slave1234567891011mysql&gt; CHANGE MASTER TO \\ -&gt; MASTER_HOST=&apos;master&apos;,\\ -&gt; MASTER_PORT=3306,\\ -&gt; MASTER_USER=&apos;repl&apos;,\\ -&gt; MASTER_PASSWORD=&apos;repl&apos;,\\ -&gt; MASTER_LOG_FILE=&apos;binlog.000001&apos;,\\ -&gt; MASTER_LOG_POS=120;Query OK, 0 rows affected, 2 warnings (0.29 sec)mysql&gt; start slave;Query OK, 0 rows affected (0.03 sec) Check Sync ResultDo something in master Mysql then check the change in new slave.","link":"/2017/08/30/Build-new-mysql-slave-without-stop-service-or-locked-table-by-Xtrabackup/"},{"title":"Composer Memory Limit","text":"Server environment12345$ cat /etc/issueUbuntu 16.04.1 LTS$ php -vPHP 7.0.30-0ubuntu0.16.04.1 (cli) ( NTS ) Maybe you have been meet this problem12345...Reading /home/ubuntu/.cache/....json from cacheReading /home/ubuntu/.cache/....json from cacheReading /home/ubuntu/.cache/....json from cacheKilled Or1234...mmap() failed: [12] Cannot allocate memoryPHP Fatal error: Out of memory (allocated 762322944) (tried to allocate 20480 bytes) in phar:///usr/local/bin/composer/src/Composer/DependencyResolver/GenericRule.php on line 36Fatal error: Out of memory (allocated 762322944) (tried to allocate 20480 bytes) in phar:///usr/local/bin/composer/src/Composer/DependencyResolver/GenericRule.php on line 36 These problems are all due to the need of memory during composer running. Here are two solution. Limit PHP Memory1$ vim /etc/php/7.0/cli/php.ini 1memory_limit = 1024M memory_limit is not limited default in PHP. Increase Virtual MemoryYou can see my another blogIt will show you how to increase the virtual memory in Ubuntu","link":"/2017/11/07/Composer-Memory-Limit/"},{"title":"Cross domain access to cookie by setting P3P header","text":"Server environment12345$ cat /etc/issue Ubuntu 14.04.5 LTS $ php -v PHP 5.6.32-1+ubuntu14.04.1+deb.sury.org+2 (cli) What is P3P?The Platform for Privacy Preferences Project (P3P) enables Websites to express their privacy practices in a standard format that can be retrieved automatically and interpreted easily by user agents. P3P user agents will allow users to be informed of site practices (in both machine- and human-readable formats) and to automate decision-making based on these practices when appropriate. Thus users need not read the privacy policies at every site they visit. I think the P3P is a protocol that allows websiteA to tell the browser that it can receive request from websiteB and set websiteB ‘s cookies. Browser supports BROWSER IE FIREFOX CHROME SAFARI OPERA Cookie Limit Yes No No Yes No 12345$ tree /www/www├── a_getcookie.php├── a_setcookie.php└── b_setcookie.php a_setcookie.php1234&lt;?php header(&apos;P3P: CP=&quot;CURa ADMa DEVa PSAo PSDo OUR BUS UNI PUR INT DEM STA PRE COM NAV OTC NOI DSP COR&quot;&apos;); setcookie(&quot;test&quot;, $_GET[&apos;id&apos;], time()+3600, &quot;/&quot;, &quot;.a.com&quot;); a_getcookie.php12&lt;?php var_dump($_COOKIE); b_setcookie.php1&lt;script src=&quot;http://www.a.com/a_setcookie.php?id=www.b.com&quot;&gt;&lt;/script&gt; Access the follow urls in order:12http://www.b.com/b_setcookie.phphttp://www.a.com/a_getcookie.php How to use in passport1234567891011function login($username,$passwrod){ // Check account is available // ... header('P3P: CP=\"CURa ADMa DEVa PSAo PSDo OUR BUS UNI PUR INT DEM STA PRE COM NAV OTC NOI DSP COR\"'); session_start(); $sessionId = session_id(); setcookie(\"PHPSESSID\", $sessionId, time()+3600, \"/\", \".a.com\"); // Check your php.ini whether the session key is 'PHPSESSID' } What P3P code meanshttps://www.w3.org/P3P/ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061P3P Header is present:CP=&quot;CURa ADMa DEVa PSAo PSDo OUR BUS UNI PUR INT DEM STA PRE COM NAV OTC NOI DSP COR&quot;Compact Policy token is present. A trailing &apos;o&apos; means opt-out, a trailing &apos;i&apos; means opt-in.CURaInformation is used to complete the activity for which it was provided.ADMaInformation may be used for the technical support of the Web site and its computer system.DEVaInformation may be used to enhance, evaluate, or otherwise review the site, service, product, or market.PSAoInformation may be used to create or build a record of a particular individual or computer that is tied to a pseudonymous identifier, without tying identified data (such as name, address, phone number, or email address) to the record. This profile will be used to determine the habits, interests, or other characteristics of individuals for purpose of research, analysis and reporting, but it will not be used to attempt to identify specific individuals. PSDoInformation may be used to create or build a record of a particular individual or computer that is tied to a pseudonymous identifier, without tying identified data (such as name, address, phone number, or email address) to the record. This profile will be used to determine the habits, interests, or other characteristics of individuals to make a decision that directly affects that individual, but it will not be used to attempt to identify specific individuals.OURWe share information with ourselves and/or entities acting as our agents or entities for whom we are acting as an agent.BUSInfo is retained under a service provider&apos;s stated business practices. Sites MUST have a retention policy that establishes a destruction time table. The retention policy MUST be included in or linked from the site&apos;s human-readable privacy policy.UNINon-financial identifiers, excluding government-issued identifiers, issued for purposes of consistently identifying or recognizing the individual. These include identifiers issued by a Web site or service.PURInformation actively generated by the purchase of a product or service, including information about the method of payment.INTData actively generated from or reflecting explicit interactions with a service provider through its site -- such as queries to a search engine, or logs of account activity.DEMData about an individual&apos;s characteristics -- such as gender, age, and income.STAMechanisms for maintaining a stateful session with a user or automatically recognizing users who have visited a particular site or accessed particular content previously -- such as HTTP cookies.PREData about an individual&apos;s likes and dislikes -- such as favorite color or musical tastes.COMInformation about the computer system that the individual is using to access the network -- such as the IP number, domain name, browser type or operating system.NAVData passively generated by browsing the Web site -- such as which pages are visited, and how long users stay on each page.OTCOther types of data not captured by the above definitions.NOIWeb Site does not collected identified data.DSPThe privacy policy contains DISPUTES elements.CORErrors or wrongful actions arising in connection with the privacy policy will be remedied by the service.","link":"/2017/05/12/Cross-domain-access-to-cookie-by-setting-P3P-header/"},{"title":"Different Between MyISAM And InnoDB In MySQL","text":"Server environment12345$ cat /etc/issue Ubuntu 14.04.5 LTS$ mysqld -Vmysqld Ver 5.5.58-0ubuntu0.14.04.1 for debian-linux-gnu on x86_64 ((Ubuntu)) Diff MyISAM InnoDB Default False True Storage Structure Each table will be saved in three file. frm: Defined TableMYD: Data fileMYI: Index File All table will be saved in the same data file(Maybe in multi files).The table size is limited by system file size.It is 2GB generally. Storage Space Can be compressed. The space size is less than InnoDB. For high speed caching data and indexes, Tables in InnoDB need more memory and space to build its dedicated buffer pool in main memory. Transaction Safe Not Support Support. Safe Transaction Table includes commit 、rollback and crash recovery methods. COUNT without WHERE BetterMyISAM has saved the table row number. It need to scan all rows to count Lock Only Table Lock Table Lock and Row Lock (The row lock is only work that using primary key in the where condition , otherwise the table will be locked until the write operation finished) Foreign Key Not Support Support FULLTEXT Index Support Not Support (But you can use Sphinx to do this) In many cases, Lots of select operation in MyISAM is a better choise. Because InnoDB will cache data block but the MyISAM only cache data index block which can reduce the number of times that data swapped in and out of memory InnoDB positioning addressing must mapping to block then to row. MyISAM records the file offset then direct the position so it will faster than InnoDB. InnoDB also needs to maintain MVCC(Multi Version Concurrency Control) consistency. Although you haven’t use it, but it still needs to check and maintain.","link":"/2017/05/08/Different-Between-MyISAM-And-InnoDB-In-MySQL/"},{"title":"Fail2ban Pass Mysql Remote Login through","text":"Server environment12345$ cat /etc/issue Ubuntu 14.04.5 LTS$ fail2ban-server -VFail2Ban v0.8.11 Recently, I found that there are many ssh attack in the auth.log. 12345678910$ cat /var/log/auth.log | grep ssh2...Jan 1 01:20:49 app sshd[22438]: Failed password for root from 59.55.140.173 port 16421 ssh2Jan 1 01:27:22 app sshd[22440]: Failed password for invalid user snort from 91.121.112.123 port 33773 ssh2Jan 1 01:58:52 app sshd[22459]: Failed password for invalid user admin from 196.219.96.116 port 53758 ssh2Jan 1 01:59:01 app sshd[22461]: Failed password for invalid user admin from 14.170.244.58 port 42785 ssh2Jan 1 02:05:25 app sshd[22466]: Failed password for invalid user vmail from 5.135.161.94 port 60876 ssh2Jan 1 02:08:04 app sshd[22468]: Failed password for invalid user test9 from 5.135.161.94 port 55094 ssh2Jan 1 02:10:52 app sshd[22483]: Failed password for invalid user oracle from 5.135.161.94 port 49413 ssh2 So I decided to use fail2ban with iptables to limit this part of ip address to access. Check The Iptables12345678910$ sudo iptables -nvLChain INPUT (policy ACCEPT 1850K packets, 353M bytes) pkts bytes target prot opt in out source destination Chain FORWARD (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination Chain OUTPUT (policy ACCEPT 1786K packets, 571M bytes) pkts bytes target prot opt in out source destination Add Iptables Rules1$ sudo vim /etc/iptables.my.rules 1234567891011121314151617181920212223242526272829303132333435363738394041*filter# Allows all loopback (lo0) traffic and drop all traffic to 127/8 that doesn&apos;t use lo0-A INPUT -i lo -j ACCEPT-A INPUT ! -i lo -d 127.0.0.0/8 -j REJECT# Accepts all established inbound connections-A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT# Allows all outbound traffic# You could modify this to only allow certain traffic-A OUTPUT -j ACCEPT# Allows HTTP and HTTPS connections from anywhere (the normal ports for websites)-A INPUT -p tcp --dport 80 -j ACCEPT-A INPUT -p tcp --dport 443 -j ACCEPT# Allows SSH connections # The --dport number is the same as in /etc/ssh/sshd_config-A INPUT -p tcp -m state --state NEW --dport 22 -j ACCEPT# Allows Mysql connections-A INPUT -p tcp --dport 3306 -j ACCEPT# Now you should read up on iptables rules and consider whether ssh access # for everyone is really desired. Most likely you will only allow access from certain IPs.# Allow ping# note that blocking other types of icmp packets is considered a bad idea by some# remove -m icmp --icmp-type 8 from this line to allow all kinds of icmp:# https://security.stackexchange.com/questions/22711-A INPUT -p icmp -m icmp --icmp-type 8 -j ACCEPT# log iptables denied calls (access via &apos;dmesg&apos; command)-A INPUT -m limit --limit 5/min -j LOG --log-prefix &quot;iptables denied: &quot; --log-level 7# Reject all other inbound - default deny unless explicitly allowed policy:-A INPUT -j REJECT-A FORWARD -j REJECTCOMMIT You can follow this example to add other applications port access like this Mysql(3306) port. Enable This Rules1$ sudo iptables-restore &lt; /etc/iptables.my.rules Save This Rules1$ sudo iptables-save &gt; /etc/iptables.up.rules Auto Start After Boot1$ sudo vim /etc/network/if-pre-up.d/iptables 12#!/bin/sh/sbin/iptables-restore &lt; /etc/iptables.up.rules Add file execution permission1$ sudo chmod +x /etc/network/if-pre-up.d/iptables Install Fail2ban12$ sudo apt-get update$ sudo apt-get install -y fail2ban Copy A Jail Config file12$ cd /etc/fail2ban $ cp jail.conf jail.local Change This Configuration12345678910111213141516171819[DEFAULT]### Continuous access in 600 secondsfindtime = 600### Entry wrong password 2 times.maxretry = 2### Block access for 1 days.bantime = 36000000backend = polling...# Choose default action. To change, just override value of &apos;action&apos; with the# interpolation to the chosen action shortcut (e.g. action_mw, action_mwl, etc) in jail.local# globally (section [DEFAULT]) or per specific section#action = %(action_)saction = iptables[name=SSH, port=ssh, protocol=tcp] Start Fail2ban1$ sudo service fail2ban restart Check the log print123456789101112$ sudo tail -f /var/log/fail2ban.log2018-01-01 04:44:39,853 fail2ban.jail : INFO Creating new jail &apos;ssh&apos;2018-01-01 04:44:39,853 fail2ban.jail : INFO Jail &apos;ssh&apos; uses poller2018-01-01 04:44:39,868 fail2ban.jail : INFO Initiated &apos;polling&apos; backend2018-01-01 04:44:39,869 fail2ban.filter : INFO Added logfile = /var/log/auth.log2018-01-01 04:44:39,870 fail2ban.filter : INFO Set maxRetry = 62018-01-01 04:44:39,871 fail2ban.filter : INFO Set findtime = 6002018-01-01 04:44:39,871 fail2ban.actions: INFO Set banTime = 360000002018-01-01 04:44:39,919 fail2ban.jail : INFO Jail &apos;ssh&apos; started2018-01-01 04:45:10,981 fail2ban.actions: WARNING [ssh] Ban 118.212.143.432018-01-01 04:52:52,562 fail2ban.actions: WARNING [ssh] Ban 220.191.194.22 Check Result In iptables12345678910111213141516171819202122232425262728$ sudo iptables -nvLChain INPUT (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination 8468 549K fail2ban-SSH tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpt:22 0 0 ACCEPT all -- lo * 0.0.0.0/0 0.0.0.0/0 0 0 REJECT all -- !lo * 0.0.0.0/0 127.0.0.0/8 reject-with icmp-port-unreachable31873 13M ACCEPT all -- * * 0.0.0.0/0 0.0.0.0/0 state RELATED,ESTABLISHED 3 140 ACCEPT tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpt:80 498 33053 ACCEPT tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpt:443 13 764 ACCEPT tcp -- * * 0.0.0.0/0 0.0.0.0/0 state NEW tcp dpt:22 29 1740 ACCEPT tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpt:3306 1 84 ACCEPT icmp -- * * 0.0.0.0/0 0.0.0.0/0 icmptype 8 376 114K LOG all -- * * 0.0.0.0/0 0.0.0.0/0 limit: avg 5/min burst 5 LOG flags 0 level 7 prefix &quot;iptables denied: &quot; 1813 442K REJECT all -- * * 0.0.0.0/0 0.0.0.0/0 reject-with icmp-port-unreachableChain FORWARD (policy DROP 0 packets, 0 bytes) pkts bytes target prot opt in out source destination 0 0 REJECT all -- * * 0.0.0.0/0 0.0.0.0/0 reject-with icmp-port-unreachableChain OUTPUT (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination 31767 20M ACCEPT all -- * * 0.0.0.0/0 0.0.0.0/0 Chain fail2ban-SSH (1 references) pkts bytes target prot opt in out source destination 10 988 REJECT all -- * * 220.191.194.22 0.0.0.0/0 reject-with icmp-port-unreachable 369 22140 REJECT all -- * * 118.212.143.43 0.0.0.0/0 reject-with icmp-port-unreachable 8089 525K RETURN all -- * * 0.0.0.0/0 0.0.0.0/0 You can see the part Chain fail2ban-SSH (1 references), the fail2ban is working and filter 2 ip attack by iptables.","link":"/2018/01/01/Fail2ban-Pass-Mysql-Remote-Login-through/"},{"title":"Get real client IP from Nginx to Apache","text":"Server environment12345678$ cat /etc/issue Ubuntu 14.04.5 LTS$ nginx -v nginx version: nginx/1.4.6 (Ubuntu)$ apachectl -v Server version: Apache/2.4.7 (Ubuntu) Server IP Web Client IP 192.168.33.10 192.168.33.11 Web Client -&gt; Nginx -&gt; Apache -&gt; PHP WebsiteWhen i use nginx to proxy to apache, apache gets the ip address of nginx proxy as the client.Through the apache log files, the PHP website running on the apache backend will all receive the same IP address ( 127.0.0.1, if apache and nginx are running in the same server). Apache website configurationMy Apache listens 8013 port and accesses directory /www 123456789101112131415$ cat /etc/apache2/sites-enabled/site.conf &lt;VirtualHost *:8013&gt; ServerName www.a.com ServerAlias *.a.com DocumentRoot /www &lt;Directory /www&gt; Options Indexes FollowSymLinks MultiViews AllowOverride all Require all granted &lt;/Directory&gt; ErrorLog ${APACHE_LOG_DIR}/error.log CustomLog ${APACHE_LOG_DIR}/access.log combined&lt;/VirtualHost&gt; Nginx proxy configurationMy nginx listens 80 port and passes all request to 127.0.0.1:8013 (Apache website) if request host is “www.a.com&quot; 123456789101112$ cat /etc/nginx/sites-enabled/a.com.confserver { server_name www.a.com listen 80; client_max_body_size 100M; location / { proxy_pass http://127.0.0.1:8013; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; }} Very useful, but…When i want to collect client IPs, it got all “127.0.0.1” in PHP script Luckily, nginx provides a HTTP X-Forward-For header containing the client real ip address to allow Apache to recognize the origin client IP. Install Apache rpaf module1$ sudo apt-get install -y libapache2-mod-rpaf Check Apache rpaf module12345678910111213141516$ cat /etc/apache2/mods-available/rpaf.conf &lt;IfModule rpaf_module&gt; RPAFenable On # When enabled, take the incoming X-Host header and # update the virtualhost settings accordingly: RPAFsethostname On # Define which IP&apos;s are your frontend proxies that sends # the correct X-Forwarded-For headers: RPAFproxy_ips 127.0.0.1 ::1 # Change the header name to parse from the default # X-Forwarded-For to something of your choice:# RPAFheader X-Real-IP&lt;/IfModule&gt; Enable Apache rpaf module1$ sudo a2enmod rpaf Add apache website rpaf module support1234RPAFenable OnRPAFsethostname OnRPAFproxy_ips 127.0.0.1 xx.xx.xx.xxRPAFheader X-Real-IP xx.xx.xx.xx is your proxy server ip address(es), multiple addresses can be space separated. It will tell mod_rpaf which host to get X-real-IP headers from. The full apache website configuration:1234567891011121314151617181920$ cat /etc/apache2/sites-enabled/site3.conf &lt;VirtualHost *:8013&gt; ServerName www.a.com ServerAlias *.a.com DocumentRoot /www &lt;Directory /www&gt; Options Indexes FollowSymLinks MultiViews AllowOverride all Require all granted &lt;/Directory&gt; RPAFenable On RPAFsethostname On RPAFproxy_ips 127.0.0.1 192.168.33.10 RPAFheader X-Real-IP ErrorLog ${APACHE_LOG_DIR}/error.log CustomLog ${APACHE_LOG_DIR}/access.log combined&lt;/VirtualHost&gt; Restart Apache1$ sudo apachectl restart Check Apache running access log to know the real access ipAfter Web Client access to server123456$ sudo tail -f /var/log/apache2/access.log......127.0.0.1 - - [18/May/2017:11:41:08 +0000] &quot;GET / HTTP/1.0&quot; 200 265 &quot;-&quot; &quot;curl/7.35.0&quot;192.168.33.11 - - [18/May/2017:12:46:12 +0000] &quot;GET / HTTP/1.0&quot; 200 265 &quot;-&quot; &quot;curl/7.35.0&quot;...","link":"/2017/05/18/Get-real-client-IP-from-Nginx-to-Apache/"},{"title":"Golang meet 'float64bits redeclared in this block'","text":"My computer develop environment12345$ cat /etc/issueUbuntu 14.04 LTS$ go versiongo version go1.8 linux/amd64 I have meet this error usr/local/go/src/runtime/float.go:45: float64bits redeclared in this blockafter update golang version from 1.8 to 1.9 Don’t worry, delete the go directory and re-install golang, problem will be fixed.1$ sudo rm -rf /usr/local/go 1$ curl -s https://storage.googleapis.com/golang/go${GO_VERSION}.linux-amd64.tar.gz | tar -v -C /usr/local -xz","link":"/2018/02/22/Golang-meet-float64bits-redeclared-in-this-block/"},{"title":"How Http Server Add Http Basic Auth In Golang","text":"My computer develop environment1234567$ sw_vers ProductName: Mac OS XProductVersion: 10.12.6BuildVersion: 16G29$ go versiongo version go1.8.3 darwin/amd64 How to add bash auth to filter the request in web server12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061package mainimport ( \"bytes\" \"io\" \"net/http\" \"strings\" \"encoding/base64\")type requestFunc func(http.ResponseWriter, *http.Request)func BasicAuth(execFunc requestFunc, username, password []byte) requestFunc { return func(w http.ResponseWriter, r *http.Request) { basicAuthPrefix := \"Basic \" // get the request header auth := r.Header.Get(\"Authorization\") // if request with http basic auth if strings.HasPrefix(auth, basicAuthPrefix) { // decode it payload, err := base64.StdEncoding.DecodeString( auth[len(basicAuthPrefix):], ) if err == nil { pair := bytes.SplitN(payload, []byte(\":\"), 2) if len(pair) == 2 &amp;&amp; bytes.Equal(pair[0], username) &amp;&amp; bytes.Equal(pair[1], password) { execFunc(w, r) return } } } // if auth failed, restricted other value w.Header().Set(\"WWW-Authenticate\", `Basic realm=\"Restricted\"`) // return 401 status code w.WriteHeader(http.StatusUnauthorized) }}func secretRequest(w http.ResponseWriter, r *http.Request) { io.WriteString(w, \"You got 1 million!!!\\n\")}func main() { username := []byte(\"admin\") password := []byte(\"password\") http.HandleFunc(\"/\", BasicAuth(secretRequest, username, password)) http.ListenAndServe(\":8000\", nil)}","link":"/2017/09/14/How-Http-Server-Add-Http-Basic-Auth-In-Golang/"},{"title":"How Nginx Add Basic Auth","text":"Server Environment1234$ cat /etc/issue Ubuntu 14.04.4 LTS$ nginx -v nginx version: nginx/1.10.3 (Ubuntu) Install Nginx1$ sudo apt-get install -y apache2-utils nginx apache2-utils is a tool that generate a password file Create A Nginx User Named “liyuliang”12$ sudo htpasswd -c /etc/nginx/.htpasswd liyuliangNew password: Entry your password after this command Add Auth Type In Your Nginx Config1$ sudo vim /etc/nginx/sites-enabled/default 12345678910location / { # First attempt to serve request as file, then # as directory, then fall back to displaying a 404. try_files $uri $uri/ =404; # Uncomment to enable naxsi on this location # include /etc/nginx/naxsi.rules auth_basic &quot;Private Property&quot;; auth_basic_user_file /etc/nginx/.htpasswd;} Restart Nginx1$ sudo service nginx restart Proxy Pass With Auth123456789101112131415server { server_name _ listen 80; client_max_body_size 100M; location / { proxy_pass http://127.0.0.1:3030; proxy_set_header Host $host:80; proxy_set_header X-Real-IP $remote_addr; auth_basic &quot;Private Property&quot;; auth_basic_user_file /etc/nginx/.htpasswd; }} It must be passed to proxy before checking authorization","link":"/2017/09/13/How-Nginx-Add-Basic-Auth/"},{"title":"How Symfony create entity class from mysql database","text":"My computer develop environment12345678910$ cat composer.json ... &quot;require&quot;: { &quot;php&quot;: &quot;&gt;=5.5.9&quot;, &quot;symfony/symfony&quot;: &quot;2.8.*&quot;, ... }, $ mysqld -V mysqld Ver 5.7.18 for Linux on x86_64 (MySQL Community Server (GPL)) In some case, you will meet a secondary development project based on mysql.It has a complete database and your new project bases on Symfony framework.As you know, if there are entities mapping class will speed up your development. So this is why this tutorial exist. First of all: this converter doesn’t support some basic MySQL data types by default, so you have to edit your app/config/config.yml file as follows: Update Doctrine Configuration12345doctrine: dbal: mapping_types: enum: string bit: integer You might need to add some other types, too.As Doctrine does not support any of these: BIT, BINARY, VARBINARY, TINYBLOB, MEDIUMBLOB, BLOB, LONGBLOB, ENUM, SET, GEOMETRY, POINT, MULTIPOINT, LINESTRING, MULTILINESTRING, POLYGON, MULTIPOLYGON. Make sure all tables have primary keysOK, that might sound obvious.But I real had some tables without the key. Generate the mapping file1$ php app/console doctrine:mapping:import --force AppBundle xml -vvv Generate Entities under directory ‘src/AppBundle/Entity’1$ php app/console doctrine:mapping:convert annotation ./src Generate the Get and Set method in entity1$ php app/console generate:doctrine:entities AppBundle Ok, entities will all be done!","link":"/2018/01/25/How-Symfony-create-entity-class-from-mysql-database/"},{"title":"How PHP Connect to Mysql","text":"My computer develop environment12$ php -vPHP 7.0.26 Features PHP mysql extension PHP mysqli extension PDO PHP Support Version Before 3.0 5.0 5.0 Include In PHP5.x Yes Yes Yes Development Status Maintenance Only Active Active in PHP5.3 Recommended Usage in New MySQL Projects Not Recommended Recommendations - Preferred Suggestions API Character Set Support No Yes Yes Prepare statement support in Server No Yes Yes Prepare statement support in Client No No Yes Stored Procedure Support No Yes Yes Multi-State Execution Support No Yes Most Does it support all MySQL 4.1+ features No Yes Most PHP Mysql extension Just MySQL database Most primitive mysql database extension of PHP Mysql 4.1.3- Procedure Oriented 123456789101112&lt;?phpheader('Content-Type: text/html; charset=utf-8');$username = \"liyuliang\";$password = \"123456\";$database = \"testDB\";$conn = @mysql_connect(\"localhost:3306\",$username,$password) or die(\"connect mysql failed\");mysql_select_db($database,$conn);echo \"database connect success\"; PHP mysqli extension Just MySQL database Mysql 4.1.3+ “i” means “Improvement” (Security Increased) Pre-processing statements support (Prevent SQL injection) Procedure Oriented + Object Oriented It will open a new connection process each time when running PHP mysql extension but Mysqli extension will use the same connection. Because mysqli uses a persistent connection function and mysql is not 12345678910111213141516&lt;?phpheader('Content-Type: text/html; charset=utf-8');$username = \"liyuliang\";$password = \"123456\";$database = \"testDB\";$conn = mysqli_connect(\"localhost\", $username, $password, $database, \"3306\");if (!$conn) { die(\"connect mysql failed\" . mysqli_connect_error());}else{ echo \"database connect success\";} PHP Data Object(PDO) PDO can be used in 12 different database Database abstract standard Pre-processing statements support (Prevent SQL injection) Object Oriented 12345678910111213141516&lt;?phpheader('Content-Type: text/html; charset=utf-8');$username = \"liyuliang\";$password = \"123456\";$database = \"testDB\";try { $conn = new PDO(\"mysql:host={$username};dbname={$database}\", $username, $password); echo \"database connect success\";} catch (PDOException $e) { echo $e-&gt;getMessage();}","link":"/2017/05/04/How-PHP-Connect-to-Mysql/"},{"title":"How To Change Tmux Prefix Keymap In Iterm2","text":"My computer develop environment123456789$ sw_vers ProductName: Mac OS XProductVersion: 10.12.3$ tmux -Vtmux 2.8$ Iterm2Build 3.2.5 Default Prefix Key is Ctrl + B. I think it is too hard to press this combination button before next command.Can i change the prefix key ?According to the official document, the solution is inseparable from the Ctrl button. It means that Ctrl must be one of the prefix key. But fortunately under mac, if you are using Iterm instead of system terminal, you can customize the key combination to mapping tmux ‘s prefix through by Hex Code Configuration In Item2: Preferences -&gt; Profiles -&gt; Keys Add A new Key MappingThe Action choose Send Hex CodeAfter recording the keyboard shortcut. You can set the mapping shortcut by hex code.The key Ctrl b ‘s hex code is 0x02.The next things becomes simpler.Let me give an example. If you want to achieve the effect to mapping Ctrl B + w, you can entry the hex code 0x02 0x77 Here i have search a example picture, you can compare it to get more information to build your own unique shortcut! More Hex CodeYou can check this ASCII Mapping Keyboard Table to look for the hex code which you need.","link":"/2018/04/03/How-To-Change-Tmux-Prefix-Keymap-In-Iterm2/"},{"title":"How To Sort Map In Golang","text":"My computer develop environment1234567$ sw_vers ProductName: Mac OS XProductVersion: 10.12.6BuildVersion: 16G29$ go versiongo version go1.8.3 darwin/amd64 Introduction Golang mapA Map in Go is a collection of unordered key-value pairs.The most important point of Map is to quickly retrieve data by key. The key is similar to the index and points to the value of the data.Maps are a collection, so we can iterate over it like iterating over arrays and slices. However, Map is unordered, and we can’t determine its return order, because Map is implemented using a chained hash table. So, when the map is traversed by range, the order of the keys is randomized. 123456789101112131415161718package main import ( \"fmt\") func main() { m := make(map[string]string) m[\"error\"] = \"xxxxxx\" m[\"code\"] = \"xxxxxx\" m[\"message\"] = \"xxxxxx\" m[\"is\"] = \"xxxxxx\" m[\"list\"] = \"xxxxxx\" for k, v := range m { fmt.Printf(\"k=%v, v=%v\\n\", k, v) }} It can be clearly seen that the order of the keys is different each time it is traversed.In Golang office document Go maps in action, you can find the reason. When iterating over a map with a range loop, the iteration order is not specified and is not guaranteed to be the same from one iteration to the next. Since Go 1 the runtime randomizes map iteration order, as programmers relied on the stable iteration order of the previous implementation. How to sort a Map?1234567891011121314151617181920212223242526package sortimport \"sort\"type pair struct { Key string Val string}func SortMap(m map[string]string) (pairs []pair) { keys := []string{} for k, _ := range m { keys = append(keys, k) } sort.Strings(keys) for _, k := range keys { v := m[k] if k != \"\" &amp;&amp; v != \"\" { pairs = append(pairs, pair{ Key: k, Val: v, }) } } return pairs} Slice is ordered, using this feature, create a slice of the struct type for storage.These slices will iteratively output the key-value pairs in the Map in order.","link":"/2017/07/15/How-To-Sort-Map-In-Golang/"},{"title":"How a variadic parameter pass through multi function in golang","text":"My computer develop environment1234567$ sw_vers ProductName: Mac OS XProductVersion: 10.12.6BuildVersion: 16G29$ go versiongo version go1.8.3 darwin/amd64 Why Variadic ParameterFunction accept variadic numbers of arguments or parameters that it can take a list of arguments and call the function only once, whatever string or []string you pass Simple Use123456789101112131415161718192021222324252627package mainimport \"fmt\"// Here's a function that will take an arbitrary number// of `ints` as arguments.func sum(nums ...int) { total := 0 for _, num := range nums { total += num } fmt.Println(total)}func main() { // Variadic functions can be called in the usual way // with individual arguments. sum(1, 2) sum(1, 2, 3) // If you already have multiple args in a slice, // apply them to a variadic function using // `func(slice...)` like this. nums := []int{1, 2, 3, 4} sum(nums...)} It’s not difficult, right? Ok let’s see what i meet Pass Variadic Arguments through mutli functions1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package maintype User interface { Name() string Age() string}func primaryStudents() (users []User) { //... return users}func studentToDB() (err error) { users := primaryStudents() if len(users) &gt; 0 { err = saveToDB(users) } return err}func saveToDB(datas ...interface{}) (err error) { err = Mysql().Save(datas...) //... return err}type mysqlDB struct {}func Mysql() *mysqlDB { db := new(mysqlDB) return db}func (db *mysqlDB) Save(models ...interface{}) (err error) { if len(models) &gt; 1 { // } else { // !!! models is a slice, but models[0] is a slice too! err = db.Create(models[0]) } return err} Get a Interface type slice named Users Pass Users to method saveToDB() Pass Users to method Mysql().Save() again in method saveToDB() Get the first element method Mysql().Save() But models[0] got a slice at line 44. When User slice was been passed to method saveToDB as the first argument in datas, so datas in method saveToDB look like actually1234567datas = [ 0 =&gt; Users, 1 =&gt; null, 2 =&gt; null, ...] Even force the datas to “Mysql().Save(datas…)” once again doesn’t work Solution:12//lint 17err = saveToDB(users) change to12//lint 17err = saveToDB(users...)","link":"/2017/04/07/How-a-variadic-parameter-pass-through-multi-function-in-golang/"},{"title":"How To Storage Session By Redis In Symfony2","text":"Server Environment123456789101112$ cat /etc/issueUbuntu 16.04.2 LTS \\n \\l$ redis-server --versionRedis server v=3.0.6 sha=00000000:0 malloc=jemalloc-3.6.0 bits=64$ cat composer.json&quot;require&quot;: { &quot;php&quot;: &quot;&gt;=5.5.9&quot;, &quot;symfony/symfony&quot;: &quot;2.8.*&quot;, ... }, Using Redis instead of the default PHP file storage sessionFor the first preparation, we need to have a running Redis service. Then, installing php redis extension. This tutorial bases on the Ubuntu:16.04 system, so i will use command apt install to do this.1$ sudo apt-get install -y php-redis Next, find the config.yml in your Symfony project and register the session handler which named session_handler_redis. 1$ vim app/config/config.yml 123456framework: ... session: # handler_id set to null will use default session handler from php.ini# handler_id: ~ handler_id: session_handler_redis This configuration uses a new session handler to figure out the session service. Now we can declare a Symfony Service in services.yml. 1$ vim app/config/services.yml 123456services: ... session_handler_redis: class: AppBundle\\SessionHandler\\NativeRedisSessionHandler arguments: [\"%session_handler_redis_scheme%\",\"%session_handler_redis_host%\",\"%session_handler_redis_port%\",\"%session_handler_redis_auth%\",\"%session_guest_life_seconds%\"] How can we manage the session in these new handler AppBundle\\SessionHandler\\NativeRedisSessionHandler ?There are a lot of tools on github that can do this, but I found that there is already such code in the Symfony source to help us do this. So declare a new file named NativeRedisSessionHandler in your project:1$ vim src/AppBundle/SessionHandler/NativeRedisSessionHandler.php 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152&lt;?php/** * Created by PhpStorm. * User: liang * Date: 18年01月17日 * Time: 下午12:07 */namespace AppBundle\\SessionHandler;use \\Symfony\\Component\\HttpFoundation\\Session\\Storage\\Handler\\NativeSessionHandler;/** * NativeRedisSessionStorage. * * Driver for the redis session &lt;div id=\"R77EHMs\" style=\"position: absolute; top: -1183px; left: -1358px; width: 243px;\"&gt;&lt;/div&gt; save hadlers provided by the redis PHP extension. * * @see https://github.com/nicolasff/phpredis * * @author Andrej Hudec &amp;lt;pulzarraider@gmail.com&amp;gt; * @author Piotr Pelczar &amp;lt;me@athlan.pl&amp;gt; */class NativeRedisSessionHandler extends NativeSessionHandler{ /** * Constructor. * * @param string $host Path of redis server. * @param string $scheme * @param int $port * @param null $auth * @param int $saveTime */ public function __construct($scheme, $host, $port = 6379, $auth = null, $saveTime = 1440) { if (!extension_loaded('redis')) { throw new \\RuntimeException('PHP does not have \"redis\" session module registered'); } if ($host) { $savePath = \"{$scheme}://{$host}:{$port}\"; if ($auth){ $savePath = \"{$savePath}?auth={$auth}\"; } ini_set('session.save_handler', 'redis'); ini_set('session.save_path', $savePath); ini_set('session.gc_maxlifetime', $saveTime); } }} That’s all! How simple it is! Restrict account logins by controlling sessionsIn some business contexts, user accounts should not be allowed to log in to multiple devices at the same time.When an account is logged in to a new device, the old account session should be cleaned up.We can maintain a new key map to control the session. For example, uniqueAccount - sessionId. In order to control redis more conveniently, we need to install the package about redis. Here we use the predis.12345&quot;require&quot;: { ... &quot;snc/redis-bundle&quot;: &quot;2.1&quot;, &quot;predis/predis&quot;: &quot;^1.0&quot;} Then we add a new record in redis which key is user unique account and value is the sessionId after login success.However, it should be noted that you need to first find and delete all corresponding sessionIds through the account, and then generate a new session record. 12345678910111213141516171819202122232425/** * @Route(\"/login/success\",name=\"login_success\") */public function loginSuccessAction(){ ... $CurrentUser = $this-&gt;getCurrentUser(); $Client = new \\Predis\\Client([ 'scheme' =&gt; $this-&gt;getParameter('session_handler_redis_scheme'), 'host' =&gt; $this-&gt;getParameter('session_handler_redis_host'), 'port' =&gt; $this-&gt;getParameter('session_handler_redis_port'), 'password' =&gt; $this-&gt;getParameter('session_handler_redis_auth'), ]); $sessionId = $Client-&gt;get($CurrentUser-&gt;account()); if ($sessionId) { $Client-&gt;del([$CurrentUser-&gt;account(), $sessionId]); } $session = $this-&gt;get('session'); $session-&gt;start(); $sessionId = \"PHPREDIS_SESSION:\" . $session-&gt;getId(); $Client-&gt;set($CurrentUser-&gt;account(), $sessionId); $Client-&gt;expire($sessionId, $this-&gt;getParameter('session_user_life_seconds'));} Ok, after this step, repeat account login limit is completed too. Clear Session record if logoutDefault logout method in Symfony is declared in the firewall configuration. When we need to customize the behavior of this account logout, we need to declare a new handler. 1$ vim app/config/security.yml 12345678910security: firewalls: ... main: ... logout:# path: /logout# target: / handlers: [app.webservice_logout_listener] First, we should register a new logout service1$ vim app/config/services.yml 123456services: ... app.webservice_logout_listener: class: AppBundle\\Security\\LogoutListener arguments: [\"%session_handler_redis_scheme%\",\"%session_handler_redis_host%\",\"%session_handler_redis_port%\",\"%session_handler_redis_auth%\",\"%session_guest_life_seconds%\"] And this listener will clear all session by current login user’s account.12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061&lt;?php/** * Created by PhpStorm. * User: liyuliang * Date: 17/01/2018 * Time: 3:17 PM */namespace AppBundle\\Security;use AppBundle\\Controller\\me;use Symfony\\Component\\HttpFoundation\\Request;use Symfony\\Component\\HttpFoundation\\Response;use Symfony\\Component\\Security\\Core\\Authentication\\Token\\TokenInterface;use Symfony\\Component\\Security\\Http\\Logout\\LogoutHandlerInterface;class LogoutListener implements LogoutHandlerInterface{ use me; private $scheme; private $host; private $port; private $auth; private $saveTime; public function __construct($scheme, $host, $port = 6379, $auth = null, $saveTime = 1440) { $this-&gt;scheme = $scheme; $this-&gt;host = $host; $this-&gt;port = $port; $this-&gt;auth = $auth; $this-&gt;saveTime = $saveTime; } /** * This method is called by the LogoutListener when a user has requested * to be logged out. Usually, you would unset session variables, or remove * cookies, etc. * @param Request $request * @param Response $response * @param TokenInterface $token */ public function logout(Request $request, Response $response, TokenInterface $token) { $user = $token-&gt;getUser(); $Client = new \\Predis\\Client([ 'scheme' =&gt; 'tcp', 'host' =&gt; $this-&gt;host, 'port' =&gt; $this-&gt;port, 'password' =&gt; $this-&gt;auth, ]); $sessionId = $Client-&gt;get($user-&gt;getUsername()); if ($sessionId) { $Client-&gt;del([$user-&gt;getUsername(), $sessionId]); } }}","link":"/2018/01/17/How-To-Storage-Session-By-Redis-In-Symfony2/"},{"title":"How to Add Virtual Memory In Linux","text":"Server environment12$ cat /etc/issue Ubuntu 14.04.5 LTS View the memory1$ free -h 1234 total used free shared buffers cachedMem: 2.0G 243M 1.7G 424K 11M 145M-/+ buffers/cache: 86M 1.9GSwap: 0B 0B 0B As shown, the memory is 2G. Use the dd command to create a file swapfile with a size of 2G：1$ dd if=/dev/zero of=/mnt/swapfile bs=1M count=2048 “if” means input_file, “of” means output_file, “bs” means block_size, “count” means counting way. Here, I used the data block size 1M, the number of data blocks 2048, so that the allocation of space is 2G size. Format the swap file：1$ mkswap /mnt/swapfile 12Setting up swapspace version 1, size = 2 GiB (2147479552 bytes)no label, UUID=e509fbd9-f640-446d-9261-180f2bdb444c Mount the swap file：1$ swapon /mnt/swapfile 1swapon: /mnt/swapfile: insecure permissions 0644, 0600 suggested. In this way, you can see the memory size after adding 2G virtual memory, as shown in the figure, a total of 4G.1$ free -h 1234 total used free shared buffers cachedMem: 2.0G 243M 1.7G 424K 11M 145M-/+ buffers/cache: 86M 1.9GSwap: 2.0G 0B 2.0G Modify the configuration file, automatically load virtual memory after boot1$ sudo vim /etc/fstab Add the following command to the /etc/fstab file：1/mnt/swapfile swap swap defaults 0 0 Ok ,the tutorial has been done. Change swap size to 8G1234$ sudo swapoff -a$ dd if=/dev/zero of=/mnt/swapfile bs=1M count=8192$ mkswap /mnt/swapfile$ swapon /mnt/swapfile If you use a period of time, the free memory becomes less and less, you can try to release some spare memory space123$ sync$ su$ echo 3 &gt; /proc/sys/vm/drop_caches","link":"/2017/09/18/How-to-Add-Virtual-Memory-In-Linux/"},{"title":"How sentinel build Redis master-slave","text":"Server Environment12345$ cat /etc/issueUbuntu 16.04.2 LTS \\n \\l$ redis-server --versionRedis server v=3.0.6 sha=00000000:0 malloc=jemalloc-3.6.0 bits=64 Target Redis cluster1234redis-master //Port: 6300redis-slave1 //Port: 6301redis-slave2 //Port: 6302redis-sentinel //Port: 16300 Install redis-server1$ sudo apt-get install -y redis-server Install redis-sentinel1$ sudo apt-get install -y redis-sentinel Make the directory and config file123456$ cd /app/$ mkdir -p redis-cluster/node1 redis-cluster/node2 redis-cluster/node3 redis-sentinel/node1$ touch redis-cluster/node1/redis.conf $ touch redis-cluster/node2/redis.conf $ touch redis-cluster/node3/redis.conf$ touch redis-sentinel/node1/sentinel.conf Directory looks like this:123456789101112$ tree appapp|-- redis-cluster| |-- node1| | `-- redis.conf| |-- node2| | `-- redis.conf| `-- node3| `-- redis.conf`-- redis-sentinel `-- node1 `-- sentinel.conf And the redis cluster config file content:123456789$ cat redis-cluster/node1/redis.confbind 127.0.0.1port 6300daemonize yesslave-read-only yesdir /app/redis-cluster/node1pidfile /app/redis-cluster/node1/node.pidlogfile /app/redis-cluster/node1/node.logdbfilename node.rdb 12345678910$ cat redis-cluster/node2/redis.confbind 127.0.0.1port 6301slaveof 127.0.0.1 6300daemonize yesslave-read-only yesdir /app/redis-cluster/node2pidfile /app/redis-cluster/node2/node.pidlogfile /app/redis-cluster/node2/node.logdbfilename node.rdb 12345678910$ cat redis-cluster/node3/redis.confbind 127.0.0.1port 6302slaveof 127.0.0.1 6300daemonize yesslave-read-only yesdir /app/redis-cluster/node3pidfile /app/redis-cluster/node3/node.pidlogfile /app/redis-cluster/node3/node.logdbfilename node.rdb Redis sentinel config file content:1234567891011$ cat redis-sentinel/node1/sentinel.confbind 127.0.0.1port 16300daemonize yesdir /app/redis-sentinel/node1pidfile /app/redis-sentinel/node1/s.pidlogfile /app/redis-sentinel/node1/s.logsentinel monitor redis-cluster 127.0.0.1 6300 1sentinel down-after-milliseconds redis-cluster 5000sentinel failover-timeout redis-cluster 10000 Look at line 9:“redis-cluster” is redis cluster name which master node ‘s ip is 127.0.0.1 and port is 6300. The last param “1” means that one sentinel can elect the master redis node. Line 10 means that if no response in 5 seconds, the election will start Start Redis server and Sentinel1234$ redis-server redis-cluster/node1/redis.conf $ redis-server redis-cluster/node2/redis.conf $ redis-server redis-cluster/node3/redis.conf$ redis-sentinel redis-sentinel/node1/sentinel.conf 12345$ ps aux | grep redisroot 109 0.0 0.1 37220 3528 ? Ssl 06:43 0:00 redis-server 127.0.0.1:6300root 113 0.0 0.1 37220 3500 ? Ssl 06:43 0:00 redis-server 127.0.0.1:6301root 118 0.0 0.1 37220 3428 ? Ssl 06:43 0:00 redis-server 127.0.0.1:6302root 125 0.2 0.1 37220 3420 ? Ssl 06:44 0:00 redis-sentinel 127.0.0.1:16300 [sentinel] Check the Redis cluster info1234567$ redis-cli -p 6300 info | grep -A 5 Replication# Replicationrole:masterconnected_slaves:2slave0:ip=127.0.0.1,port=6301,state=online,offset=32382,lag=1slave1:ip=127.0.0.1,port=6302,state=online,offset=32382,lag=1master_repl_offset:32382 Cluster has been build success. Is Node automatically selected as the master nodeHalt the master node11$ redis-cli -p 6300 debug segfault Check the process1234$ ps aux | grep redisroot 113 0.0 0.1 37220 3500 ? Ssl 06:43 0:00 redis-server 127.0.0.1:6301root 118 0.0 0.1 37220 3428 ? Ssl 06:43 0:00 redis-server 127.0.0.1:6302root 125 0.2 0.1 37220 3420 ? Ssl 06:44 0:01 redis-sentinel 127.0.0.1:16300 [sentinel] Then Check the redis cluster status123456789101112131415$ redis-cli -p 6301 info | grep -A 5 Replication# Replicationrole:slavemaster_host:127.0.0.1master_port:6302master_link_status:upmaster_last_io_seconds_ago:1$ redis-cli -p 6302 info | grep -A 5 Replication# Replicationrole:masterconnected_slaves:1slave0:ip=127.0.0.1,port=6301,state=online,offset=2840,lag=1master_repl_offset:2840repl_backlog_active:1 Slave’s election will mainly evaluate slave in the following reason: 1) Slave priority 2) Process ID Get the master node by sentinel123$ redis-cli -p 16300 sentinel get-master-addr-by-name redis-cluster1) &quot;127.0.0.1&quot;2) &quot;6302&quot; Ok, we can see the node with port 6302 has been elected as the master node.","link":"/2017/03/23/How-sentinel-build-Redis-master-slave/"},{"title":"How to Install docker-ce in Ubuntu17","text":"Server environment12345$ cat /etc/issueUbuntu 17.10$ docker -vDocker version 17.09.0-ce, build afdb6d4 Install using the repository12$ sudo apt-get update$ sudo apt-get install -y apt-transport-https ca-certificates curl software-properties-common Add Docker’s official GPG key123$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -$ sudo apt-key fingerprint 0EBFCD88$ sudo add-apt-repository &quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu zesty stable&quot; Install Docker-ce12$ sudo apt-get update$ sudo apt-get install -y docker-ce Update registry source1$ sudo vim /etc/systemd/system/multi-user.target.wants/docker.service Specified the registry source12#ExecStart=/usr/bin/dockerd -H fd://ExecStart=/usr/bin/dockerd -H fd:// --registry-mirror=https://registry.docker-cn.com 12$ sudo service docker restart$ sudo systemctl daemon-reload","link":"/2017/11/29/How-to-Install-docker-ce-in-Ubuntu17/"},{"title":"How to build github page blog with hexo","text":"My computer develop environment1234567891011121314151617181920212223242526$ sw_vers ProductName: Mac OS XProductVersion: 10.12.6BuildVersion: 16G29$ brew -vHomebrew 1.3.0Homebrew/homebrew-core (git revision 83c2;$ hexo -vhexo: 3.3.7hexo-cli: 1.0.3os: Darwin 16.7.0 darwin x64http_parser: 2.7.0node: 8.4.0v8: 6.0.286.52uv: 1.13.1zlib: 1.2.11ares: 1.10.1-DEVmodules: 57nghttp2: 1.22.0openssl: 1.0.2licu: 59.1unicode: 9.0cldr: 31.0.1tz: 2017b Install hexo by npm1$ sudo npm install -g hexo-cli Create your hexo project and init123$ mkdir hexo$ cd hexo$ hexo init Install hexo plugins12345678910111213141516npm install hexo-generator-index --savenpm install hexo-generator-archive --savenpm install hexo-generator-category --savenpm install hexo-generator-tag --savenpm install hexo-server --savenpm install hexo-deployer-git --savenpm install hexo-deployer-heroku --savenpm install hexo-deployer-rsync --savenpm install hexo-deployer-openshift --savenpm install hexo-renderer-marked --savenpm install hexo-renderer-stylus --savenpm install hexo-generator-feed --savenpm install hexo-generator-sitemap --savenpm install hexo-excerpt --savenpm install node-sass --savenpm install gitalk --save Create a github New repository Repository name must be the host which same with your github page domain, such as xxxx.github.io Set your git global config12$ git config --global user.email &quot;you@email.com&quot;$ git config --global user.name &quot;your name&quot; Generate ssh key1$ ssh-keygen -t rsa -C you@email.com Config ssh key into githubcopy id_rsa.pub content to your github project ‘s Deploy keys1$ cat ~/.ssh/id_rsa.pub Set your git repository url into the hexo project config1$ vim hexo/_config.yml 123deploy: type: git repo: git@github.com:xxxx/xxxx.github.io.git Generate static code by hexo and upload to your github page projectgenerate the code 1$ hexo g upload to github page project1$ hexo d it will be public to your domain xxxx.github.io If something went wrong12345$ hexo new &quot;your new post&quot;dyld: Library not loaded: /usr/local/opt/icu4c/lib/libicui18n.58.dylib Referenced from: /usr/local/bin/node Reason: image not foundAbort trap: 6 Maybe node has been updated during your blog writing ,Try to reinstall node1$ brew reinstall node --without-icu4c","link":"/2017/03/19/How-to-build-github-page-blog-with-hexo/"},{"title":"How to enlarge the window in MacOS","text":"My computer develop environment123$ sw_vers ProductName: Mac OS XProductVersion: 10.12.6 Today i want to share a little operation about keyboard shortcuts.I’m a keyboard lover and i spend 90% of the time on keyboard during my work timeEach time opening the terminal or some other applications, they were too small in my screen but somethings they were perfect size so i want to control the size manually by keyboard. First, Open The System PreferencesThen Find Keyboard -&gt; Shortcuts -&gt; Application ShortcutsAdd A New Option Menu Title : Zoom (In Chinese System, It is 缩放) Then Add Your Application Shortcuts","link":"/2017/05/05/How-to-enlarge-the-window-in-MacOS/"},{"title":"How to inject global variable into Twig","text":"My computer develop environment123456&quot;require&quot;: { &quot;php&quot;: &quot;&gt;=5.6.10&quot;, &quot;symfony/symfony&quot;: &quot;2.8.*&quot;, &quot;twig/twig&quot;: &quot;1.35.*&quot;, ... } In project development, you will often need to display the user’s name, avatar, etc. in the top navigation bar of the website.By default, Symfony using the Twig template engine to store account information into the app.user.username after logging in, but it will not be able to get other user information in app.user. I have a few solutions for this problem Using PHP $_SESSIONstoring the user’s account information into the session as key-value pair after the user logs in. Using Twig Extension, Query user information by user accountTwig Extension can refer to my previous blog Add variables using Symfony’s global methodI think this is the most beautiful way. 1$this-&gt;get('twig')-&gt;addGlobal('header', \"http://xxx.jpg\"\"); When you want to use it in a twig file, you only need a simple output.1{{ header }}","link":"/2017/07/20/How-to-inject-global-variable-into-Twig/"},{"title":"How to install ELK and Filebeat","text":"ELK Server Environment1234567891011$ cat /etc/issue Ubuntu 14.04.4 LTS$ ifconfig eth0 | grep inet inet addr:192.168.0.110$ cat /proc/cpuinfo | grep processor processor : 0 processor : 1 processor : 2 processor : 3$ cat /proc/meminfo | grep MemTotal MemTotal: 8125540 kB Filebeat Client Server Environment1234$ cat /etc/issue Ubuntu 14.04.5 LTS$ ifconfig eth0 | grep inet inet addr:192.168.0.120 Dependence Software Version12345Elasticsearch 2.2.xLogstash 2.2.xKibana 4.4.xFilebeat:1.3.1Java 1.8 Server Distribution Filebeat offers a lightweight way to forward and centralize logs and files without using SSH Logstash collects and filters logs from client server then sends the logs to elasticsearch Elasticsearch is a distributed, scalable, real-time search engine. Here it ‘s used to store the logs. Kibana enables visual exploration and real-time search of your data in elasticsearh Install Java8(openJDK)123$ sudo add-apt-repository ppa:openjdk-r/ppa$ sudo apt-get update $ sudo apt-get install -y openjdk-8-jdk Check java version123456$ javac -versionjavac 1.8.0_141$ java -versionopenjdk version &quot;1.8.0_141&quot;OpenJDK Runtime Environment (build 1.8.0_141-8u141-b15-3~14.04-b15)OpenJDK 64-Bit Server VM (build 25.141-b15, mixed mode) Install Elasticsearch1234$ wget -qO - https://packages.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -$ echo &quot;deb http://packages.elastic.co/elasticsearch/2.x/debian stable main&quot; | sudo tee -a /etc/apt/sources.list.d/elasticsearch-2.x.list$ sudo apt-get update$ sudo apt-get -y install elasticsearch Limit your Elasticsearch api just can be accessed in local machine.123$ sudo vim /etc/elasticsearch/elasticsearch.ymlnetwork.host: 127.0.0.1$ sudo service elasticsearch restart Configure auto start elasticsearch when System started1$ sudo update-rc.d elasticsearch defaults 95 10 Install Logstash123$ echo &apos;deb http://packages.elastic.co/logstash/2.2/debian stable main&apos; | sudo tee /etc/apt/sources.list.d/logstash-2.2.x.list$ sudo apt-get update$ sudo apt-get install logstash Add a listening port for receiving data1$ sudo vim /etc/logstash/conf.d/10-filebeat-input.conf The config file “10-filebeat-input.conf” may not exist. Create a new one. 1234567891011121314input { beats { port =&gt; 5044 }}output { elasticsearch { hosts =&gt; [&quot;http://127.0.0.1:9200&quot;] index =&gt; &quot;%{[@metadata][beat]}-%{+YYYY.MM.dd}&quot; document_type =&gt; &quot;%{[@metadata][type]}&quot; }} 1$ /etc/init.d/logstash restart Logstash will build a new elasticsearch index named with date then send the data which was received in port 5044 to elasticsearch Install Kibana123$ echo &quot;deb http://packages.elastic.co/kibana/4.4/debian stable main&quot; | sudo tee -a /etc/apt/sources.list.d/kibana-4.4.x.list$ sudo apt-get update$ sudo apt-get -y install kibana Limit Kibana access ip123$ sudo vim /opt/kibana/config/kibana.ymlserver.host: 127.0.0.1$ sudo service kibana start Configure auto start kibana when System started 1$ sudo update-rc.d kibana defaults 96 9 Install Filebeat In Client Server (Machine IP: 192.168.0.120)1234$ echo &quot;deb https://packages.elastic.co/beats/apt stable main&quot; | sudo tee -a /etc/apt/sources.list.d/beats.list$ wget -qO - https://packages.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -$ sudo apt-get update$ sudo apt-get install filebeat Configure Filebeat1$ vim /etc/filebeat/filebeat.yml Filebeat only read the config file named “filebeat.yml” under directory “/etc/filebeat/“ Find the property named “paths” which should be crawled and fetched in this config file 12345678910111213############################# Filebeat ######################################filebeat: # List of prospectors to fetch data. prospectors: # Each - is a prospector. Below are the prospector specific configurations - # Paths that should be crawled and fetched. Glob based paths. # To fetch all &quot;.log&quot; files from a specific level of subdirectories # /var/log/*/*.log can be used. # For each file found under this path, a harvester is started. # Make sure not file is defined twice as this can lead to unexpected behaviour. paths: - /tmp/liyuliang-test.log Filebeat will watch the file “/tmp/liyuliang-test.log” and send its content after file ‘s content has changed. paths：which file will be watched . It is processed according to the glob function in Go language.It does not do the recursion. Such as 1/var/log/*/*.log It will never find the file “/var/log/*.log” Annotate all elaticsearch output part123456789101112131415161718############################# Output ########################################### Configure what outputs to use when sending the data collected by the beat.# Multiple outputs may be used.output: ### Elasticsearch as output #elasticsearch: # Array of hosts to connect to. # Scheme and port can be left out and will be set to the default (http and 9200) # In case you specify and additional path, the scheme is required: http://localhost:9200/path # IPv6 addresses should always be defined as: https://[2001:db8::1]:9200 #hosts: [&quot;localhost:9200&quot;] # Optional protocol and basic auth credentials. #protocol: &quot;https&quot; #username: &quot;admin&quot; #password: &quot;s3cr3t&quot; Find the logstash output part and config logstash ip:port and elasticsearch index name 123456789101112131415161718192021222324252627282930############################# Output ########################################### Configure what outputs to use when sending the data collected by the beat.# Multiple outputs may be used.output: ... ... ... ### Logstash as output logstash: # The Logstash hosts hosts: [&quot;192.168.0.110:5044&quot;] # Number of workers per Logstash host. worker: 2 # The maximum number of events to bulk into a single batch window. The # default is 2048. bulk_max_size: 2048 # Set gzip compression level. #compression_level: 3 # Optional load balance the events between the Logstash hosts loadbalance: true # Optional index name. The default index name depends on the each beat. # For Packetbeat, the default is set to packetbeat, for Topbeat # top topbeat and for Filebeat to filebeat. index: &quot;filebeat-logstash&quot; Restart filebeat1$ /etc/init.d/filebeat restart Test Something Input The Log1echo “123456789” &gt;&gt; /tmp/liyuliang-test.log Then check the data can be received in ELK sever123$ curl 192.168.0.110:9200/_cat/indicesyellow open filebeat-logstash-2017.09.12 5 1 60 0 99kb 99kb yellow open .kibana 1 1 1 0 3.1kb 3.1kb It’s success that elasticsearch index filebeat-logstash-2017.09.12 was built.","link":"/2017/09/12/How-to-install-ELK-and-Filebeat/"},{"title":"How to install EduSoho in Ubuntu","text":"Server environment12$ cat /etc/issueUbuntu 14.04.5 LTS Install nginx1$ sudo apt-get install -y nginx Config Nginx1$ sudo vim /etc/nginx/nginx.conf Add this in http{} field1client_max_body_size 1024M; Install mysql1$ sudo apt-get install -y mysql-server Create database1$ mysql -uroot -p Create edusoho database, add database user12CREATE DATABASE `edusoho` DEFAULT CHARACTER SET utf8 ; GRANT ALL PRIVILEGES ON `edusoho`.* TO 'esuser'@'localhost' IDENTIFIED BY 'edusoho'; username: esuserpassword: edusoho Install PHP1$ sudo apt-get -y install php5 php5-cli php5-curl php5-fpm php5-intl php5-mcrypt php5-mysqlnd php5-gd Change PHP file upload size limit1$ sudo vim /etc/php5/fpm/php.ini 123post_max_size = 1024M memory_limit = 1024Mupload_max_filesize = 1024M Config PHP-FPM1$ sudo vim /etc/php5/fpm/pool.d/www.conf 123;listen.owner = www-data;listen.group = www-data;listen.mode = 0660 Remove the semicolon then restart PHP-FPM1$ sudo /etc/init.d/php5-fpm restart Install EduSoho123456$ sudo -s$ mkdir -p /var/www$ cd /var/www$ wget http://download.edusoho.com/edusoho-8.2.5.tar.gz$ tar -zxvf edusoho-8.2.5.tar.gz$ chown www-data:www-data edusoho/ -Rf If this zip package can not download, please visit the edusoho home page for it. Configure nginx virtual host1$ sudo vim /etc/nginx/sites-enabled/default Replace all its contents with following 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566server { listen 80; server_name _; root /var/www/edusoho/web; access_log /var/log/nginx/example.com.access.log; error_log /var/log/nginx/example.com.error.log; location / { index app.php; try_files $uri @rewriteapp; } location @rewriteapp { rewrite ^(.*)$ /app.php/$1 last; } location ~ ^/udisk { internal; root /var/www/edusoho/app/data/; } location ~ ^/(app|app_dev)\\.php(/|$) { fastcgi_pass unix:/var/run/php5-fpm.sock; fastcgi_split_path_info ^(.+\\.php)(/.*)$; include fastcgi_params; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; fastcgi_param HTTPS off; fastcgi_param HTTP_X-Sendfile-Type X-Accel-Redirect; fastcgi_param HTTP_X-Accel-Mapping /udisk=/var/www/edusoho/app/data/udisk; fastcgi_buffer_size 128k; fastcgi_buffers 8 128k; } location ~* \\.(jpg|jpeg|gif|png|ico|swf)$ { # three year expire time expires 3y; # close log record access_log off; # close gzip compression to reduce CPU pressure because the picture compression rate is low gzip off; } location ~* \\.(css|js)$ { access_log off; expires 3y; } # forbidden to access user upload file directory for security location ~ ^/files/.*\\.(php|php5)$ { deny all; } location ~ \\.php$ { fastcgi_pass unix:/var/run/php5-fpm.sock; fastcgi_split_path_info ^(.+\\.php)(/.*)$; include fastcgi_params; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; fastcgi_param HTTPS off; fastcgi_param HTTP_PROXY &quot;&quot;; }} Restart nginx1$ sudo /etc/init.d/nginx restart Open you browse to visit http://localhost/","link":"/2018/01/03/How-to-install-EduSoho-in-Ubuntu/"},{"title":"How to install Go1.8 in ubuntu14.04","text":"Server environment12345$ cat /etc/issue Ubuntu 14.04.5 LTS# go version go version go1.8.4 linux/amd64 Download Install package123$ wget https://storage.googleapis.com/golang/go1.8.4.linux-amd64.tar.gz$ tar -zxvf go1.8.4.linux-amd64.tar.gz$ sudo mv go /usr/local Create the GOPATH directory1$ mkdir -p /home/liang/code/golang Add this code in file: ~/.bash_profile1$ vim ~/.bash_profile 123export GOROOT=/usr/local/go export GOPATH=$HOME/code/golang export PATH=$GOPATH/bin:$GOROOT/bin:$PATH 1$ source ~/.bash_profile","link":"/2017/06/09/How-to-install-Go1-8-in-ubuntu14-04/"},{"title":"How to install Homestead for laravel in Mac","text":"My computer develop environment12345678$ sw_vers ProductName: Mac OS XProductVersion: 10.12.6BuildVersion: 16G29$ brew -vHomebrew 1.3.0Homebrew/homebrew-core (git revision 83c2; last commit 2017-08-03) Install toolsVagrant uses Virtualbox to manage the virtual dependencies. You can directly download virtualbox and install or use homebrew for it.12$ brew cask install virtualbox$ brew cask install vagrant Vagrant-Manager helps you manage all your virtual machines in one place directly from the menubar.1$ brew cask install vagrant-manager After this virtualbox、vagrant and vagrant-manager have been installed under brew cask list12$ brew cask listvagrant Vagrant-Manager virtualbox Add Homestead box to vagrant1$ vagrant box add laravel/homestead The speed is too slow to cause I can’t download this box, if you also meet this situation. Get it by ‘wget’ then put it to the box ‘s directory Get download url in infomation12345678Bringing machine &apos;homestead-7&apos; up with &apos;virtualbox&apos; provider...==&gt; homestead-7: Box &apos;laravel/homestead&apos; could not be found. Attempting to find and install... homestead-7: Box Provider: virtualbox homestead-7: Box Version: &gt;= 3.0.0==&gt; homestead-7: Loading metadata for box &apos;laravel/homestead&apos; homestead-7: URL: https://vagrantcloud.com/laravel/homestead==&gt; homestead-7: Adding box &apos;laravel/homestead&apos; (v3.0.0) for provider: virtualbox homestead-7: Downloading: https://vagrantcloud.com/laravel/boxes/homestead/versions/3.0.0/providers/virtualbox.box Download laravel vagrant box1$ cd ~/.vagrant.d/boxes/; wget -c https://vagrantcloud.com/laravel/boxes/homestead/versions/3.0.0/providers/virtualbox.box Add offline box1$ vagrant box add laravel/homestead ./virtualbox.box Install Homestead1234$ cd ~/$ git clone https://github.com/laravel/homestead.git Homestead$ cd Homestead$ bash init.sh Config HomesteadBecause we use virtualbox as virtual environment 12$ cd ~/Homestead$ vim Homestead.yaml So change the provider:1provider: virtualbox Homestead using the nginx website serverLook at the config file about share directory and nginx site directory 1234567folders: - map: ~/code/php/laravelDemo to: /home/vagrant/Codesites: - map: homestead.app to: /home/vagrant/Code/public I will share my directory “~/code/php/laravelDemo” on the hard disk to virtual directoryAnd the nginx will direct site directory to “/home/vagrant/Code/public” in it’s config file named “homestead.app” Now start the homestead virtual machine1$ vagrant up If vagrant can not find the local box and show a error message “Box ‘laravel/homestead’ could not be found”, please check the laravel virtualbox verion first.12$ vagrant box listlaravel/homestead (virtualbox, 0) My box’s version is 0, which was downloaded beforeSo modify the Homestead script 1$ vim ~/Homestead/scripts/homestead.rb change box min version in the script 12# config.vm.box_version = settings[&quot;version&quot;] ||= &quot;&gt;= 3.0.0&quot; config.vm.box_version = settings[&quot;version&quot;] ||= &quot;&gt;= 0&quot; then1$ vagrant up Now, you can see a virtual machine running in VirtualBoxOpen the url “http://192.168.10.10/&quot; in browser that you can see some information about laravel Add homestead to your project123$ cd ~/code/php/AnotherLaravelDemo;$ composer require laravel/homestead --dev$ php vendor/bin/homestead make There will be a new Homestead.yaml file under your project1$ vagrant up You can see the new project config information for this project","link":"/2017/09/03/How-to-install-Homestead-for-laravel-in-Mac/"},{"title":"How to install sock5 proxy on ubuntu","text":"Server environment12$ cat /etc/issueUbuntu 14.04.4 LTS Install dante1$ apt-get install dante-server If System is 64-bits12$ cd /lib/x86_64-linux-gnu/$ ln -s libc.so.6 libc.so Add a user for dante12$ useradd proxyuser$ passwd proxyuser Create log directory1$ mkdir /var/log/sockd Config Dante1$ vim /etc/danted.conf 12345678910111213141516171819202122232425logoutput: /var/log/sockd/sockd.loginternal: 112.74.79.4 port = 1080external: 112.74.79.4method: username noneuser.privileged: proxyuseruser.notprivileged: nobodyuser.libwrap: nobodyclient pass { from: 0.0.0.0/0 to: 0.0.0.0/0 log: connect disconnect}pass { from: 0.0.0.0/0 to: 0.0.0.0/0 port gt 1023 command: bind log: connect disconnect}pass { from: 0.0.0.0/0 to: 0.0.0.0/0 command: connect udpassociate log: connect disconnect}block { from: 0.0.0.0/0 to: 0.0.0.0/0 log: connect error} Start Dante1$ /etc/init.d/danted start Check listen success1$ netstat -anp | grep 10080","link":"/2017/03/27/How-to-install-sock5-proxy-on-ubuntu/"},{"title":"How to load an image from url to buffer in nodejs","text":"My computer develop environment12node -v //v8.1.0npm -v //5.3.0 After http request a image url , I want the buffer for picture storage serviceso the code look like:1234var request = require('request').defaults({encoding: null});request(url, function (error, response, body) { //body default is string , but it's buffer type now}); or12345678910var request = require('request');var requestSettings = { method: 'GET', url: url, encoding: null};request(url, function (error, response, body) { //body default is string , but it's buffer type now});","link":"/2017/08/30/How-to-load-an-image-from-url-to-buffer-in-nodejs/"},{"title":"How to upload a picture url's content by form POST without local storage in Golang","text":"Server environment12345$ cat /etc/issue Ubuntu 16.04 LTS# go version go version go1.8.4 linux/amd64 Recently, i want to download a few pictures as my souvenirs, but these pictures all comes from other people’s website link.And i have a personal image server which have form post api.So this program will finished these steps in this tutorial with a few line code. Download a picture and save in memory temporary as byte buffer Generate a post form and include picture ‘s information and content Http request to image server The key is to set a new multipart.Writer Download a pictureIt ‘s so sample to do this. 1resp, err := req.Get(imageUrl) Init a empty byte buffer1body = &amp;bytes.Buffer{} Init a writer1writer := multipart.NewWriter(body) The mime/multipart package hides all the complexity of creating a multipart request. Create a writer form and init a fired for picture1writerField, err := writer.CreateFormField(&quot;file&quot;) Create a part for the file form entry with the name of the file param and the name of the file. Translate the picture as byte buffer123456_, err = io.Copy(writerField, resp.Response().Body)if err != nil { return body, &quot;&quot;, err}writer.Close() We need to add the content of the file to the file part, we use the io.Copy() to do so.The multipart.Writer takes care of setting the boundary and formatting the form data for us, nice isn’t it?! Don’t forget to close our writer. One last thing before post form request, you should know the form content type1contentType := writer.FormDataContentType() New a Post form request12request, err := http.NewRequest(&quot;POST&quot;, apiUrl, body)request.Header.Set(&quot;Content-Type&quot;, contentType) Send this request123client := &amp;http.Client{}response, err := client.Do(request) This tutorial is written based on Static file upload.Hopefully this example will be helpful to some of you.","link":"/2018/02/03/How-to-upload-a-picture-url-s-content-by-form-POST-without-local-storage-in-Golang/"},{"title":"How to push multi repositories in same project","text":"My computer develop environment1234567$ sw_versProductName: Mac OS XProductVersion: 10.12.3BuildVersion: 16D30$ git --versiongit version 2.18.0 I recently created a outsourcing project that needs to submit the project code to empolyer’s github repository,But i want to save the project in my private repository at the same time, so i thick i can submit it to two repositoriesat the same time when i push the git.Here is my personal code repository gitee. The first step, create a new repositoryIt named https://gitee.com/liyuliang/xxxxBe careful not to check the option Use Readme file to initialize this project (使用Readme文件初始化这个项目)The first time i checked this option, i got an error when git pushing 12345678To gitee.com:liyuliang/xxxx.git ! [rejected] master -&gt; master (fetch first)error: failed to push some refs to &apos;git@gitee.com:liyuliang/xxxx.git&apos;hint: Updates were rejected because the remote contains work that you dohint: not have locally. This is usually caused by another repository pushinghint: to the same ref. You may want to first integrate the remote changeshint: (e.g., &apos;git pull ...&apos;) before pushing again.hint: See the &apos;Note about fast-forwards&apos; in &apos;git push --help&apos; for details. Second, Add a git repository in your project1$ git remote set-url --add origin git@gitee.com:liyuliang/xxxx.git The project git config file looks like this123456789101112131415$ cat .git/config [core] repositoryformatversion = 0 filemode = true bare = false logallrefupdates = true ignorecase = true precomposeunicode = true[remote &quot;origin&quot;] fetch = +refs/heads/*:refs/remotes/origin/* url = git@github:liyuliang/xxxx.git url = git@gitee.com:liyuliang/xxxx.git[branch &quot;master&quot;] remote = origin merge = refs/heads/master The Last, push your code to all your repositories1$ git push --all","link":"/2018/02/11/How-to-push-multi-repositories-in-same-project/"},{"title":"How to use 'time.After' and 'default' in Golang","text":"My computer develop environment123456$ sw_vers ProductName: Mac OS XProductVersion: 10.12.3$ go versiongo version go1.11.1 darwin/amd64 Interrupted by signalIn the process of using goroutine, for{} is often used to maintain some long-term work. We will control the exit by sending signals to control the goroutine.123select { case &lt;-ch: } When ch channel receives a signal, the for{} loop can be terminated by break.12345678910111213loop: for { select { case &lt;-stopChan: log.Println(&quot;Receive stop sign, stop loop. &quot;) break loop } case &lt;-ch2: // do something } ... } When there are multiple case, select will randomly select a case to execute. At this time, select will block, until that the selected case is finished, and other case will not be executed.But it’s easy to see the code in the selected case run for a long time, which seriously affects the execution of other case. This time you need to have timeout logic. Timeout interrupt loopIn some scenarios, the loop should be time controlled. For example, in one of the cases, the network request timed out.123456for { select { case &lt;-ch: // http get timeout } } Of course, you can also add a timeout attribute to the http request to ensure the timeliness of the network request. But here we use time.After to complete it.First look at its structure123456789// After waits for the duration to elapse and then sends the current time// on the returned channel.// It is equivalent to NewTimer(d).C.// The underlying Timer is not recovered by the garbage collector// until the timer fires. If efficiency is a concern, use NewTimer// instead and call Timer.Stop if the timer is no longer needed.func After(d Duration) &lt;-chan Time { return NewTimer(d).C} This function will return a channel, which is exactly what select{} case needs.time.After() returns a channel message of type Time.Based on this function, it is equivalent to the implementation of the timer, and is non-blocking. Then the code should look like this:12345678loop: for { select { case &lt;-time.After(60 * time.Second): log.Println(&quot;Time to stop. &quot;) break loop } } Exiting the loop after 60 seconds, it looks very simple, right? ‘time.After’ and ‘default’ used togetherIn select{}, not only case, but also a default keyword. 1234select { default: // do default thing} Go back to the previous example, if the limit loop is executed for up to 1 minute. 12345678910111213141516loop: for { select { case &lt;-ch1: // do something 1 case &lt;-ch2: // do something 2 case &lt;-time.After(60 * time.Second): log.Println(&quot;Time %d to stop. &quot;) break loop default: // do default thing } } The code doesn’t seem to have any problems, but when executed, it will be found that the loop will not be aborted!case &lt;-time.After(60 * time.Second): has no work!Why? We can see inside the time.After function, which is actually return a new timer channel.In the code above, each time you execute time.After(60 * time.Second) you create a new timer channel.There’s no way the select statement can remember the channel it selected on in the previous iteration. Now that you know the reason, then the next step is how to update these code. 1234567891011121314151617timeout := time.After(60 * time.Second)loop: for { select { case &lt;-ch1: // do something 1 case &lt;-ch2: // do something 2 case &lt;-timeout: log.Println(&quot;Time %d to stop. &quot;) break loop default: // do default thing } } Very simple, right?select default is blocked, define a timer type channel outside the loop, this time select{} is also fair to select one of the channel to execute.","link":"/2018/03/26/How-to-use-time-After-and-default-in-Golang/"},{"title":"Http Request With Password Header in Golang","text":"My computer develop environment1234567$ sw_vers ProductName: Mac OS XProductVersion: 10.12.6BuildVersion: 16G29$ go versiongo version go1.8.3 darwin/amd64 Request with office golang http package1234567891011121314151617181920212223242526package mainimport ( \"net/http\" \"log\" \"io/ioutil\")func main() { url := \"http://localhost:9112\" username := \"liyuliang\" password := \"password\" request, err := http.NewRequest(\"GET\", url, nil) if err != nil { log.Println(\"error\", err) } if username != \"\" || password != \"\" { request.SetBasicAuth(username, password) } cli := &amp;http.Client{ } response, _ := cli.Do(request) body, err := ioutil.ReadAll(response.Body) println(string(body))} HTTP Basic AuthenticationWhen http request was sent that this request could use http basic auth to access the permission validation by providing a username and passwordIt’s the easiest way to verify permissions because it does not depend on any external factors such as cookies ,sessionsIt must add authorization header each request but it does not encrypt username or password unfortunately Formatted as the following structure: username and password are connected by a colon, such as “username:password” base64 encoded it then be placed after the keyword “Basic” Sometime the header looks like “Basic ZGVtbzpwQDU1dzByZA==” Notice that there is a space behind the keyword “Basic” Request with third party http packages12345678910111213141516171819202122package mainimport ( \"github.com/imroc/req\" \"encoding/base64\")func basicAuth(username, password string) string { auth := username + \":\" + password return base64.StdEncoding.EncodeToString([]byte(auth))}func main() { authHeader := req.Header{ \"Authorization\": \"Basic \" + basicAuth(\"liyuliang\", \"password\"), } resp, _ := req.Get(\"http://localhost:9112\", authHeader) println(resp.String())}","link":"/2017/09/14/Http-Request-With-Password-Header-in-Golang/"},{"title":"Install Beanstalkd in Ubuntu14","text":"Server environment1234567891011$ cat /etc/issue Ubuntu 14.04.5 LTS$ beanstalkd -v beanstalkd 1.9$ php -v PHP 5.5.9-1ubuntu4.22$ composer -V Composer version 1.5.2 Beanstalkd ‘s core concept of design job：A task that needs to be handled asynchronously is the basic unit in Beanstalkd and needs to be placed in a tube。 tube：A task queue, used to store a uniform type of job, is an object of producer and consumer operations。 producer：Job producer, put a job by put command into a tube。 consumer：Job consumers, through the reserve/ release/ bury/ delete command to get the job or change the status of the job。 Job ‘s life123456789101112131415161718put with delay release with delay ----------------&gt; [DELAYED] &lt;------------. | | kick | (time passes) | | | put v reserve | delete -----------------&gt; [READY] ---------&gt; [RESERVED] --------&gt; *poof* ^ ^ | | | \\ release | | | `-------------&apos; | | | | kick | | | | bury | [BURIED] &lt;---------------&apos; | | delete `--------&gt; *poof* Job ‘s status READY : Tasks that need to be processed immediately, automatically become the current task when the DELAYED task expires; DELAYED : delay the implementation of the task, when the consumer to deal with the task, you can use the message back to the DELAYED queue delayed execution; RESERVED : Has been consumer acquisition, the ongoing task. Beanstalkd is responsible for checking whether the task is completed within the time-to-run (TTR) BURIED : Reserved tasks: The task will not be executed, it will not disappear unless someone “kicked” it back in the queue; DELETED : The message is completely deleted. Beanstalkd no longer maintain these messages. Install Beanstalkd1$ sudo aptitude install -y beanstalkd Auto start1$ vim /etc/default/beanstalkd 1234567891011## Defaults for the beanstalkd init script, /etc/init.d/beanstalkd on## Debian systems.BEANSTALKD_LISTEN_ADDR=173.82.218.209BEANSTALKD_LISTEN_PORT=11300# You can use BEANSTALKD_EXTRA to pass additional options. See beanstalkd(1)# for a list of the available options. Uncomment the following line for# persistent job storage.BEANSTALKD_EXTRA=&quot;-b /var/lib/beanstalkd&quot;START=yes Pay attention to this place &quot;-b /var/lib/beanstalkd&quot; which can recovery your queue data after machine shutdown suddenly.Beanstalkd will find its binlog files which is 10M max size default under this directory Using Beanstalkd php clientI’m using the php client pda/pheanstalkIt needs php5.3+ Install PHP5.51$ sudo apt-get install -y php5 Install PHP composer123456$ sudo apt-get update$ sudo apt-get install -y curl$ sudo curl -s https://getcomposer.org/installer | php$ sudo mv composer.phar /usr/local/bin/composer$ composer -VComposer version 1.5.2 Create a Beanstalked PHP demo1234$ mkdir beanstalkedDemo$ cd beanstalkedDemo/$ composer init $ composer require pda/pheanstalk If dependence package cannot be download, please add Chinese resp in composer.json1234567... &quot;repositories&quot;: { &quot;packagist&quot;: { &quot;type&quot;: &quot;composer&quot;, &quot;url&quot;: &quot;https://packagist.phpcomposer.com&quot; } } PHP demo1$ vim main.php 1234567891011121314151617181920212223242526272829303132&lt;?phprequire &apos;vendor/autoload.php&apos;;use Pheanstalk\\Pheanstalk;$level = 1;// most urgent: 0, least urgent: 4294967295$delay = 10; //10s$ttl = 10; //10s$pheanstalk = new Pheanstalk(&apos;127.0.0.1&apos;);// Create a job then put into the tube named testtube$jobId = $pheanstalk -&gt;useTube(&apos;testtube&apos;) -&gt;put( &quot;job payload goes here\\n&quot;, $level, $delay, $ttl );echo ($jobId) . &quot;\\n&quot;;// The consumer watchs the tube, waits a ready job$job = $pheanstalk -&gt;watch(&apos;testtube&apos;) -&gt;reserve();// Deal with the dataecho $job-&gt;getData();// Delete the job after logic success$pheanstalk-&gt;delete($job); 12$ php main.php job payload goes here","link":"/2017/03/19/Install-Beanstalkd-in-Ubuntu14/"},{"title":"Install Homebrew Slow On Mac","text":"My computer develop environment12345678$ sw_vers ProductName: Mac OS XProductVersion: 10.12.6BuildVersion: 16G29$ brew -vHomebrew 1.3.0Homebrew/homebrew-core (git revision 83c2; last commit 2017-08-03) Brew is a package manage on Mac. Official installation brew method:1$ /usr/bin/ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&quot; But sometimes, it is too slow that we can download this installation script1$ wget https://raw.githubusercontent.com/Homebrew/install/master/install You can change the source:1$ vim install 123#BREW_REPO = &quot;https://github.com/Homebrew/brew&quot;.freeze# Replace toBREW_REPO = &quot;git://mirrors.ustc.edu.cn/brew.git&quot;.freeze Or using global proxy which is supported the socks proxy in brew1$ export ALL_PROXY=socks5://127.0.0.1:portnumber Install12$ chmod +x install$ ./install","link":"/2017/04/02/Install-Homebrew-Slow-On-Mac/"},{"title":"JQuery-file-upload plugins work with Symfony3","text":"My computer develop environment1234567891011121314Ubuntu 16.04.1 LTSPHP 7.0.15-0ubuntu0.16.04.4 (cli) ( NTS )Symfony Installer version 1.5.9Composer version 1.3.2 2017-01-27 18:23:41Symfony Installer version 1.5.9mysql&gt; select version();+-----------+| version() |+-----------+| 5.7.12 |+-----------+ About what i want in my projectFor a new project requirement, people can upload their article picture more then 10M that only 2M in the past.Out of consideration for browser compatibility, we choose the jquery-file-upload plugins blueimp at last Here is the sample tutorial about how this plugin works with a symfony3 project code in html1234567&lt;input type=\"file\" accept=\"image/jpg,image/jpeg,image/png,image/gif\" class=\"uploadBigPictures\" name=\"file\"&gt; &lt;script type=\"text/javascript\" src=\"{{ asset('js/jquery-ui/jquery.ui.widget.js') }}\"&gt;&lt;/script&gt;&lt;script type=\"text/javascript\" src=\"{{ asset('js/jquery-file-upload/jquery.iframe-transport.js') }}\"&gt;&lt;/script&gt;&lt;script type=\"text/javascript\" src=\"{{ asset('js/jquery-file-upload/jquery.fileupload.js') }}\"&gt;&lt;/script&gt; 1234567891011121314151617&lt;script&gt; $('.uploadBigPictures').fileupload({ maxChunkSize: 15000000, //set the max file size is very important! url: uploadUrl, success: function (data) { // do what upload request finished }, progressall: function (e, data) { // show the progress during uploading var progress = parseInt(data.loaded / data.total * 100, 10); $('#progress .bar').css( 'width', progress + '%' ); } });&lt;/script&gt; Pay attention to the setting of ‘maxChunkSize‘ in this script code, i have uploaded 12M picture for testing without setting up at the beginning. but empty file content was received in image server api.i checked the settings about the file uploaded max size in nginx and apache configuration file but it dons’t help, it ‘s wasting my time code in php123456789101112131415161718192021222324252627282930/** * @Route( * \"/image/big/upload\", * name=\"img_big_upload\" * ) * @internal param Request $Request * @param Request $request * @return JsonResponse */public function uploadBigAction(Request $request){ try { $uploadedFile = $request-&gt;files-&gt;get('file'); $ext = '.' . $uploadedFile-&gt;guessExtension(); $imgName = time() . $ext; $dir = $this-&gt;urlToDirPath($imgName); $fileName = $this-&gt;urlToFileName($imgName) . $ext; $uploadedFile-&gt;move($dir, $fileName); return $this-&gt;json([ 'success' =&gt; true, ]); } catch (\\Exception $e) { return $this-&gt;json(['success' =&gt; false, 'message' =&gt; $e-&gt;getMessage()]); }}","link":"/2017/06/14/JQuery-file-upload-plugins-work-with-Symfony3/"},{"title":"Keep update MySQL error collection","text":"This is an article about how to solve the error i meet in using MySQL 12ERROR 1064 (42000) at line 22: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near &apos;json DEFAULT NULL, `avatar` varchar(255) COLLATE utf8mb4_unicode_ci DEFAULT NU&apos; at line 9 About this error, you can change your database character 12345678910mysql&gt; show processlist;+----+----------------------+------------------+------+---------+------+----------+------------------+| Id | User | Host | db | Command | Time | State | Info |+----+----------------------+------------------+------+---------+------+----------+------------------+| 4 | root | 172.17.0.1:55022 | NULL | Query | 0 | starting | show processlist || 15 | unauthenticated user | 172.17.0.1:55324 | NULL | Connect | 16 | login | NULL || 16 | unauthenticated user | 172.17.0.1:55342 | NULL | Connect | 6 | login | NULL || 17 | unauthenticated user | 172.17.0.1:55340 | NULL | Connect | 6 | login | NULL |+----+----------------------+------------------+------+---------+------+----------+------------------+4 rows in set (0.00 sec) I have run a mysql docker container for a symfony website.But all of the pages open are very slow, even if loading a blank page.So i check the blank page speed before mysql start and after mysql stop.I found that every time there are these database connection during the page loading.About this error ,you can add following code to solve it.12[mysqld]skip-name-resolve This prevents MySQL trying to resolve the host name for each thread, bypassing the name server problem.","link":"/2017/04/14/Keep-update-MySQL-error-collection/"},{"title":"Install Supervisor For A PHP Script","text":"Server environment12345$ cat /etc/issue Ubuntu 14.04.5 LTS$ supervisord -v 3.0b2 Install Supervisor1$ sudo apt-get install -y supervisor Add A New Supervisor Config File1$ sudo vim /etc/supervisor/conf.d/php-script.conf 12345678910111213141516171819202122[program:updateGoods]directory = /wwwcommand=/usr/bin/php bin/console update:goodsprocess_name = %(program_name)s_%(process_num)02dnumprocs = 1autostart=trueautorestart=truestartsecs=1startretries=3exitcodes = 0,2redirect_stderr=truestopsignal = QUITstopwaitsecs = 10stopasgroup=truestdout_logfile=/www/log/update.goodes.loguser=daemonenvironment = SYMFONY_ENV=prod[supervisord]nodaemon=true[supervisorctl] Here, I have a Symfony3 project under the /www directory, I hope the supervisor can start running a php script by command php bin/console update:goodsif supervisor startup.numprocs means it will start one process and stopsignal means that it will send a quit signal to this php process before supervisor stopnodaemon under [supervisord] is very helpful if you use the docker to manager your container. Start Supervisor Service1$ sudo service supervisor start Check The PHP running status12$ sudo supervisorctl statusapp:updateGoods_00 RUNNING pid 7, uptime 0:44:12 or you can using this commands to control these process123$ sudo supervisorctl stop app:updateGoods_00$ sudo supervisorctl start app:updateGoods_00$ sudo supervisorctl restart app:updateGoods_00 Something wrong i meetAfter first time i start the supervisor service. I saw these errors in my project error.It means that the user (user=daemon) in supervisor config file can not access the /www directory.So you can update the config user=root to fix this problem123456789root@app:~# tail -f /www/log/update.goodes.log supervisor: couldn&apos;t chdir to /www: EACCESsupervisor: child process was not spawnedsupervisor: couldn&apos;t chdir to /www: EACCESsupervisor: child process was not spawnedsupervisor: couldn&apos;t chdir to /www: EACCESsupervisor: child process was not spawnedsupervisor: couldn&apos;t chdir to /www: EACCESsupervisor: child process was not spawned","link":"/2017/10/11/Install-Supervisor-For-A-PHP-Script/"},{"title":"Keep update Symfony3 tips","text":"My computer develop environment12345&quot;require&quot;: { &quot;php&quot;: &quot;&gt;=5.5.9&quot;, &quot;symfony/symfony&quot;: &quot;3.2.*&quot;, ... } Doctrine ‘s column default value declares in annotationoptions={&quot;default&quot;:0} 123456/** * @var int * * @ORM\\Column(name=\"view\", type=\"integer\", nullable=true, options={\"default\":0}) */private $view; This can not be set in command “php bin/console doctrine:generate:entity” , you should add this in entity class manually Doctrine add column index@ORM\\Table(name=&quot;book&quot;,indexes={@ORM\\Index(name=&quot;view_praise&quot;, columns={&quot;view&quot;,&quot;praise&quot;})}) 12345678910/** * Book * * @ORM\\Table(name=\"book\",indexes={@ORM\\Index(name=\"view_praise\", columns={\"view\",\"praise\"})}) * @ORM\\Entity(repositoryClass=\"AppBundle\\Repository\\BookRepository\") */class Book{ ...} Create Console CommandCreate a Command directory under the bundle and the php script file suffixed with Command.php.1234567$ tree yourProject/src//yourProject/src/└── AppBundle ├── AppBundle.php ├── Command │ └── xxxxCommand.php ... or entry this command1$ php bin/console generate:command How to LogoutAdd new config in Symfony firewall1$ vim app/config/security.ym 12345678910111213141516security: providers: webservice: id: app.webservice_user_provider firewalls: ... main: form_login: login_path: login check_path: login use_referer: true logout: path: /logout target: / And new a controller method in your security provider 1$ vim src/AppBundle/Controller/SecurityController.php My provider app.webservice_user_provider direct to the class SecurityController.php 1234567/** * @Route(\"/logout\", name=\"logout\") */public function logoutAction(){ //don't need do anything} If composer install very slow even if the composer source has been changedCheck if php-zip is installed, if it is now installed, composer will download the source installation ans case it to be slow1$ sudo apt-get install php-zip Get twig html content in controller12345$response = $this-&gt;forward(&apos;AppBundle:Preview:articlePreviewContent&apos;, [ &apos;articleId&apos; =&gt; $articleId,]);$content = $response-&gt;getContent(); Maximum function nesting level of ‘256’ reached, aborting!This problem will happen if you are using xdeubg.You should change this limit in xdebug default configuration.1$ vim /etc/php5/conf.d/ext-xdebug.ini Add this option and restart php service.123[xdebug]...xdebug.max_nesting_level = 1000 Argument 1 passed to Twig_Filter::__construct() must be an instance of string, string givenIf you upgrade you php version from 5.6.9 to 7.0+You should upgrade the twig version too.12//composer.json &quot;twig/twig&quot;: &quot;~1.34&quot;, Get all params in controllerIf request is GET method1$arr = $request-&gt;query-&gt;all(); If request is POST method1$arr = $request-&gt;request-&gt;all(); Add twig error pages like 404, 403Add the error page twig under this directory 123456$ tree -L 1 app/Resources/TwigBundle/views/Exception/app/Resources/TwigBundle/views/Exception/├── error.html.twig├── error403.html.twig├── error404.html.twig└── error500.html.twig","link":"/2017/04/12/Keep-update-Symfony3-tips/"},{"title":"Linux upload file to Baidu cloud disk","text":"Server environment12345678$ cat /etc/issue Ubuntu 16.04.2 LTS$ pip -V pip 9.0.1 from /usr/local/lib/python2.7/dist-packages (python 2.7) $ php -v PHP 7.0.22-0ubuntu0.16.04.1 (cli) ( NTS ) Install bypy1$ pip install requests bypy Authorized Login123456$ bypy infoPlease visit:https://openapi.baidu.com/oauth/2.0/authorize?scope=basic+netdisk&amp;redirect_uri=oob&amp;response_type=code&amp;client_id=q8WE4EpCsau1oS0MplgMKNBnAnd authorize this appPaste the Authorization Code here within 10 minutes.Press [Enter] when you are done Visit the url then copy the code in terminal and entry1bc773b9b3884019d284b59c21fccf449 123456Authorizing, please be patient, it may take upto None seconds...Authorizing/refreshing with the OpenShift server ...OpenShift server failed, authorizing/refreshing with the Heroku server ...Successfully authorizedQuota: 2.008TBUsed: 192.089GB Authorized success. 12$ bypy list/apps/bypy ($t $f $s $m $d): Limited by Baidu PCS api, files just can be accessed under the /apps/bypy directory in Baidu cloud disk Upload to cloud disk123$ cd ~/data/$ bypy upload[____________________] 0% (85.2MB/1.3GB) All files under the data directory are uploading. Download file from cloud disk1$ bypy downdir Files under the directory /apps/bypy will be downloaded now.","link":"/2017/12/15/Linux-upload-file-to-Baidu-cloud-disk/"},{"title":"Move gitlab repository to new server","text":"Server environment12$ cat /etc/issueUbuntu 14.04.4 LTS It was using gitlab-rake to backup and restore the gitlab repository in this tutorial First of all, you must check the gitlab version in old server!!12$ cat /opt/gitlab/embedded/service/gitlab-rails/VERSION8.7.0 The old gitlab server version is 8.7.0. So the new machine gitlab version must be 8.7.0, because the data backup by gitlab tool named ‘gitlab-rake’ which can only be recovery in same version.It was worked for me as well. Install dependant library and tools in new machine1$ sudo apt-get install -y curl openssh-server ca-certificates postfix Install gitlab-ce in new machineIn china, it may be slowly to download, so you can use another apt resource image. Of course you can pass this step with a fast network.1$ sudo sh -c &quot;echo &apos;deb https://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/debian jessie main&apos; &gt;&gt; /etc/apt/sources.list.d/gitlab-ce.list&quot; 1$ sudo apt-get update then search gitlab 8.7.0 version123$ sudo apt-cache policy gitlab-ce | grep 8.7.0 Installed: 8.7.0-ce.0 *** 8.7.0-ce.0 0 1$ sudo apt-get install gitlab-ce=8.7.0-ce.0 Backup gitlab repository as a tar package in old server1$ gitlab-rake gitlab:backup:create RAILS_ENV=production In general, backup files are usually stored in directory ‘/var/opt/gitlab/backups’, check the tar file named ‘1490248589_gitlab_backup.tar’. The number 1490248589 was timestamp.Distinguish different backup version.1$ scp /var/opt/gitlab/backups/1490248589_gitlab_backup.tar username@new-server:/var/opt/gitlab/backups/ Copy the gitlab repository backup file to new server Restore repositofy in new server1$ gitlab-rake gitlab:backup:restore RAILS_ENV=production BACKUP=1490248589 If something went wrong /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version `CXXABI_1.3.8’ not found (required by … 123$ sudo add-apt-repository ppa:ubuntu-toolchain-r/test $ sudo apt-get update $ sudo apt-get install g++-4.9 Or you can install by apt-fast when apt install g++ as a snails1$ sudo apt-fast install gcc-4.9 g++-4.9 Repositories backup successfully but getting a 500 page after click the project name in the web view, and gitlab error log shows that 1$ gitlab-ctl tail 1gitlab ErrorPage: serving predefined error page: 500 It caused by the gitlab security file ‘/etc/gitlab/gitlab-secrets.json’, you should copy this file in old server to new server at this time","link":"/2017/03/27/Move-gitlab-repository-to-new-server/"},{"title":"Mysql Slave Config With Docker","text":"Server environment123456789101112$ cat /etc/issue Ubuntu 14.04.5 LTS$ docker -v Docker version 1.6.2, build 7c8fca2mysql&gt; select version();+------------+| version() |+------------+| 5.7.12-log |+------------+ Get mysql basic docker1$ docker pull mysql Mkdir the config file directory123$ mkdir -p ./config/mysql/master/1$ mkdir -p ./config/mysql/slave/11$ mkdir -p ./config/mysql/slave/12 The directory tree:1234567config/└── mysql ├── master │ └── 1 └── slave ├── 11 └── 12 New the config filesmaster 11$ vim config/mysql/master/1/master.cnf 12345678910111213141516171819202122[mysqld]server_id = 1character-set-server=utf8mb4collation-server=utf8mb4_unicode_cidefault-storage-engine=INNODB#Optimize omitsql_mode=NO_ENGINE_SUBSTITUTION,STRICT_TRANS_TABLESlog-bin = /var/lib/mysql/binloglog_bin_trust_function_creators=1binlog_format = ROWexpire_logs_days = 99sync_binlog = 0slow-query-log=1slow-query-log-file=/var/log/mysql/slow-queries.loglong_query_time = 3log-queries-not-using-indexesbinlog-row-image=full slave 111$ vim config/mysql/slave/11/slave.cnf 1234567891011121314151617181920212223242526[mysqld]server_id = 11character-set-server=utf8mb4collation-server=utf8mb4_unicode_cidefault-storage-engine=INNODB#Optimize omitsql_mode=NO_ENGINE_SUBSTITUTION,STRICT_TRANS_TABLESlog-bin = /var/lib/mysql/binloglog_bin_trust_function_creators=1binlog_format = ROWexpire_logs_days = 99sync_binlog = 0relay_log=slave-relay-binlog-slave-updates=1slave-skip-errors=allslow-query-log=1slow-query-log-file=/var/log/mysql/slow-queries.loglong_query_time = 3log-queries-not-using-indexesbinlog-row-image=full slave 121$ vim config/mysql/slave/12/slave.cnf 1234567891011121314151617181920212223242526[mysqld]server_id = 12character-set-server=utf8mb4collation-server=utf8mb4_unicode_cidefault-storage-engine=INNODB#Optimize omitsql_mode=NO_ENGINE_SUBSTITUTION,STRICT_TRANS_TABLESlog-bin = /var/lib/mysql/binloglog_bin_trust_function_creators=1binlog_format = ROWexpire_logs_days = 99sync_binlog = 0relay_log=slave-relay-binlog-slave-updates=1slave-skip-errors=allslow-query-log=1slow-query-log-file=/var/log/mysql/slow-queries.loglong_query_time = 3log-queries-not-using-indexesbinlog-row-image=full Now there will be master-1, slave1-1,slave1-2 three mysql config files12345678910config└── mysql ├── master │ └── 1 │ └── master.cnf └── slave ├── 11 │ └── slave.cnf └── 12 └── slave.cnf Build the mysql containers123$ sudo docker run -v $PWD/config/mysql/master/1:/etc/mysql/conf.d -v $PWD/data/mysql/master/1:/var/lib/mysql -v $PWD/log/mysql/master/1:/var/log/mysql -e MYSQL_ROOT_PASSWORD=mysqlpassword -d -p 4306:3306 --name=mysql-master-1 mysql$ sudo docker run -v $PWD/config/mysql/slave/11:/etc/mysql/conf.d -v $PWD/data/mysql/slave/11:/var/lib/mysql -v $PWD/log/mysql/slave/11:/var/log/mysql -e MYSQL_ROOT_PASSWORD=mysqlpassword -d -p 4307:3306 --name=mysql-slave-11 --link mysql-master-1:master mysql$ sudo docker run -v $PWD/config/mysql/slave/12:/etc/mysql/conf.d -v $PWD/data/mysql/slave/12:/var/lib/mysql -v $PWD/log/mysql/slave/12:/var/log/mysql -e MYSQL_ROOT_PASSWORD=mysqlpassword -d -p 4308:3306 --name=mysql-slave-12 --link mysql-master-1:master mysql 1$ sudo docker ps 1234CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES9716b7e177ad mysql:latest &quot;docker-entrypoint.s 34 minutes ago Up 34 minutes 0.0.0.0:4308-&gt;3306/tcp mysql-slave-12b731f3b7a8d0 mysql:latest &quot;docker-entrypoint.s 34 minutes ago Up 34 minutes 0.0.0.0:4307-&gt;3306/tcp mysql-slave-116a3cbc3f50d4 mysql:latest &quot;docker-entrypoint.s 34 minutes ago Up 34 minutes 0.0.0.0:4306-&gt;3306/tcp mysql-master-1 Entry the master database1$ sudo docker exec -it mysql-master-1 mysql -p Then check the master status1mysql&gt; show master status\\G; 1234567*************************** 1. row ***************************File: binlog.000004Position: 743Binlog_Do_DB:Binlog_Ignore_DB:Executed_Gtid_Set:1 row in set (0.00 sec) Remember the binlog file name and position. Here the file is binlog.000004 and 743 Create master user123mysql&gt; CREATE USER &apos;repl&apos;@&apos;%&apos; IDENTIFIED BY &apos;repl&apos;;mysql&gt; GRANT REPLICATION SLAVE ON *.* TO &apos;repl&apos;@&apos;%&apos;;mysql&gt; FLUSH PRIVILEGES; username is ‘repl’ and password is ‘repl’ Entry the slave Mysql and config them1$ sudo docker exec -it mysql-slave-1 mysql -p 12345678mysql&gt; CHANGE MASTER TO \\ -&gt; MASTER_HOST=&apos;master&apos;,\\ -&gt; MASTER_PORT=3306,\\ -&gt; MASTER_USER=&apos;repl&apos;,\\ -&gt; MASTER_PASSWORD=&apos;repl&apos;,\\ -&gt; MASTER_LOG_FILE=&apos;binlog.000004&apos;,\\ -&gt; MASTER_LOG_POS=743;Query OK, 0 rows affected, 2 warnings (0.03 sec) 12mysql&gt; start slave;Query OK, 0 rows affected (0.00 sec) MASTER_HOST=’master’ // ‘master’ is the docker link alias MASTER_LOG_FILE=’binlog.000004’ and MASTER_LOG_POS=743; //come from master status Check Slave status123456789101112131415mysql&gt; show slave status\\G;*************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: master Master_User: repl Master_Port: 3306 Connect_Retry: 60 Master_Log_File: binlog.000004 Read_Master_Log_Pos: 743 Relay_Log_File: slave-relay-bin.000002 Relay_Log_Pos: 473 Relay_Master_Log_File: binlog.000004 Slave_IO_Running: Yes Slave_SQL_Running: Yes... Ok slave mysql has connected to the master database.You can change something in master then check the changes on slave machine. Set Slave Readonly123456789101112131415161718mysql&gt; show global variables like &quot;%read_only%&quot;;+---------------+-------+| Variable_name | Value |+---------------+-------+| read_only | OFF |+---------------+-------+1 row in set (0.00 sec)mysql&gt; set global read_only=1;Query OK, 0 rows affected (0.06 sec)mysql&gt; show global variables like &quot;%read_only%&quot;;+---------------+-------+| Variable_name | Value |+---------------+-------+| read_only | ON |+---------------+-------+1 row in set (0.00 sec) Set Slave Read-Write12mysql&gt; set global read_only=0;Query OK, 0 rows affected (0.00 sec) Lock All Tables During Sync DataTo ensure that all user, include super user, can not write, look the table read only,12mysql&gt; flush tables with read lock;Query OK, 0 rows affected (0.00 sec) 12mysql&gt; unlock tables;Query OK, 0 rows affected (0.00 sec)","link":"/2017/08/20/Mysql-Slave-Config-With-Docker/"},{"title":"Mysql Specify Slave Database And Table","text":"Server environment12345$ cat /etc/issueUbuntu 16.04 LTS$ mysqld --versionmysqld Ver 5.7.22-0ubuntu0.16.04.1 for Linux on x86_64 ((Ubuntu)) Before this tutorial, your database whatever master or slave , should be connected and synchronize successful.About the slave config, you can find on this blog. Specify the master database which should be synced12345678[mysqld]...# The binlog about this database named 'website' will be sent to all slave mysql# If you do not specify this option, all database's binlog will be sent out# Using ',' as delimiters to specify multi databasebinlog-do-db = website# Ignore be syncedbinlog-ignore-db = performance_schema, information_schema Configuration in slave mysql Only Sync Database In Slave 1234[mysqld]...# Synchronize the specified databasebinlog-do-db = website, app Sync Tables In Slave12345[mysqld]...replicate-do-table = website.userreplicate-do-table = website.articlereplicate-do-table = app.message","link":"/2017/08/25/Mysql-Specify-Slave-Database-And-Table/"},{"title":"Nginx Add Request Host In Log Format","text":"Server environment12345$ cat /etc/issueUbuntu 16.04.3 LTS$ nginx -vnginx version: nginx/1.10.3 (Ubuntu) 12345678910111213141516# cat nginx.conf user www-data;worker_processes auto;...http { ... ## # Logging Settings ## log_format main &apos;$host - $remote_addr - [$time_local] &quot;$request&quot; &apos; &apos;$status $upstream_response_time $request_time &quot;$http_referer&quot;&apos; &apos;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot; $body_bytes_sent &apos;; access_log /var/log/nginx/access.log main; It only take effect that main in access_log /var/log/nginx/access.log main; match the log format name.","link":"/2017/12/03/Nginx-Add-Request-Host-In-Log-Format/"},{"title":"Mysql5.7 MTS(multi-threaded slave)","text":"Server environment12345$ cat /etc/issue Ubuntu 14.04.5 LTS $ mysqld -V mysqld Ver 5.7.20 for Linux on x86_64 (MySQL Community Server (GPL)) Before version 5.6, There are two threads on Mysql Slave, I/O and SQL.I/O thread is responsible for receiving binary log.SQL thread is responsible for play back binary log. At version 5.6, SQL thread became the Coordinator thread. Coordinator thread is mainly responsible for two part. Worker thread will be chosen to execute binary log if Coordinator thread thinks that it can be parallel executed. If not, such as DDL operation or cross-schema transaction, current log will be executed after all Workder threads are finished. It means that Coordinator thread does not only send log to Worker thread, also plays back the log itself, but all parallel operations are handle by Worker thread. Coordinator thread and Worker thread are typical producer and customer models. Only from the MySQL 5.7 release can be called truly parallel replication.The main reason is that the play back on Slave is consistent with the Master.There is no longer parallel replication limit for Slave and special requirement to binary log. MySQL 5.7 introduces a event type binary log called Anonymous_GtidThere will be an Anonymous_Gtid before each transaction begins and there is group submission information in this GTID. There are two more things in new MySQL version ‘s binlog, last_committed and sequence_numberlast_committed is always equal to sequence_number of previous transaction, because transactions are submitted sequentially.last_committed and sequence_number represent the LOGICAL_CLOCK About Master-Slave SetupYou can see my earlier blog Check MySQL 5.7 Group Submission Attributes12345678mysql&gt; show global variables like &apos;%group_commit%&apos;;+-----------------------------------------+-------+| Variable_name | Value |+-----------------------------------------+-------+| binlog_group_commit_sync_delay | 0 || binlog_group_commit_sync_no_delay_count | 0 |+-----------------------------------------+-------+2 rows in set (0.00 sec) binlog_group_commit_sync_delay: global dynamic variable, microsecond unit, default is 0, from 0~1000000 second.How long it will delay to sync to disk after binlog commit. If the value is more than 0, multi transaction ‘s log can be allowed to commit by group. binlog_group_commit_sync_no_delay_count: global dynamic variable, number unit, default is 0, from 0~1000000.Max transaction number to delay commit. If the time of binlog_group_commit_sync_delay does not arrive,data will sync to disk directly. If binlog_group_commit_sync_delay has not be set, this one will not be available. Check Sync process num12345678910mysql&gt; show processlist;+----+-------------+-----------+------+---------+------+--------------------------------------------------------+------------------+| Id | User | Host | db | Command | Time | State | Info |+----+-------------+-----------+------+---------+------+--------------------------------------------------------+------------------+| 1 | system user | | NULL | Connect | 119 | Waiting for master to send event | NULL || 2 | system user | | NULL | Connect | 117 | Slave has read all relay log; waiting for more updates | NULL || 5 | root | localhost | NULL | Query | 0 | starting | show processlist || 7 | root | localhost | NULL | Sleep | 2 | | NULL |+----+-------------+-----------+------+---------+------+--------------------------------------------------------+------------------+4 rows in set (0.00 sec) It shows there is only one master process waiting for synchronization. Check Parallel Worker Num1234567mysql&gt; show variables like &apos;slave_parallel_workers&apos;;+------------------------+-------+| Variable_name | Value |+------------------------+-------+| slave_parallel_workers | 0 |+------------------------+-------+1 row in set (0.00 sec) Current parallel process num is 0. Check Replication Type1234567mysql&gt; show variables like &apos;slave_parallel_type&apos;;+---------------------+----------+| Variable_name | Value |+---------------------+----------+| slave_parallel_type | DATABASE |+---------------------+----------+1 row in set (0.01 sec) Current replication type is DATABASE, it means that only one thread is replicating in one database and cannot replicate parallel. DATABASE：Default, replicate parallel method base on Slave.LOGICAL_CLOCK：replicate parallel method base on group submission Configure multi-thread Set binlog_group_commit_sync_delay bigger than 0 in Master 1mysql&gt; set global binlog_group_commit_sync_delay=10; Stop Replication In Slave 12mysql&gt; stop slave;Query OK, 0 rows affected (0.00 sec) Set Replication Type To LOGICAL_CLOCK In Slave 12345678910mysql&gt; set global slave_parallel_type=&apos;logical_clock&apos;;Query OK, 0 rows affected (0.00 sec)mysql&gt; show variables like &apos;slave_parallel_type&apos;;+---------------------+---------------+| Variable_name | Value |+---------------------+---------------+| slave_parallel_type | LOGICAL_CLOCK |+---------------------+---------------+1 row in set (0.00 sec) Set The Parallel Number To 4 In Slave Machine 12345678910mysql&gt; set global slave_parallel_workers=4;Query OK, 0 rows affected (0.00 sec)mysql&gt; show variables like &apos;slave_parallel_workers&apos;;+------------------------+-------+| Variable_name | Value |+------------------------+-------+| slave_parallel_workers | 4 |+------------------------+-------+1 row in set (0.00 sec) Restart Slave 12mysql&gt; start slave;Query OK, 0 rows affected (0.02 sec) Check Current Running Process Num In Slave 1234567891011121314mysql&gt; show processlist;+----+-------------+-----------+------+---------+------+--------------------------------------------------------+------------------+| Id | User | Host | db | Command | Time | State | Info |+----+-------------+-----------+------+---------+------+--------------------------------------------------------+------------------+| 5 | root | localhost | NULL | Sleep | 230 | | NULL || 7 | root | localhost | NULL | Query | 0 | starting | show processlist || 8 | system user | | NULL | Connect | 4 | Waiting for master to send event | NULL || 9 | system user | | NULL | Connect | 4 | Slave has read all relay log; waiting for more updates | NULL || 10 | system user | | NULL | Connect | 4 | Waiting for an event from Coordinator | NULL || 11 | system user | | NULL | Connect | 4 | Waiting for an event from Coordinator | NULL || 12 | system user | | NULL | Connect | 4 | Waiting for an event from Coordinator | NULL || 13 | system user | | NULL | Connect | 4 | Waiting for an event from Coordinator | NULL |+----+-------------+-----------+------+---------+------+--------------------------------------------------------+------------------+8 rows in set (0.00 sec) Because there is sync replication delay between master and slave, the purpose of multi-thread is to minimize the delay time.","link":"/2017/10/03/Mysql5-7-MTS-multi-threaded-slave/"},{"title":"Nginx + Keepalived, high available load balance","text":"Server environment12345678$ cat /etc/issue Ubuntu 14.04.5 LTS$ nginx -v nginx version: nginx/1.4.6 (Ubuntu) $ apachectl -V Server version: Apache/2.4.7 (Ubuntu) Server Keepalived Virtual IP Upstream IP Web Port Nginx 1 Master 192.168.33.150 WebSite 1、WebSite 2 192.168.33.1 9001 Nginx 2 Backup 192.168.33.150 WebSite 1、WebSite 2 192.168.33.2 9002 WebSite 1 – – – 192.168.33.11 8011 WebSite 2 – – – 192.168.33.12 8011 Install Nginx1$ sudo apt-get install -y nginx Nginx configuration123$ cd /etc/nginx/sites-enabled/$ sudo rm -f default $ sudo vim site.conf 1234567891011121314upstream apache { server 192.168.33.11:8011 weight=1; server 192.168.33.12:8011 weight=1;}server { server_name _ listen 80; location / { proxy_pass http://apache; proxy_set_header X-NGINX &quot;NGINX-1&quot;; // Just add a header info to distinguish }} Restart nginx1$ sudo service nginx restart Install Keepalived1$ sudo apt-get install -y keepalived Config KeepalivedNginx Master 192.168.33.1 : 1$ sudo vim /etc/keepalived/keepalived.conf 12345678910111213141516171819202122232425262728293031vrrp_script chk_nginx { script &quot;/etc/keepalived/check_nginx.sh&quot; //check nginx process script interval 2 weight -20}global_defs { notification_email { //something to notify by mail }}vrrp_instance VI_1 { state MASTER // Mark this one is the master server interface eth0 // Your NIC &apos;s name virtual_router_id 51 // Virtual router, whether master or backup keepalived, it must be the same mcast_src_ip 192.168.33.1 // Master nginx machine &apos;s ip priority 250 // Master priority must higher than backup advert_int 1 authentication { auth_type PASS auth_pass 123456 // Password for communication in keepalived } track_script { chk_nginx } virtual_ipaddress { 192.168.33.150 // Virtual public ip }} Check nginx process script1$ sudo vim /etc/keepalived/check_nginx.sh 123456789#!/bin/bash A=`ps -C nginx --no-header |wc -l`if [ $A -eq 0 ];then /etc/init.d/nginx start sleep 3 if [ `ps -C nginx --no-header |wc -l`-eq 0 ];then killall keepalived fifi Nginx Slave 192.168.33.2 : 1$ sudo vim /etc/keepalived/keepalived.conf 12345678910111213141516171819202122232425262728293031vrrp_script chk_nginx { script &quot;/etc/keepalived/check_nginx.sh&quot; //check nginx process script interval 2 weight -20}global_defs { notification_email { //something to notify by mail }}vrrp_instance VI_1 { state BACKUP // Mark this one is the backup server interface eth0 // Your NIC &apos;s name virtual_router_id 51 // Virtual router, whether master or backup keepalived, it must be the same mcast_src_ip 192.168.33.2 // Slace nginx machine &apos;s ip priority 240 // Master priority must higher than backup advert_int 1 authentication { auth_type PASS auth_pass 123456 // Password for communication in keepalived } track_script { chk_nginx } virtual_ipaddress { 192.168.33.150 // Virtual public ip }} Check nginx process script1$ sudo vim /etc/keepalived/check_nginx.sh 123456789#!/bin/bash A=`ps -C nginx --no-header |wc -l`if [ $A -eq 0 ];then /etc/init.d/nginx start sleep 3 if [ `ps -C nginx --no-header |wc -l`-eq 0 ];then killall keepalived fifi Start Keepalived And Check the process123$ sudo service keepalived start$ ps aux | grep keepalived$ ps -ef | grep nginx Check Virtual IP (192.168.33.150) Has Been Bound192.168.33.1123456789101112131415$ ip add1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 08:00:27:80:6e:f8 brd ff:ff:ff:ff:ff:ff inet 10.0.2.15/24 brd 10.0.2.255 scope global eth0 valid_lft forever preferred_lft forever inet 192.168.33.150/32 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::a00:27ff:fe80:6ef8/64 scope link valid_lft forever preferred_lft forever Look at line 12, the virtual ip is bound on this machine. Also try on backup server 192.168.33.2. 12345678910111213$ ip add1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 08:00:27:80:6e:f8 brd ff:ff:ff:ff:ff:ff inet 10.0.2.15/24 brd 10.0.2.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::a00:27ff:fe80:6ef8/64 scope link valid_lft forever preferred_lft forever You can find that the virtual IP is not bound to the backup server. Visit The Virtual Public IPVisit this link several times to check if there is any change1$ curl http://192.168.33.150 Keepalived Non-Preempt ModeChange Keepalived configuration 123456vrrp_instance VI_1 { state BACKUP // All machines&apos; state change to &apos;BACKUP&apos; ... ... nopreempt // Add non-preempt mode keyword} The master keepalived machine is decided by priority The Meaning Of Non-Preempt ModeMASTER provides service, BACKUP waiting.MASTER crash, BACKUP provides service instead of MASTER.MASTER resume, MASTER waiting, BACKUP still provides service.BACKUP crash, MASTER rebinds virtual IP and provides service.","link":"/2017/10/23/Nginx-Keepalived-high-available-load-balance/"},{"title":"Nginx using X-Accel-Redirect response header to control file been download","text":"Server Environment12345678$ cat /etc/issue Ubuntu 14.04.5 LTS \\n \\l$ php -v PHP 5.6.33-1+ubuntu14.04.1+deb.sury.org+1 (cli) $ nginx -v nginx version: nginx/1.4.6 (Ubuntu) This demo directory structure1234567$ tree /www/download//www/download/├── index.php└── public └── android-x86-5.1-rc1.iso1 directory, 2 files Nginx website configuration12345678910111213141516171819202122server { listen 80; server_name _; root /www/download; location /public { internal; limit_rate 200k; alias /www/download/public; } location / { index index.php; } location ~ \\.php$ { fastcgi_pass unix:/var/run/php/php5.6-fpm.sock; fastcgi_split_path_info ^(.+\\.php)(/.*)$; include fastcgi_params; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; }} The most important kayword is internal in code location /public which told the nginx that URL ‘http://xx/public' can not be accessed.limit_rate limits the file download speed. PHP Code12345678&lt;?php$dir = '/public/';$file = \"android-x86-5.1-rc1.iso\";header('Content-Type: application/octet-stream');header(\"Content-Disposition: attachment; filename='$file'\");header(\"X-Accel-Redirect: {$dir}{$file}\");","link":"/2017/10/09/Nginx-using-X-Accel-Redirect-response-header-to-control-file-been-download/"},{"title":"Omitmepty in Golang","text":"My computer develop environment1234567$ sw_vers ProductName: Mac OS XProductVersion: 10.12.6BuildVersion: 16G29$ go versiongo version go1.8.3 darwin/amd64 Omitempty BehaviorWe often use json to serialize an object. In some scenarios, we only need to pass a property with a value. At this time we will use the omitempty property. Read the official documentation for the encoding/json package and we will find a description of the omitempty behavior: Struct values encode as JSON objects. Each exported struct field becomes a member of the object unless the field’s tag is “-“, orthe field is empty and its tag specifies the “omitempty” option.The empty values are false, 0, any nil pointer or interface value, and any array, slice, map, or string of length zero. The object’s default key string is the struct field name but can be specified in the struct field’s tag value. The “json” key in the struct field’s tag value is the key name, followed by an optional comma and options. Examples: The role of omitempty is to ignore this field when serializing JSON when the value of a field is empty. What needs to be noted here is the definition of empty. The empty values are false, 0, any nil pointer or interface value, and any array, slice, map, or string of length zero. A simple example will help better understand12345678910111213141516171819package mainimport ( \"encoding/json\")type people struct { Name string `json:\"name\"` AccountBalance float32 `json:\"balance\"`}func main() { p := new(people) p.Name = \"liyuliang\" json,_ := json.Marshal(p) println(string(json))} The result is that my account balance is 0.1{&quot;name&quot;:&quot;liyuliang&quot;,&quot;money&quot;:0} When the property AccountBalance is added with omitempty1234type people struct { Name string `json:\"name\"` Money float32 `json:\"money,omitempty\"`} I don’t even have an account!!1{&quot;name&quot;:&quot;liyuliang&quot;} What a impressive property! Isn’t it?","link":"/2018/03/08/Omitmepty-in-Golang/"},{"title":"PHP Allow Multi Cross Domain Request","text":"Server environment12345$ cat /etc/issue Ubuntu 14.04.5 LTS$ php -v PHP 5.6.32-1+ubuntu14.04.1+deb.sury.org+2 (cli) When using PHP to develop a picture server, the image upload interface will be called by multi projects.If these projects use completely different domains(non-subdomains), the access interface will fail due to cross-domain restrictions. If you meet this problem, add the follow php code can help you1234567891011121314$origin = isset($_SERVER['HTTP_ORIGIN']) ? $_SERVER['HTTP_ORIGIN'] : '';$allowOrigin = [ 'http://www.a.com', 'http://www.b.com', 'http://www.c.com',];if (in_array($origin, $allowOrigin)) { header(\"Access-Control-Allow-Origin: \" . $origin);}header('Access-Control-Allow-Headers: X-Requested-With,X_Requested_With'); //Allow access domain headerheader('Access-Control-Allow-Methods:OPTIONS, GET, POST');header(\"Content-Type: text/html; charset=utf-8\");","link":"/2017/06/13/PHP-Allow-Multi-Cross-Domain-Request/"},{"title":"PHP Singleton Mode","text":"Server environment12345$ cat /etc/issue Ubuntu 16.04.2 LTS $ php -v PHP 7.0.22-0ubuntu0.16.04.1 (cli) ( NTS ) The Real One and Modern way to make Singleton Pattern is: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748&lt;?phpClass Singleton{ /** * Singleton constructor. * Make constructor private. so nobody can call \"new Class\". */ private function __construct() { } /** * Make clone magic method private. so nobody can clone instance. */ private function __clone() { } /** * Make sleep magic method private. so nobody can serialize instance. */ private function __sleep() { } /** * Make wakeup magic method private. so nobody can unserialize instance. */ private function __wakeup() { } /** * Call this method to get singleton * @return static */ public static function Instance() { static $instance = false; if ($instance === false) { $instance = new static(); } return $instance; }} So now you can use it like. 12345678910111213141516171819202122&lt;?phpclass Database extends Singleton{ protected $label; /** * @return mixed */ public function getLabel() { return $this-&gt;label; } /** * @param mixed $label */ public function setLabel($label) { $this-&gt;label = $label; }} Test it12345678910111213&lt;?php$database = Database::Instance();$database-&gt;setLabel(\"Liang-app\");echo $database-&gt;getLabel() . PHP_EOL; //Liang-app$other_db = Database::Instance();echo $other_db-&gt;getLabel() . PHP_EOL; //Liang-app$other_db-&gt;setLabel(\"Liang-app-2\");echo $database-&gt;getLabel() . PHP_EOL; //Liang-app-2echo $other_db-&gt;getLabel() . PHP_EOL; //Liang-app-2","link":"/2018/01/03/PHP-Singleton-Mode/"},{"title":"Run PHP Script As Daemon Service","text":"Server environment12345$ cat /etc/issue Ubuntu 14.04.5 LTS$ php -v PHP 5.6.32-1+ubuntu14.04.1+deb.sury.org+2 (cli) How to run php script as daemon process? Nohup1nohup php script.php &amp; Upstart1$ sudo vim /etc/init/myphpworker.conf 123456789start on startupstop on shutdownrespawnrespawn limit 20 5script exec /usr/bin/php -f /app/script.phpend script or you can check the php script if return error then stop this service123script [ $(exec /usr/bin/php -f /app/script.php) = &apos;ERROR&apos; ] &amp;&amp; ( stop; exit 1; )end script If /app/script.php echo a “ERROR” string ,this service will stop Check this service status12345$ sudo service myphpworker statusmyphpworker stop/waiting$ sudo service myphpworker startmyphpworker start/running, process 2648 Check the script output log1234$ sudo tail -f /var/log/upstart/myphpworker.log welcomewelcome... The php script demo12&lt;?phpecho &quot;welcome&quot;. PHP_EOL; SupervisorYou can see my another blog Start-stop-daemonstart-stop-daemon is a linux tool I have wrote a demo and the demo directory is like this1234567$ pwd/home/vagrant/start-stop-daemon# ls -lhtotal 8.0K-rw-rw-r-- 1 vagrant vagrant 59 Jul 11 18:29 a.php-rwxr-xr-x 1 root root 791 Jul 11 18:29 run The run script file:1234567891011121314151617181920212223242526272829303132333435363738394041#! /bin/shDAEMON=/usr/bin/phpDAEMON_OPTS=&apos;/home/vagrant/start-stop-daemon/a.php&apos;NAME=&apos;daemon-demo&apos;DESC=&apos;daemon-demo&apos;PIDFILE=&quot;/var/run/${NAME}.pid&quot;QUIET=&quot;--quiet&quot;START_OPTS=&quot;--start ${QUIET} --background --make-pidfile --pidfile ${PIDFILE} --exec ${DAEMON} ${DAEMON_OPTS}&quot;STOP_OPTS=&quot;--stop --pidfile ${PIDFILE}&quot;test -x $DAEMON || exit 0set -ecase &quot;$1&quot; in start) echo -n &quot;Starting $DESC: &quot; start-stop-daemon $START_OPTS echo &quot;$NAME.&quot; ;; stop) echo -n &quot;Stopping $DESC: &quot; start-stop-daemon $STOP_OPTS echo &quot;$NAME.&quot; ;; restart|force-reload) echo -n &quot;Restarting $DESC: &quot; start-stop-daemon $STOP_OPTS sleep 1 start-stop-daemon $START_OPTS echo &quot;$NAME.&quot; ;; *) N=/etc/init.d/$NAME echo &quot;Usage: $N {start|stop|restart|force-reload}&quot; &gt;&amp;2 exit 1 ;;esacexit 0 Execute it:12345678# ./run startStarting daemon-demo: daemon-demo.# ./run restartRestarting daemon-demo: daemon-demo.# ./run stopStopping daemon-demo: daemon-demo. Or you can put this script in $PATH","link":"/2017/10/11/Run-PHP-Script-As-Daemon-Service/"},{"title":"PHP shared session","text":"Server environment12345$ cat /etc/issue Ubuntu 14.04.5 LTS $ php -v PHP 5.6.32-1+ubuntu14.04.1+deb.sury.org+2 (cli) Session in PHP has two problem in a web project. If cookie is disable in client browser How to synchronize session Session base usage in PHP12session_start(); //Start session$_SESSION['name'] = \"liyuliang\"; Different sub-domains direct to the same server:There are three way to do this. Add this code at the beginning of the php page (before any output and before the method session_start() ) 123ini_set(&apos;session.cookie_path&apos;, &apos;/&apos;);ini_set(&apos;session.cookie_domain&apos;, &apos;.mydomain.com&apos;);ini_set(&apos;session.cookie_lifetime&apos;, &apos;1800&apos;); or1session_set_cookie_params(1800 , &apos;/&apos;, &apos;.mydomain.com&apos;); Point to domain forever in php configuration file php.ini:123session.cookie_path = /session.cookie_domain = .mydomain.comsession.cookie_lifetime = 1800 The above methods have the same effect. Different sub-domains direct to different server:Use NFS to share session If session number is relatively large and all session files are under the same directory If NFS Server get malfunction If NFS network can not be connected It may causes a serious load problem There will be three server machine NFS Hostname IP Server site1 192.168.33.10 Client site2 192.168.33.11 Client site3 192.168.33.12 Step 1. Change PHP session storage directoryStep 2. Install NFS on each server machineStep 3. NFS clients mount to the main server then share the main server ‘s session directory ServerUse /tmp/php_sess as the session sharing directory 1$ mkdir /tmp/php_sess Direct php session save path to redis 1234567$ sudo vim /etc/php/5.6/apache2/php.inisession.save_handler = filessession.save_path = &quot;/tmp/php_sess&quot;session.cookie_path = /session.cookie_domain = .mydomain.comsession.cookie_lifetime = 1800 Restart apache 1$ sudo service apache2 restart Install NFS client1$ sudo apt-get install -y nfs-kernel-server Limit NFS client connecting ip123$ sudo vim /etc/exports /tmp/php_sess 192.168.33.11(rw,sync,no_root_squash,no_subtree_check)/tmp/php_sess 192.168.33.12(rw,sync,no_root_squash,no_subtree_check) Restart NFS service1$ sudo service nfs-kernel-server restart ClientUse /tmp/php_sess as the session sharing directory 1$ mkdir /tmp/php_sess Install NFS client1$ sudo apt install -y nfs-common Show NFS server info123$ sudo showmount -e 192.168.33.10Export list for 192.168.33.10:/tmp/php_sess 192.168.33.12,192.168.33.11 Mount the directory1$ sudo mount 192.168.33.10:/tmp/php_sess /tmp/php_sess Check NFS mount result1$ ls -lh /tmp/php_sess/ Direct php session save path to redis 1234567$ sudo vim /etc/php/5.6/apache2/php.inisession.save_handler = filessession.save_path = &quot;/tmp/php_sess&quot;session.cookie_path = /session.cookie_domain = .mydomain.comsession.cookie_lifetime = 1800 Restart apache1$ sudo service apache2 restart Mount NFS share directory automatically as machine boot12$ sudo vim /etc/fstab192.168.33.10:/tmp/php_sess /tmp/php_sess nfs auto,nofail,noatime,nolock,intr,tcp,actimeo=1800 0 0 Different domains direct to different server:Install redis 1$ sudo apt-get install -y redis-server Bind redis ip12$ sudo vim /etc/redis/redis.conf bind 192.168.33.10 Restart redis1$ sudo service redis-server restart Install php-redis extension1$ sudo apt-get install -y php5.6-redis Direct the session save path to redis in php123$ sudo vim /etc/php/5.6/apache2/php.inisession.save_handler = Redissession.save_path = &quot;tcp://192.168.33.10:6379&quot; Restart apache1$ sudo service apache2 restart","link":"/2017/05/12/PHP-shared-session/"},{"title":"Pass parameters to PHP by shell commands","text":"My computer develop environment12$ php -vPHP 7.0.26 Using $argv or $argc$argv is an array that includes parameters$argc is the number of parameters 123&lt;?phpecho \"total parameters count : {$argc}\" . PHP_EOL;print_r($argv); 12345678910$ php test.php name liyuliang age 25total parameters count : 5Array( [0] =&gt; test1.php [1] =&gt; name [2] =&gt; liyuliang [3] =&gt; age [4] =&gt; 25) Using function getopt123456789101112131415&lt;?php$shortopts = \"\";$shortopts .= \"f:\"; // Required value$shortopts .= \"v::\"; // Optional value$shortopts .= \"abc\"; // These options do not accept values$longopts = [ \"required:\", // Required value \"optional::\", // Optional value \"option\", // No value \"opt\", // No value];$options = getopt($shortopts, $longopts);print_r($options); 123456789$ php test.php -f &quot;value for f&quot; -v -a --required value --optional=&quot;optional value&quot;Array( [f] =&gt; value for f [v] =&gt; [a] =&gt; [required] =&gt; value [optional] =&gt; optional value) Using STDOUT1234&lt;?phpfwrite(STDOUT, 'What is your name：');$name = trim(fgets(STDIN));echo \"Welcome ! {$name} \" . PHP_EOL; 123$ php test.php What is your name：liyuliangWelcome ! liyuliang","link":"/2017/05/05/Pass-parameters-to-PHP-by-shell-commands/"},{"title":"Separate Mysql Slow Log Using Logrotate In Ubuntu","text":"Server environment1234$ cat /etc/issue Ubuntu 16.04.2 LTS$ logrotate --version logrotate 3.8.7 Logrotate is the default Linux system installation tool Create logrotate config fileCreate file /etc/logrotate.d/3306_error.conf 1$ vim /etc/logrotate.d/3306_error 12345678910111213/var/log/mysql/error.log { # log absolute path monthly # cut once a month rotate 13 # save 13 times then rotate, the thirteenth will cover the first file dateext # log file named as &quot;origin file name + 20170821&quot; compress # using gzip to compress delaycompress # the last file without compress (easy to analyze) missingok # during log rotation, any errors will be ignored,such as &quot;File no found&quot; notifempty # it will not be split if no new log after last split postrotate # command will be executed between label &apos;postrotate&apos; and &apos;endscript&apos; mysql -h127.0.0.1 -uroot -puman73 --login-path=3306 -e &apos;flush error logs;&apos; # mysql flush the error logs endscript} Create file /etc/logrotate.d/3306_slow 1$ vim /etc/logrotate.d/3306_slow 12345678910111213/var/log/mysql/slow-queries.log { # log absolute path daily # cut once a month rotate 13 # save 13 times then rotate, the thirteenth will cover the first file dateext # log file named as &quot;origin file name + 20170821&quot; compress # using gzip to compress delaycompress # the last file without compress (easy to analyze) missingok # during log rotation, any errors will be ignored,such as &quot;File no found&quot; notifempty # it will not be split if no new log after last split postrotate # command will be executed between label &apos;postrotate&apos; and &apos;endscript&apos; mysql -h127.0.0.1 -uroot -puman73 --login-path=3306 -e &apos;flush slow logs;&apos; endscript} Manual cutting the loglogrotate -f /etc/logrotate.d/3306_error Check result123$ ls -lh /var/log/mysql/error.log* -rw-r----- 1 mysql adm 0 Aug 21 21:06 /var/log/mysql/error.log -rw-r----- 1 mysql adm 107K Aug 21 21:05 /var/log/mysql/error.log-20171227","link":"/2017/08/21/Separate-Mysql-Slow-Log-Using-Logrotate-In-Ubuntu/"},{"title":"Some tips in Alipay development process","text":"My computer develop environment123$ sw_vers ProductName: Mac OS XProductVersion: 10.12.6 About the PublicKeyIn the pay notify callback, signature verification uses the Alipay public key, not the merchant application public key Pay notify signature check failedYou should check the merchant public and private key is correct by the official toolshttps://tech.open.alipay.com/support/knowledge/index.htm?knowledgeId=201602111105&amp;categoryId=20069#/?_k=455b9g","link":"/2017/06/23/Some-tips-in-Alipay-development-process/"},{"title":"Solve the usb not available problem under Virtualbox win7","text":"Server environment12345$ cat /etc/issue Ubuntu 14.04.5 LTS$ virtualbox --help Oracle VM VirtualBox Manager 5.1.30_Ubuntu Install Virtualbox Extension PackBase on your Virtualbox version to download Oracle VM VirtualBox Extension Pack. My ExtPack version is 5.1.0. After Download.Double Click the execute package “Oracle_VM_VirtualBox_Extension_Pack-5.1.0-xx.vbox-extpack” and Install.Then restart the Virtualbox. If USB still can not be identified.Add User GroupIf USB still can not be identified.Add usbfs user group. (It will have the group vboxusers and group vboxsf after Virtualbox installed) 1$ sudo groupadd usbfs Add username to group12$ sudo adduser liang vboxusers $ sudo adduser liang usbfs My computer username is ‘liang’. Restart the computer, start Virtualbox, confirm in the virtualbox manager, USB device enable usb controler, enable usb3.0 controler tick. There should not appear error message. Virtualbox have USB info,but error drive information in WindowThe error drive detail info mation: Failed to open a session for the virtual machine Windoze 7. The device helper structure version has changed. If you have upgraded VirtualBox recently, please make sure you have terminated all VMs and upgraded any extension packs. If this error persists, try re-installing VirtualBox. (VERR_PDM_DEVHLPR3_VERSION_MISMATCH). Result Code: NS_ERROR_FAILURE (0x80004005) Component: ConsoleInterface: IConsole {db7ab4ca-2a3f-4183-9243-c1208da92392} 1$ sudo apt-get --reinstall install virtualbox-ext-pack It should looks like this:1Successfully installed &quot;Oracle VM VirtualBox Extension Pack&quot;. Entry Windows and Install some drive soft to finish it. I’m using the 360 drive","link":"/2017/06/02/Solve-the-usb-not-available-problem-under-Virtualbox-win7/"},{"title":"TCP 3-Way Handshake Simple Explanation","text":"My computer develop environment1234$ sw_vers ProductName: Mac OS XProductVersion: 10.12.6BuildVersion: 16G29 12345678Host A sends a TCP SYNchronize packet to Host BHost B receives A&apos;s SYNHost B sends a SYNchronize-ACKnowledgementHost A receives B&apos;s SYN-ACKHost A sends ACKnowledgeHost B receives ACK. TCP socket connection is ESTABLISHED. It means that STEP Sender Receiver Purpose 1 Host A Host B B knows A can send 2 Host B Host A A knows B can receive and send 3 Host A Host B B knows A can receive TCP socket connection is ESTABLISHED.","link":"/2017/06/12/TCP-3-Way-Handshake-Simple-Explanation/"},{"title":"Using customer function in Twig","text":"My computer develop environment12345&quot;require&quot;: { &quot;php&quot;: &quot;&gt;=5.6.10&quot;, &quot;symfony/symfony&quot;: &quot;2.8.*&quot;, ... } For example, i want to replace the string in twigFirst, you should add a twig extension under your bundle123$ tree -l src/AppBundle/Twig/src/AppBundle/Twig/└── Preg.php And the php file content look like this.1234567891011121314151617181920&lt;?phpnamespace AppBundle\\Twig;class Preg extends \\Twig_Extension{ public function getFilters() { return [ new \\Twig_SimpleFilter('preg',[ $this, 'contentReplace' ]) ]; } public function contentReplace($content,$search, $replace) { return preg_replace($search,$replace,$content); }} Then modify the config/services.yml to add the new method to system’s service 12345678910111213141516# Learn more about services, parameters and containers at# http://symfony.com/doc/current/book/service_container.htmlparameters:# parameter_name: valueservices:# service_name:# class: AppBundle\\Directory\\ClassName# arguments: [\"@another_service_name\", \"plain_value\", \"%parameter_name%\"] ... preg.twig_extension: class: AppBundle\\Twig\\Preg public: false arguments: [] tags: - { name: twig.extension } After these steps, you can use the method called ‘preg’ defined by Preg.php in the twig template file.1234567&lt;div class=\"panel-body\"&gt; &lt;div class=\"well\"&gt; &lt;p&gt; {{ exception.getMessage|preg(\"/.*lilyuliang'blog/\",'') }} &lt;/p&gt; &lt;/div&gt;&lt;/div&gt;","link":"/2017/07/09/Using-customer-function-in-Twig/"},{"title":"Vertically Scrollable In Div Element Using CSS ","text":"My computer develop environment1234567$ sw_vers ProductName: Mac OS XProductVersion: 10.12.6BuildVersion: 16G29Chrome67.0.3396.99（64 bit） Today I want to share a method that limit div height and view its content by scrolling if content ‘s length is too much for current screen height. First your should make sure the height you want, and set it in div style 1&lt;div style=&quot;height:100%;&quot;&gt; Next, fixed it being vertical direction in the screen and make it can be scroll.The core code is overflow-y: scroll; At last, the complete code is as follow1&lt;div style=&quot;overflow-y: scroll; height:100%;&quot;&gt;","link":"/2018/01/19/Vertically-Scrollable-In-Div-Element-Using-CSS/"},{"title":"About Mysql Partition","text":"Server environment12345$ cat /etc/issueUbuntu 14.04.5 LTS$ mysqld -Vmysqld Ver 5.5.58-0ubuntu0.14.04.1 for debian-linux-gnu on x86_64 ((Ubuntu)) Check Mysql supportMysql version is required at least 5.1 to using partition.Check currency database version whether the partition is supported.1234567mysql&gt; show variables like &apos;%partition%&apos;;+-------------------+-------+| Variable_name | Value |+-------------------+-------+| have_partitioning | YES |+-------------------+-------+1 row in set (0.00 sec) RANGEAssign the records to different partition based on the range of store id(0-6,7-11,12-16,17-21,&gt;21). 12345678910111213141516CREATE TABLE employees_range ( id INT NOT NULL, fname VARCHAR(30), lname VARCHAR(30), hired DATE NOT NULL DEFAULT &apos;1970-01-01&apos;, separated DATE NOT NULL DEFAULT &apos;9999-12-31&apos;, job_code INT NOT NULL, store_id INT NOT NULL)partition BY RANGE (store_id) ( partition p0 VALUES LESS THAN (6), partition p1 VALUES LESS THAN (11), partition p2 VALUES LESS THAN (16), partition p3 VALUES LESS THAN (21), partition p4 VALUES LESS THAN MAXVALUE); Add partition can only increase by value, don’t use the MAXVALUE If defined the MAXVALUE, partition can not be extended. LISTAssign the records to different partition based on the specified store ids. 123456789101112131415CREATE TABLE employees_list ( id INT NOT NULL, fname VARCHAR(30), lname VARCHAR(30), hired DATE NOT NULL DEFAULT &apos;1970-01-01&apos;, separated DATE NOT NULL DEFAULT &apos;9999-12-31&apos;, job_code INT, store_id INT)PARTITION BY LIST(store_id) ( PARTITION pNorth VALUES IN (3,5,6,9,17), PARTITION pEast VALUES IN (1,2,10,11,19,20), PARTITION pWest VALUES IN (4,12,13,14,18), PARTITION pCentral VALUES IN (7,8,15,16)); HASHAssign the records to different partition based on customer expression’s return value.1234567891011CREATE TABLE employees_hash ( id INT NOT NULL, fname VARCHAR(30), lname VARCHAR(30), hired DATE NOT NULL DEFAULT &apos;1970-01-01&apos;, separated DATE NOT NULL DEFAULT &apos;9999-12-31&apos;, job_code INT, store_id INT)PARTITION BY HASH(YEAR(hired))PARTITIONS 4; PARTITIONS 4: the number of partition is 4 KEYSimilar to the HASH partition, KEY partition only supports one or more columns.12345678910CREATE TABLE employees_key ( id INT NOT NULL, fname VARCHAR(30), lname VARCHAR(30), hired DATE NOT NULL DEFAULT &apos;1970-01-01&apos;, separated DATE NOT NULL DEFAULT &apos;9999-12-31&apos;, job_code INT, store_id INT)PARTITION BY HASH (id) PARTITIONS 10; Created a total of 10 partitionsIf Mysql configuration ‘innodb_file_per_table = 1’ is true, each partition table has its own separate file Linear hash partition: When the number of linear hash partition is n times 2, the result of linear hash partition is consistent with the result of regular hash partition Check Partition1234567891011mysql&gt; SELECT PARTITION_NAME,TABLE_ROWS FROM INFORMATION_SCHEMA.PARTITIONS WHERE TABLE_NAME = &apos;employees_range&apos;;+----------------+------------+| PARTITION_NAME | TABLE_ROWS |+----------------+------------+| p0 | 0 || p1 | 0 || p2 | 0 || p3 | 0 || p4 | 0 |+----------------+------------+5 rows in set (0.00 sec) Increase partition in no partition tableChekc users table structure1234567891011121314mysql&gt; show create table users;+-------+-----------------------------+| Table | Create Table |+-------+-----------------------------+| users | CREATE TABLE `users` ( `id` int(10) unsigned NOT NULL AUTO_INCREMENT, ... ... `created_at` timestamp NULL DEFAULT NULL, PRIMARY KEY (`id`), KEY `country_age_income` (`country`,`age`,`sex`,`income`) ) ENGINE=MyISAM AUTO_INCREMENT=110305176 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci |+-------+----------------------------+1 row in set (0.00 sec) Add partition12mysql&gt; ALTER TABLE users PARTITION BY HASH (id) PARTITIONS 10;ERROR 1503 (HY000): A UNIQUE INDEX must include all columns in the table&apos;s partitioning function The partition filed id must be included in the primary key (id ,country_age_income). So remove the previous index for test.123mysql&gt; drop index `country_age_income` on users;Query OK, 10150175 rows affected (6 min 15.40 sec)Records: 10150175 Duplicates: 0 Warnings: 0 Re-execute1234567891011121314151617181920mysql&gt; ALTER TABLE users PARTITION BY HASH (id) PARTITIONS 10;Query OK, 10150175 rows affected (49.95 sec)Records: 10150175 Duplicates: 0 Warnings: 0mysql&gt; SELECT PARTITION_NAME,TABLE_ROWS FROM INFORMATION_SCHEMA.PARTITIONS WHERE TABLE_NAME = &apos;users&apos;;+----------------+------------+| PARTITION_NAME | TABLE_ROWS |+----------------+------------+| p0 | 1015017 || p1 | 1015018 || p2 | 1015018 || p3 | 1015018 || p4 | 1015018 || p5 | 1015018 || p6 | 1015017 || p7 | 1015017 || p8 | 1015017 || p9 | 1015017 |+----------------+------------+11 rows in set (0.01 sec) Selected a few id to test the partition1234567891011121314151617181920212223mysql&gt; explain partitions select * from users where id = 230;+----+-------------+-------+------------+-------+---------------+---------+---------+-------+------+-------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+------------+-------+---------------+---------+---------+-------+------+-------+| 1 | SIMPLE | users | p0 | const | PRIMARY | PRIMARY | 4 | const | 1 | |+----+-------------+-------+------------+-------+---------------+---------+---------+-------+------+-------+1 row in set (0.00 sec)mysql&gt; explain partitions select * from users where id = 231;+----+-------------+-------+------------+-------+---------------+---------+---------+-------+------+-------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+------------+-------+---------------+---------+---------+-------+------+-------+| 1 | SIMPLE | users | p1 | const | PRIMARY | PRIMARY | 4 | const | 1 | |+----+-------------+-------+------------+-------+---------------+---------+---------+-------+------+-------+1 row in set (0.01 sec)mysql&gt; explain partitions select * from users where id = 232;+----+-------------+-------+------------+-------+---------------+---------+---------+-------+------+-------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+------------+-------+---------------+---------+---------+-------+------+-------+| 1 | SIMPLE | users | p2 | const | PRIMARY | PRIMARY | 4 | const | 1 | |+----+-------------+-------+------------+-------+---------------+---------+---------+-------+------+-------+1 row in set (0.01 sec) Check users table partition file structure1234567891011121314151617181920212223$ ls -lh /var/lib/mysql/app/users*-rw-rw---- 1 mysql mysql 17K /var/lib/mysql/app/users.frm-rw-rw---- 1 mysql mysql 60 /var/lib/mysql/app/users.par-rw-rw---- 1 mysql mysql 453M /var/lib/mysql/app/users#P#p0.MYD-rw-rw---- 1 mysql mysql 10M /var/lib/mysql/app/users#P#p0.MYI-rw-rw---- 1 mysql mysql 453M /var/lib/mysql/app/users#P#p1.MYD-rw-rw---- 1 mysql mysql 10M /var/lib/mysql/app/users#P#p1.MYI-rw-rw---- 1 mysql mysql 453M /var/lib/mysql/app/users#P#p2.MYD-rw-rw---- 1 mysql mysql 10M /var/lib/mysql/app/users#P#p2.MYI-rw-rw---- 1 mysql mysql 453M /var/lib/mysql/app/users#P#p3.MYD-rw-rw---- 1 mysql mysql 10M /var/lib/mysql/app/users#P#p3.MYI-rw-rw---- 1 mysql mysql 453M /var/lib/mysql/app/users#P#p4.MYD-rw-rw---- 1 mysql mysql 10M /var/lib/mysql/app/users#P#p4.MYI-rw-rw---- 1 mysql mysql 453M /var/lib/mysql/app/users#P#p5.MYD-rw-rw---- 1 mysql mysql 10M /var/lib/mysql/app/users#P#p5.MYI-rw-rw---- 1 mysql mysql 453M /var/lib/mysql/app/users#P#p6.MYD-rw-rw---- 1 mysql mysql 10M /var/lib/mysql/app/users#P#p6.MYI-rw-rw---- 1 mysql mysql 453M /var/lib/mysql/app/users#P#p7.MYD-rw-rw---- 1 mysql mysql 10M /var/lib/mysql/app/users#P#p7.MYI-rw-rw---- 1 mysql mysql 453M /var/lib/mysql/app/users#P#p8.MYD-rw-rw---- 1 mysql mysql 10M /var/lib/mysql/app/users#P#p8.MYI-rw-rw---- 1 mysql mysql 453M /var/lib/mysql/app/users#P#p9.MYD-rw-rw---- 1 mysql mysql 10M /var/lib/mysql/app/users#P#p9.MYI Specify the partition file directoryMYISAM storage engine can define the data file directory and index file directory separately.INNODB storage engine can only define the the data directory. Whether InnoDB or MyISAM, can not use alter table to modify the data file location. MYISAM 12345678910111213141516171819202122232425262728CREATE TABLE partition_myisam ( id INT AUTO_INCREMENT, store_id INT, order_date DATE, INDEX idx (id)) ENGINE = MYISAMPARTITION BY LIST(store_id) ( PARTITION p1 VALUES IN (1, 3, 4, 17) DATA DIRECTORY = &apos;/data/data&apos; INDEX DIRECTORY = &apos;/data/idx&apos;, PARTITION p2 VALUES IN (2, 12, 14) DATA DIRECTORY = &apos;/data/data&apos; INDEX DIRECTORY = &apos;/data/idx&apos;, PARTITION p3 VALUES IN (6, 8, 20) DATA DIRECTORY = &apos;/data/data&apos; INDEX DIRECTORY = &apos;/data/idx&apos;, PARTITION p4 VALUES IN (5, 7, 9, 11, 16) DATA DIRECTORY = &apos;/data/data&apos; INDEX DIRECTORY = &apos;/data/idx&apos;, PARTITION p5 VALUES IN (10, 13, 15, 18) DATA DIRECTORY = &apos;/data/data&apos; INDEX DIRECTORY = &apos;/data/idx&apos;); INNODB 1234567891011121314151617181920212223CREATE TABLE partition_innodb ( id INT AUTO_INCREMENT, store_id INT, order_date DATE, INDEX idx (id)) ENGINE = INNODBPARTITION BY LIST(store_id) ( PARTITION p1 VALUES IN (1, 3, 4, 17) DATA DIRECTORY = &apos;/data/EN&apos;, PARTITION p2 VALUES IN (2, 12, 14) DATA DIRECTORY = &apos;/data/CN&apos;, PARTITION p3 VALUES IN (6, 8, 20) DATA DIRECTORY = &apos;/data/JP&apos;, PARTITION p4 VALUES IN (5, 7, 9, 11, 16) DATA DIRECTORY = &apos;/data/GB&apos;, PARTITION p5 VALUES IN (10, 13, 15, 18) DATA DIRECTORY = &apos;/data/HK&apos;); If you got this error, make sure the data directory exist and right power1ERROR 1030 (HY000): Got error 168 from storage engine 1$ mkdir /data; chown -R mysql:mysql /data/ Insert data 12345678910111213insert into partition_myisam(id, store_id) values(1, 1);insert into partition_myisam(id, store_id) values(2, 2);insert into partition_myisam(id, store_id) values(6, 6);insert into partition_myisam(id, store_id) values(8, 8);insert into partition_myisam(id, store_id) values(5, 5);insert into partition_myisam(id, store_id) values(9, 9);insert into partition_innodb(id, store_id) values(1, 1);insert into partition_innodb(id, store_id) values(2, 2);insert into partition_innodb(id, store_id) values(6, 6);insert into partition_innodb(id, store_id) values(8, 8);insert into partition_innodb(id, store_id) values(5, 5);insert into partition_innodb(id, store_id) values(9, 9); Check data file distributed 1234567891011121314151617$ tree /data .|-- CN| `-- test| `-- partition_innodb#P#p2.ibd|-- EN| `-- test| `-- partition_innodb#P#p1.ibd|-- GB| `-- test| `-- partition_innodb#P#p4.ibd|-- HK| `-- test| `-- partition_innodb#P#p5.ibd|-- JP| `-- test| `-- partition_innodb#P#p3.ibd Meet some errors MyISAM specified data directory is nvalidThe version 5.5 is no problem. Version 5.7.12 will appear warning. The data directory is still the default directory.InnoDB data files can not use the same way as MyISAM label link specified data files.123456mysql&gt; SHOW VARIABLES LIKE &apos;have_symlink&apos;; +---------------+----------+| Variable_name | Value |+---------------+----------+| have_symlink | DISABLED |+---------------+----------+ 123$ vim /etc/mysql/conf.d/mysql.cnfsymbolic-links=1 # Change to 1 than enable the symbolic link 1$ sudo service mysql restart","link":"/2017/08/26/About-Mysql-Partition/"},{"title":"Canal sync data to elasticsearch by Mysql binlog","text":"Server environment12345678910$ cat /etc/issue Ubuntu 14.04.5 LTS$ java -versionjava version &quot;1.8.0_151&quot;Java(TM) SE Runtime Environment (build 1.8.0_151-b12)Java HotSpot(TM) 64-Bit Server VM (build 25.151-b12, mixed mode) $ mysqld --versionmysqld Ver 5.7.20 for Linux on x86_64 (MySQL Community Server (GPL)) My computer develop environment1234567891011$ sw_vers ProductName: Mac OS XProductVersion: 10.12.6BuildVersion: 16G29$ brew -vHomebrew 1.3.0Homebrew/homebrew-core (git revision 83c2; last commit 2017-08-03)$ mvn -vApache Maven 3.5.2 Install Java8+123$ sudo add-apt-repository ppa:webupd8team/java$ sudo apt-get update$ sudo apt-get install oracle-java8-installer or Java91$ sudo apt-get install oracle-java9-installer Set Mysql row binlog format1$ sudo vim /etc/mysql/mysql.conf.d/mysqld.cnf 1234567891011121314...server_id = 1log-bin = /var/lib/mysql/binloglog_bin_trust_function_creators=1binlog_format = ROWexpire_logs_days = 99sync_binlog = 0slow-query-log=1slow-query-log-file=/var/log/mysql/slow-queries.loglong_query_time = 10log-queries-not-using-indexesbinlog-row-image=full Create Canal user in Mysql123mysql&gt; CREATE USER canal IDENTIFIED BY &apos;canal&apos;; mysql&gt; GRANT SELECT, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO &apos;canal&apos;@&apos;%&apos;; mysql&gt; FLUSH PRIVILEGES; or you can give all power123mysql&gt; CREATE USER canal IDENTIFIED BY &apos;canal&apos;;mysql&gt; GRANT ALL PRIVILEGES ON *.* TO &apos;canal&apos;@&apos;%&apos; ; mysql&gt; FLUSH PRIVILEGES; Install Canal12345678910$ wget https://github.com/alibaba/canal/releases/download/canal-1.0.24/canal.deployer-1.0.24.tar.gz$ tar zxvf canal.deployer-1.0.24.tar.gz -C canal$ tree canal -L 1canal├── bin├── conf├── lib└── logs Update config instance.properties12$ cd canal$ vim conf/example/instance.properties 123456789101112131415161718192021222324252627################################################### mysql serverIdcanal.instance.mysql.slaveId = 1234# position infocanal.instance.master.address = 127.0.0.1:3306canal.instance.master.journal.name =canal.instance.master.position =canal.instance.master.timestamp =#canal.instance.standby.address = #canal.instance.standby.journal.name =#canal.instance.standby.position = #canal.instance.standby.timestamp = # username/passwordcanal.instance.dbUsername = canalcanal.instance.dbPassword = canalcanal.instance.defaultDatabaseName = appcanal.instance.connectionCharset = UTF-8# table regexcanal.instance.filter.regex = .*\\\\..*# table black regexcanal.instance.filter.black.regex =################################################# Start Canal1$ ./bin/startup.sh Check Canal running status in log file123456$ tail -f logs/canal/canal.log ...2017-10-01 15:02:24.591 [main] INFO com.alibaba.otter.canal.deployer.CanalLauncher - ## start the canal server.2017-10-01 15:02:24.720 [main] INFO com.alibaba.otter.canal.deployer.CanalController - ## start the canal server[10.0.2.15:11111]2017-10-01 15:02:25.541 [main] INFO com.alibaba.otter.canal.deployer.CanalLauncher - ## the canal server is running now ...... Ok, Canal server has been started!And now we need the Canal client to sync data to elasticsearch Install MavenMy development machine is Mac, so i will install Maven by brew12345678910111213$ brew install maven$ brew info mavenmaven: stable 3.5.2Java-based project managementhttps://maven.apache.org/Conflicts with: mvnvm (because also installs a &apos;mvn&apos; executable)/usr/local/Cellar/maven/3.5.2 (104 files, 10.1MB) * Built from source on 2017-10-01 at 23:45:23From: https://github.com/Homebrew/homebrew-core/blob/master/Formula/maven.rb==&gt; RequirementsRequired: java &gt;= 1.7 ✔ Add maven image repository address12345678910111213$ cd /usr/local/Cellar/maven/3.5.2$ vim libexec/conf/settings.xml &lt;mirrors&gt;... &lt;mirror&gt; &lt;id&gt;alimaven&lt;/id&gt; &lt;name&gt;aliyun maven&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;/mirror&gt; &lt;/mirrors&gt;... Create a Maven(Java) project1$ mvn archetype:generate -DgroupId=com.alibaba.otter -DartifactId=canal.client After some information confirm, a empty maven project was create.123456$ tree canal.client/ -L 1canal.client/├── pom.xml└── src1 directory, 1 file Add com.alibaba.otter dependence1$ cd canal.client/; vim pom.xml 12345678&lt;dependencies&gt;... &lt;dependency&gt; &lt;groupId&gt;com.alibaba.otter&lt;/groupId&gt; &lt;artifactId&gt;canal.client&lt;/artifactId&gt; &lt;version&gt;1.0.22&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; Full pom.xml config file123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.alibaba.otter&lt;/groupId&gt; &lt;artifactId&gt;canal.sample&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;canal.sample&lt;/name&gt; &lt;url&gt;http://maven.apache.org&lt;/url&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;!-- base on your java version --&gt; &lt;maven.compiler.source&gt;1.9&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.9&lt;/maven.compiler.target&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;3.8.1&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- canal client --&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba.otter&lt;/groupId&gt; &lt;artifactId&gt;canal.client&lt;/artifactId&gt; &lt;version&gt;1.0.22&lt;/version&gt; &lt;/dependency&gt; &lt;!-- elasticsearch client --&gt; &lt;dependency&gt; &lt;groupId&gt;io.searchbox&lt;/groupId&gt; &lt;artifactId&gt;jest&lt;/artifactId&gt; &lt;version&gt;2.0.0&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;!-- build executable jar file plugin --&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt; &lt;version&gt;1.2.1&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;shade&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;transformers&gt; &lt;transformer implementation=\"org.apache.maven.plugins.shade.resource.ManifestResourceTransformer\"&gt; &lt;mainClass&gt;com.alibaba.otter.App&lt;/mainClass&gt; &lt;/transformer&gt; &lt;/transformers&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; Init the Maven project1$ mvn install Add the code123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163package com.alibaba.otter;import com.alibaba.otter.canal.client.CanalConnector;import com.alibaba.otter.canal.client.CanalConnectors;import com.alibaba.otter.canal.common.utils.AddressUtils;import com.alibaba.otter.canal.protocol.CanalEntry;import com.alibaba.otter.canal.protocol.Message;import io.searchbox.client.JestClient;import io.searchbox.client.JestClientFactory;import io.searchbox.client.config.HttpClientConfig;import io.searchbox.core.Delete;import io.searchbox.core.Index;import java.io.IOException;import java.net.InetSocketAddress;import java.util.LinkedHashMap;import java.util.List;import java.util.Map;/** * Hello world! */public class App { public static void main(String[] args) { CanalConnector connector = CanalConnectors.newSingleConnector(new InetSocketAddress(AddressUtils.getHostIp(), 11111), \"example\", \"\", \"\"); int batchSize = 1000; try { connector.connect(); connector.subscribe(\".*\\\\..*\"); connector.rollback(); while (true) { Message message = connector.getWithoutAck(batchSize); // get specify size long batchId = message.getId(); int size = message.getEntries().size(); if (batchId == -1 || size == 0) { try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } } else { translateDatas(message.getEntries()); } connector.ack(batchId); // commit confirm // connector.rollback(batchId); // rollback if something gets errors } } finally { connector.disconnect(); } } private static void translateDatas(List&lt;CanalEntry.Entry&gt; entrys) { for (CanalEntry.Entry entry : entrys) { if (entry.getEntryType() == CanalEntry.EntryType.TRANSACTIONBEGIN || entry.getEntryType() == CanalEntry.EntryType.TRANSACTIONEND) { continue; } CanalEntry.RowChange rowChange = null; try { rowChange = CanalEntry.RowChange.parseFrom(entry.getStoreValue()); } catch (Exception e) { throw new RuntimeException(\"ERROR ## parser of eromanga-event has an error , data:\" + entry.toString(), e); } CanalEntry.EventType eventType = rowChange.getEventType(); String dbName = entry.getHeader().getSchemaName(); String tableName = entry.getHeader().getTableName(); System.out.println(String.format(\"================&gt; binlog[%s:%s] , name[%s,%s] , eventType : %s\", entry.getHeader().getLogfileName(), entry.getHeader().getLogfileOffset(), entry.getHeader().getSchemaName(), entry.getHeader().getTableName(), eventType)); JestClient ESClient = ESClient(); for (CanalEntry.RowData rowData : rowChange.getRowDatasList()) { try { if (eventType == CanalEntry.EventType.DELETE) { for (CanalEntry.Column column : rowData.getBeforeColumnsList()) { if (column.getName().toLowerCase().equals(\"id\")) { ESClient.execute(new Delete.Builder(column.getValue()) .index(dbName) .type(tableName) .build()); } }// } else if (eventType == CanalEntry.EventType.INSERT) { } else { Map&lt;String, String&gt; mapping = columnToMapping(rowData.getAfterColumnsList()); Index index = new Index.Builder(mapping).index(dbName).type(tableName).id(mapping.get(\"id\")).build(); ESClient.execute(index); System.out.println(\"-------&gt; before\"); printColumn(rowData.getBeforeColumnsList()); System.out.println(\"-------&gt; after\"); } } catch (IOException e) { e.printStackTrace(); } } } } private static Map&lt;String, String&gt; columnToMapping(List&lt;CanalEntry.Column&gt; afterColumnsList) { Map&lt;String, String&gt; data = new LinkedHashMap&lt;String, String&gt;(); for (CanalEntry.Column column : afterColumnsList) { data.put(column.getName(), column.getValue()); } return data; } private static JestClient ESClient; private static JestClient ESClient() { if (ESClient == null) { JestClientFactory factory = new JestClientFactory(); factory.setHttpClientConfig(new HttpClientConfig .Builder(\"http://localhost:9200\") .build()); ESClient = factory.getObject(); } return ESClient; } private static void printColumn(List&lt;CanalEntry.Column&gt; columns) { for (CanalEntry.Column column : columns) { System.out.println(column.getName() + \" : \" + column.getValue() + \" update=\" + column.getUpdated()); } }} 全量同步http://shzhangji.com/cnblogs/2017/08/13/extract-data-from-mysql-with-binlog-and-canal/","link":"/2017/08/21/Canal-sync-data-to-elasticsearch-by-Mysql-binlog/"},{"title":"Compile and install LAMP in Ubuntu","text":"Server environment12$ cat /etc/issueUbuntu 14.04.5 LTS Some errors encountered during the installation at the bottom of this page Using root account1$ sudo -s 12$ mkdir -p /download$ cd /download Install dependence package1234$ sudo apt-get install -y make gcc automake bison cmake libtool wget gcc unzip \\ openssl libssl-dev libxml2 curl build-essential ruby zlib1g zlib1g.dev libncurses5-dev \\ libxml2-dev libcurl4-gnutls-dev libjpeg-dev libpng-dev libxpm-dev \\ libfreetype6-dev libt1-dev libmcrypt-dev libmysql++-dev libxslt1-dev Install Apr12345678$ cd /download$ wget http://mirror.bit.edu.cn/apache/apr/apr-1.6.3.tar.gz$ tar zxvf apr-1.6.3.tar.gz$ cd apr-1.6.3$ autoreconf --force --install$ libtoolize --automake --force$ ./configure -with-apr=/usr/local/apr$ make &amp;&amp; make install Install Pcre123456$ cd /download$ wget http://ftp.exim.llorien.org/pcre/pcre-8.36.tar.gz$ tar zxvf pcre-8.36.tar.gz$ cd pcre-8.36$ ./configure --prefix=/usr/pcre$ make &amp;&amp; make install Install Apache123456789101112131415161718192021222324252627282930313233343536373839404142434445$ cd /download$ wget http://ftp.itu.edu.tr/Mirror/Apache//httpd/httpd-2.2.34.tar.gz$ tar zxvf httpd-2.2.34.tar.gz$ cd httpd-2.2.34$ ./configure \\ &quot;--prefix=/etc/httpd&quot; \\ &quot;--exec-prefix=/etc/httpd&quot; \\ &quot;--bindir=/usr/bin&quot; \\ &quot;--sbindir=/usr/sbin&quot; \\ &quot;--sysconfdir=/etc/httpd/conf&quot; \\ &quot;--enable-so&quot; \\ &quot;--enable-dav&quot; \\ &quot;--enable-dav-fs&quot; \\ &quot;--enable-dav-lock&quot; \\ &quot;--enable-suexec&quot; \\ &quot;--enable-deflate&quot; \\ &quot;--enable-unique-id&quot; \\ &quot;--enable-mods-static=most&quot; \\ &quot;--enable-reqtimeout&quot; \\ &quot;--with-mpm=prefork&quot; \\ &quot;--with-suexec-caller=apache&quot; \\ &quot;--with-suexec-docroot=/&quot; \\ &quot;--with-suexec-gidmin=100&quot; \\ &quot;--with-suexec-logfile=/var/log/httpd/suexec_log&quot; \\ &quot;--with-suexec-uidmin=100&quot; \\ &quot;--with-suexec-userdir=public_html&quot; \\ &quot;--with-suexec-bin=/usr/sbin/suexec&quot; \\ &quot;--with-included-apr&quot; \\ &quot;--with-pcre=/usr/pcre&quot; \\ &quot;--includedir=/usr/include/apache&quot; \\ &quot;--libexecdir=/usr/lib/apache&quot; \\ &quot;--datadir=/var/www&quot; \\ &quot;--localstatedir=/var&quot; \\ &quot;--enable-logio&quot; \\ &quot;--enable-ssl&quot; \\ &quot;--enable-rewrite&quot; \\ &quot;--enable-proxy&quot; \\ &quot;--enable-expires&quot; \\ &quot;--with-ssl=/usr&quot; \\ &quot;--enable-headers&quot; $ make &amp;&amp; make install$ sed -i &quot;s/&lt;Directory \\&quot;\\/var\\/www\\/htdocs\\&quot;&gt;/&lt;Directory \\&quot;\\/var\\/www\\&quot;&gt;/g&quot; /etc/httpd/conf/httpd.conf$ sed -i &quot;s/DocumentRoot \\&quot;\\/var\\/www\\/htdocs\\&quot;/DocumentRoot \\&quot;\\/var\\/www\\&quot;/g&quot; /etc/httpd/conf/httpd.conf$ sed -i &quot;s/DirectoryIndex index.html/DirectoryIndex index.html index.php/g&quot; /etc/httpd/conf/httpd.conf Install Mysql12345678910$ cd /download$ wget http://dev.mysql.com/get/Downloads/MySQL-5.6/mysql-5.6.14.tar.gz$ tar zxvf mysql-5.6.14.tar.gz $ cd mysql-5.6.14$ groupadd mysql$ useradd -g mysql mysql -s /bin/false$ mkdir -p /data/mysql /usr/local/mysql$ chown -R mysql:mysql /data/mysql$ cmake -DMYSQL_UNIX_ADDR=/var/lib/mysql/mysql.sock$ make &amp;&amp; make install 123456$ cd /usr/local/mysql/$ chown -R mysql:mysql .$ scripts/mysql_install_db --user=mysql --datadir=/data/mysql$ cp ./support-files/my-default.cnf /etc/my.cnf datadir = /data/mysql 12345678910111213$ cp support-files/mysql.server /etc/init.d/mysqld$ chmod 755 /etc/init.d/mysqld$ update-rc.d mysqld defaults# Add Mysql service in system environment$ vim /etc/profile export PATH=$PATH:/usr/local/mysql/bin$ source /etc/profile$ service mysqld start# change mysql password$ ./bin/mysqladmin -u root password &apos;new-password&apos; Install libmcrypt123456$ cd /download$ wget http://sourceforge.net/projects/mcrypt/files/Libmcrypt/2.5.8/libmcrypt-2.5.8.tar.gz$ tar zxvf libmcrypt-2.5.8.tar.gz$ cd libmcrypt-2.5.8$ ./configure$ make &amp;&amp; make install Install PHP 5.5.5123456789101112131415161718192021222324252627282930313233343536373839$ cd /download$ wget -O php-5.5.5.tar.gz http://us2.php.net/get/php-5.5.5.tar.gz/from/this/mirror $ tar zxvf php-5.5.5.tar.gz$ cd php-5.5.5$ ./configure \\ --with-apxs2 \\ --with-curl=/usr \\ --with-gd \\ --with-gettext \\ --with-jpeg-dir=/usr \\ --with-freetype-dir=/usr \\ --with-kerberos \\ --with-openssl \\ --with-mcrypt=/usr/local/lib \\ --with-mhash \\ --with-mysql=mysqlnd \\ --with-mysqli=mysqlnd \\ --with-pcre-regex \\ --with-pear \\ --with-png-dir=/usr \\ --with-xsl \\ --with-zlib \\ --with-zlib-dir=/usr \\ --with-iconv \\ --enable-bcmath \\ --enable-calendar \\ --enable-exif \\ --enable-ftp \\ --enable-gd-native-ttf \\ --enable-soap \\ --enable-sockets \\ --enable-mbstring \\ --enable-zip \\ --enable-wddx $ make &amp;&amp; make install$ libtool --finish ./libs$ cp php.ini-production /usr/local/lib/php.ini$ sed -i &apos;s/;date.timezone =.*/ date.timezone \\= &quot;Europe\\/Istanbul&quot;/&apos; /usr/local/lib/php.ini Install phpmyadmin123456$ cd /download$ wget https://files.phpmyadmin.net/phpMyAdmin/4.7.7/phpMyAdmin-4.7.7-all-languages.tar.gz$ tar zxvf phpMyAdmin-4.7.7-all-languages.tar.gz$ mv phpMyAdmin-4.7.7-all-languages /var/www/phpMyAdmin$ cd /var/www/phpMyAdmin$ cp config.sample.inc.php config.inc.php ###12345$ echo &quot;&lt;?PHP phpInfo();?&gt;&quot; &gt; /var/www/index.php$ echo &quot;AddType application/x-httpd-php .php&quot; &gt;&gt; /etc/httpd/conf/extra/httpd-php.conf$ echo &quot;AddType application/x-httpd-php-source .phps&quot; &gt;&gt; /etc/httpd/conf/extra/httpd-php.conf$ echo &quot;Include conf/extra/httpd-php.conf&quot; &gt;&gt; /etc/httpd/conf/httpd.conf Restart Apache1$ sudo apachectl restart Ok! Open you browser and visit http://localhost and http://localhost/phpMyAdmin/ with mysql root account Meet some errors configure: error: Did not find pcre-config script at /usr You should install the apr package first cannot remove ‘libtoolT’: No such file or directory During installing apr package. 1$ vim apr-1.6.3/configure 1# $RM &quot;$cfgfile&quot; then re-configure configure: error: You need a C++ compiler for C++ support. 1$ sudo apt-get install -y build-essential checking whether to enable mod_deflate… configure: error: mod_deflate has been requested but can not be built due to prerequisite failures 1$ sudo apt-get install -y ruby zlib1g zlib1g.dev configure: error: …No recognized SSL/TLS toolkit detected 1$ sudo apt-get install -y openssl libssl-dev Couldn’t find MySQL server (/usr/bin/mysqld_safe) 1$ rm /etc/mysql/my.cnf The server quit without updating PID file (/usr/local/mysql/data/vagrant-ubuntu-trusty-64.pid). 1$ rm /etc/mysql/my.cnf mysql cmake: Curses library not found. Please install appropriate package, 12$ sudo apt-get install -y libncurses5-dev$ rm -f CMakeCache.txt mysql [ERROR] Can’t open the mysql.plugin table. Please run mysql_upgrade to create it. 123mysqld_safe Starting mysqld daemon with databases from /usr/local/mysql/dataKilledmysqld_safe mysqld from pid file /usr/local/mysql/data/vagrant-ubuntu-trusty-64.pid ended Remove the data directory and 1234567$ rm /etc/mysql/my.cnf$ rm -rf /data/mysql/$ mkdir -p /data/mysql $ chown -R mysql:mysql /data/mysql$ cd /usr/local/mysql/$ scripts/mysql_install_db --user=mysql --datadir=/data/mysql$ service mysqld restart configure: error: xml2-config not found. Please check your libxml2 installation. 1$ sudo apt-get install -y libxml2-dev configure: error: Please reinstall the libcurl distribution 1$ sudo apt-get install -y libcurl4-gnutls-dev configure: error: jpeglib.h not found. 1$ sudo apt-get install -y libjpeg-dev configure: error: png.h not found. 1$ sudo apt-get install -y libpng-dev configure: error: freetype.h not found. 1$ sudo apt-get install -y freetype-dev If you have installed this. Find out the freetype.sh realy path and change the configure file. 1234$ ls /usr/include/freetype2/freetype.h/usr/include/freetype2/freetype.h$ vim php-5.5.5/configure 123456789if test &quot;$PHP_FREETYPE_DIR&quot; != &quot;no&quot;; then for i in $PHP_FREETYPE_DIR /usr/local /usr; do if test -f &quot;$i/include/freetype2/freetype/freetype.h&quot;; then FREETYPE2_DIR=$i FREETYPE2_INC_DIR=$i/include/freetype2 break fi done Change if test -f &quot;$i/include/freetype2/freetype/freetype.h&quot;; to if test -f &quot;$i/include/freetype2/freetype.h&quot;; configure: error: Cannot find OpenSSL’s libraries 123$ find / -name libssl.so /usr/lib/x86_64-linux-gnu/libssl.so$ sudo ln -s /usr/lib/x86_64-linux-gnu/libssl.so /usr/lib","link":"/2017/04/04/Compile-and-install-LAMP-in-Ubuntu/"},{"title":"How Mongodb Build The Replica Set","text":"Server environment12345$ cat /etc/issue Ubuntu 14.04.5 LTS$ mongod -versiondb version v3.0.15 Replica setReplication is a process of synchronizing data across multiple servers. mongodb support replica set and master-slave copy, master-slave copy of the official is no longer recommended (does not support automatic failover) Migrate malfunctionReplica set can automatically failover. If primary is dropped or no response, but the most of replica members can link to each other, the new primary will be voted.In most cases, when the primary is dropped, unavailable or is not suitable for primary,failover will be automatically start in few seconds without admin intervention. There are few reasons if mongodb cannot migrate malfunction: The number of remainning members is less than half of the replca set no eligible primary member Deployment strategyThe minimum replica set recommended for three members of the collection,one for primary and other two for secondary. If there are more than three members in the replica set, it must follow these conditions: 1、There are an odd number of members participating in the voting. If there are even voting members, deploy an arbiter to change the number to an odd number. 2、After version 3.0.0 , the max node number is 50. It’s 12 before. 3、If you do not want some members to become the primary, just set their priority to zero. Ready for database123270172701827019 prepare the ports 27017、27018、27019 Stop all mongod1$ sudo service mongod stop Create database directory1$ sudo mkdir -p /data/mongodb/log /data/mongodb/rs0-0 /data/mongodb/rs0-1 /data/mongodb/rs0-2 Dir rs0-0,rs0-1,rs0-2 used to store the database file. Create mongodb instance123$ sudo mongod --port 27017 --dbpath /data/mongodb/rs0-0 --replSet rs0 --smallfiles --oplogSize 128 --fork --logpath=/data/mongodb/log/271017.log$ sudo mongod --port 27018 --dbpath /data/mongodb/rs0-1 --replSet rs0 --smallfiles --oplogSize 128 --fork --logpath=/data/mongodb/log/271018.log$ sudo mongod --port 27019 --dbpath /data/mongodb/rs0-2 --replSet rs0 --smallfiles --oplogSize 128 --fork --logpath=/data/mongodb/log/271019.log Create three nodes belong the replica set named rs0, and the data directory for each node is specified by ‘-dbpath’.‘-smallfiles’ and ‘-oplogSize’ reduce the amount of disk space used by each mongod instance. Mongodb also provides a daemon background way to start. Just add a ‘–fork’, but it must have the ‘–logpath’ parameter. The unit of ‘–oplogsize’ is M Connect to database1$ mongo --port 27017 Init replica set12345678910111213141516171819&gt; rsconf = { &quot;_id&quot; : &quot;rs0&quot;, &quot;members&quot; : [ { &quot;_id&quot; : 0, &quot;host&quot; : &quot;127.0.0.1:27017&quot; }, { &quot;_id&quot; : 1, &quot;host&quot; : &quot;127.0.0.1:27018&quot; }, { &quot;_id&quot; : 2, &quot;host&quot; : &quot;127.0.0.1:27019&quot; } ] }&gt; rs.initiate( rsconf ){ &quot;ok&quot; : 1 } Check config result123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657rs0:OTHER&gt; rs.conf(){ &quot;_id&quot; : &quot;rs0&quot;, &quot;version&quot; : 1, &quot;members&quot; : [ { &quot;_id&quot; : 0, &quot;host&quot; : &quot;127.0.0.1:27017&quot;, &quot;arbiterOnly&quot; : false, &quot;buildIndexes&quot; : true, &quot;hidden&quot; : false, &quot;priority&quot; : 1, &quot;tags&quot; : { }, &quot;slaveDelay&quot; : 0, &quot;votes&quot; : 1 }, { &quot;_id&quot; : 1, &quot;host&quot; : &quot;127.0.0.1:27018&quot;, &quot;arbiterOnly&quot; : false, &quot;buildIndexes&quot; : true, &quot;hidden&quot; : false, &quot;priority&quot; : 1, &quot;tags&quot; : { }, &quot;slaveDelay&quot; : 0, &quot;votes&quot; : 1 }, { &quot;_id&quot; : 2, &quot;host&quot; : &quot;127.0.0.1:27019&quot;, &quot;arbiterOnly&quot; : false, &quot;buildIndexes&quot; : true, &quot;hidden&quot; : false, &quot;priority&quot; : 1, &quot;tags&quot; : { }, &quot;slaveDelay&quot; : 0, &quot;votes&quot; : 1 } ], &quot;settings&quot; : { &quot;chainingAllowed&quot; : true, &quot;heartbeatTimeoutSecs&quot; : 10, &quot;getLastErrorModes&quot; : { }, &quot;getLastErrorDefaults&quot; : { &quot;w&quot; : 1, &quot;wtimeout&quot; : 0 } }} Check replica set status1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253rs0:PRIMARY&gt; rs.status(){ &quot;set&quot; : &quot;rs0&quot;, &quot;date&quot; : ISODate(&quot;2017-03-23T08:23:05.564Z&quot;), &quot;myState&quot; : 1, &quot;members&quot; : [ { &quot;_id&quot; : 0, &quot;name&quot; : &quot;127.0.0.1:27017&quot;, &quot;health&quot; : 1, &quot;state&quot; : 1, &quot;stateStr&quot; : &quot;PRIMARY&quot;, &quot;uptime&quot; : 144, &quot;optime&quot; : Timestamp(1509438130, 1), &quot;optimeDate&quot; : ISODate(&quot;2017-03-23T08:22:10Z&quot;), &quot;electionTime&quot; : Timestamp(1509438134, 1), &quot;electionDate&quot; : ISODate(&quot;2017-03-23T08:22:14Z&quot;), &quot;configVersion&quot; : 1, &quot;self&quot; : true }, { &quot;_id&quot; : 1, &quot;name&quot; : &quot;127.0.0.1:27018&quot;, &quot;health&quot; : 1, &quot;state&quot; : 2, &quot;stateStr&quot; : &quot;SECONDARY&quot;, &quot;uptime&quot; : 54, &quot;optime&quot; : Timestamp(1509438130, 1), &quot;optimeDate&quot; : ISODate(&quot;2017-03-23T08:22:10Z&quot;), &quot;lastHeartbeat&quot; : ISODate(&quot;2017-03-23T08:23:04.993Z&quot;), &quot;lastHeartbeatRecv&quot; : ISODate(&quot;2017-03-23T08:23:04.993Z&quot;), &quot;pingMs&quot; : 0, &quot;lastHeartbeatMessage&quot; : &quot;could not find member to sync from&quot;, &quot;configVersion&quot; : 1 }, { &quot;_id&quot; : 2, &quot;name&quot; : &quot;127.0.0.1:27019&quot;, &quot;health&quot; : 1, &quot;state&quot; : 2, &quot;stateStr&quot; : &quot;SECONDARY&quot;, &quot;uptime&quot; : 54, &quot;optime&quot; : Timestamp(1509438130, 1), &quot;optimeDate&quot; : ISODate(&quot;2017-03-23T08:22:10Z&quot;), &quot;lastHeartbeat&quot; : ISODate(&quot;2017-03-23T08:23:04.994Z&quot;), &quot;lastHeartbeatRecv&quot; : ISODate(&quot;2017-03-23T08:23:04.993Z&quot;), &quot;pingMs&quot; : 0, &quot;lastHeartbeatMessage&quot; : &quot;could not find member to sync from&quot;, &quot;configVersion&quot; : 1 } ], &quot;ok&quot; : 1} It’s done of building replica set! Check the syncInsert data into the Primary （127.0.0.1:27017）1234567891011$ mongo --port 27017$ show dbslocal 0.000GB$ use app$ db.test.insert({&quot;name&quot;:&quot;test data&quot;,&quot;value&quot;:1111})$ db.test.find(){ &quot;_id&quot; : ObjectId(&quot;57382a2641cff36fe7f3fc30&quot;), &quot;name&quot; : &quot;test data&quot;, &quot;value&quot; : 1111 } New a terminal connect12345678$ mongo --port 27018$ use app$ db.getMongo().setSlaveOk()$ db.test.find(){ &quot;_id&quot; : ObjectId(&quot;57382a2641cff36fe7f3fc30&quot;), &quot;name&quot; : &quot;test data&quot;, &quot;value&quot; : 1111 } It cannot use ‘db.test.find()’ to get data. Mongodb reads and writes data on primary node default, it does not allow read on replica node but can be set solve this problem. ok,data has been synchronized! Check for automatic failoverShutdown the main node Primary:12345678910111213$ mongo --port 27017$ use adminswitched to db admin$ db.shutdownServer()2017-03-23T08:24:40.887+0000 I NETWORK DBClientCursor::init call() failedserver should be down...2017-03-23T08:24:40.891+0000 I NETWORK trying reconnect to 127.0.0.1:27017 (127.0.0.1) failed2017-03-23T08:24:40.892+0000 I NETWORK reconnect 127.0.0.1:27017 (127.0.0.1) ok2017-03-23T08:24:41.066+0000 I NETWORK Socket recv() errno:104 Connection reset by peer 127.0.0.1:270172017-03-23T08:24:41.067+0000 I NETWORK SocketException: remote: 127.0.0.1:27017 error: 9001 socket exception [RECV_ERROR] server [127.0.0.1:27017] 2017-03-23T08:24:41.068+0000 I NETWORK DBClientCursor::init call() failed Entry another node:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354$ mongo --port 27018rs0:PRIMARY&gt; rs.status(){ &quot;set&quot; : &quot;rs0&quot;, &quot;date&quot; : ISODate(&quot;2017-03-23T08:26:22.753Z&quot;), &quot;myState&quot; : 1, &quot;members&quot; : [ { &quot;_id&quot; : 0, &quot;name&quot; : &quot;127.0.0.1:27017&quot;, &quot;health&quot; : 0, &quot;state&quot; : 8, &quot;stateStr&quot; : &quot;(not reachable/healthy)&quot;, &quot;uptime&quot; : 0, &quot;optime&quot; : Timestamp(0, 0), &quot;optimeDate&quot; : ISODate(&quot;1970-01-01T00:00:00Z&quot;), &quot;lastHeartbeat&quot; : ISODate(&quot;2017-03-23T08:26:21.186Z&quot;), &quot;lastHeartbeatRecv&quot; : ISODate(&quot;2017-03-23T08:24:39.063Z&quot;), &quot;pingMs&quot; : 0, &quot;lastHeartbeatMessage&quot; : &quot;Failed attempt to connect to 127.0.0.1:27017; couldn&apos;t connect to server 127.0.0.1:27017 (127.0.0.1), connection attempt failed&quot;, &quot;configVersion&quot; : -1 }, { &quot;_id&quot; : 1, &quot;name&quot; : &quot;127.0.0.1:27018&quot;, &quot;health&quot; : 1, &quot;state&quot; : 1, &quot;stateStr&quot; : &quot;PRIMARY&quot;, &quot;uptime&quot; : 312, &quot;optime&quot; : Timestamp(1509438230, 2), &quot;optimeDate&quot; : ISODate(&quot;2017-03-23T08:23:50Z&quot;), &quot;electionTime&quot; : Timestamp(1509438281, 1), &quot;electionDate&quot; : ISODate(&quot;2017-03-23T08:24:41Z&quot;), &quot;configVersion&quot; : 1, &quot;self&quot; : true }, { &quot;_id&quot; : 2, &quot;name&quot; : &quot;127.0.0.1:27019&quot;, &quot;health&quot; : 1, &quot;state&quot; : 2, &quot;stateStr&quot; : &quot;SECONDARY&quot;, &quot;uptime&quot; : 249, &quot;optime&quot; : Timestamp(1509438230, 2), &quot;optimeDate&quot; : ISODate(&quot;2017-03-23T08:23:50Z&quot;), &quot;lastHeartbeat&quot; : ISODate(&quot;2017-03-23T08:26:21.134Z&quot;), &quot;lastHeartbeatRecv&quot; : ISODate(&quot;2017-03-23T08:26:21.134Z&quot;), &quot;pingMs&quot; : 0, &quot;configVersion&quot; : 1 } ], &quot;ok&quot; : 1} The node 127.0.0.1:27017 here can’t be reachable.The node 127.0.0.1:27018 has been selected to be the new primary node.So automatic failover is successful. Add a new node12$ sudo mkdir -p /data/mongodb/rs0-3$ sudo mongod --port 27020 --dbpath /data/mongodb/rs0-3 --replSet rs0 --smallfiles --oplogSize 128 --fork --logpath=/data/mongodb/log/271020.log Entry the primary node1$ mongo --port 27018 123rs0:PRIMARY&gt; rs.add(&quot;127.0.0.1:27020&quot;){ &quot;ok&quot; : 1 }2017-03-23T08:30:54.755+0000 I NETWORK reconnect 127.0.0.1:27018 (127.0.0.1) ok We can see that node has been added success. Remove a node12rs0:PRIMARY&gt; rs.remove(&quot;127.0.0.1:27020&quot;){ &quot;ok&quot; : 1 } Manually change the primary nodeTo make the node ‘s priority become the maximum, this node will become the primary node: 12345678910rs0:PRIMARY&gt; conf = rs.conf(){ ...}rs0:PRIMARY&gt; conf.members[3].priority = 22rs0:PRIMARY&gt; rs.reconfig(conf)","link":"/2017/03/22/How-Mongodb-Build-The-Replica-Set/"},{"title":"How to implement login form by Symfony3","text":"My computer develop environment123456789101112Ubuntu 16.04.1 LTSPHP 7.0.15-0ubuntu0.16.04.4 (cli) ( NTS )Symfony Installer version 1.5.9Composer version 1.3.2 2017-01-27 18:23:41mysql&gt; select version();+-----------+| version() |+-----------+| 5.7.12 |+-----------+ Create a Symfony project1$ composer create-project symfony/framework-standard-edition login 3.2 -vvv project name is “login” then wait for symfony download and init this project Downloading Symfony…1191 KiB/5.3 MiB ▓▓░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 3% then you will get a project structure such as12345678910111213141516login/├── app├── bin├── composer.json├── composer.lock├── phpunit.xml.dist├── README.md├── src│ └── AppBundle│ ├── AppBundle.php│ └── Controller│ └── DefaultController.php├── tests├── var├── vendor└── web Config the database1$ vim app/config/parameters.yml 1234567891011# This file is auto-generated during the composer installparameters: database_host: 127.0.0.1 database_port: null database_name: symfony database_user: root database_password: root_pwd mailer_transport: smtp mailer_host: 127.0.0.1 mailer_user: null mailer_password: null you should makesure your database connection information is correct in this file. Create database in mysql by symfony command tool1$ cd login; php bin/console doctrine:database:create check database in mysql and you can see a new database named symfony which based on your config file app/config/config.yml, it loaded the other config file app/config/parameters.yml1234567mysql&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || symfony |+--------------------+ Create user_info entity and user_auth entity class1$ php bin/console doctrine:generate:entity --entity AppBundle:User 12345678910The Entity shortcut name [AppBundle:User]:Configuration format (yml, xml, php, or annotation) [annotation]: New field name (press &lt;return&gt; to stop adding fields): truenameField type [string]: Field length [255]: Is nullable [false]: Unique [false]: New field name (press &lt;return&gt; to stop adding fields): Entity generation 1$ php bin/console doctrine:generate:entity --entity AppBundle:UserAuth 12345678910111213141516171819202122The Entity shortcut name [AppBundle:UserAuth]:Configuration format (yml, xml, php, or annotation) [annotation]: New field name (press &lt;return&gt; to stop adding fields): userIdField type [string]: integerIs nullable [false]: Unique [false]: New field name (press &lt;return&gt; to stop adding fields): identifierField type [string]: Field length [255]: Is nullable [false]: Unique [false]: New field name (press &lt;return&gt; to stop adding fields): credentialField type [string]: Field length [255]: Is nullable [false]: Unique [false]: New field name (press &lt;return&gt; to stop adding fields): Entity generation After that you can see the directory structure was changed.Entity class with mapping information in Symfony help doctrine to build data table in database.After doctrine schema update, doctrine can automatically create all database tables for every entity.Repository was generated for the Doctrine ORM. 1234567891011./src/└── AppBundle ├── AppBundle.php ├── Controller │ └── DefaultController.php ├── Entity │ ├── UserAuth.php │ └── User.php └── Repository ├── UserAuthRepository.php └── UserRepository.php Create the database table based on entity class1$ php bin/console doctrine:schema:update --force 1234567mysql&gt; show tables;+-------------------+| Tables_in_symfony |+-------------------+| user || user_auth |+-------------------+ After than we can make a web api method to create a user account instead of a register form (i’m just want to simplify this tutorial) Create a Method to sign up my user account1$ vim src/AppBundle/Controller/UserController.php 12345678910111213141516171819202122232425262728293031323334353637383940414243namespace AppBundle\\Controller;use AppBundle\\Entity\\User;use AppBundle\\Entity\\UserAuth;use Sensio\\Bundle\\FrameworkExtraBundle\\Configuration\\Route;use Symfony\\Bundle\\FrameworkBundle\\Controller\\Controller;use Symfony\\Component\\HttpFoundation\\Response;class UserController extends Controller{ /** * @Route(\"/user/create\",name=\"user_create\") */ public function createAction() { $em = $this-&gt;getDoctrine()-&gt;getManager(); $username = \"useraccount\"; $password = \"userpassword\"; $User = new User(); $User-&gt;setTruename($username); $em-&gt;persist($User); $em-&gt;flush(); $UserAuth = new UserAuth(); $UserAuth-&gt;setUserId($User-&gt;getId()); $UserAuth-&gt;setIdentifier($username); $password = $this-&gt;get('security.password_encoder') -&gt;encodePassword(new \\AppBundle\\Security\\UserAuth($UserAuth), $password); $UserAuth-&gt;setCredential($password); $em-&gt;persist($UserAuth); $em-&gt;flush(); return new Response(\"success~\"); }} Here, we mapping this controller to “/user/create” route. This method will save a user info and a user auth info. At line 34, you can see how Symfony encodered password and we must create a security userauth class to finish this job. 12$ mkdir -p src/AppBundle/Security$ vim src/AppBundle/Security/UserAuth.php 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778namespace AppBundle\\Security;use Symfony\\Component\\Security\\Core\\User\\UserInterface;class UserAuth implements UserInterface{ private $UserAuth; public function __construct(\\AppBundle\\Entity\\UserAuth $UserAuth) { $this-&gt;UserAuth = $UserAuth; } /** * Returns the roles granted to the user. * * &lt;code&gt; * public function getRoles() * { * return array('ROLE_USER'); * } * &lt;/code&gt; * * Alternatively, the roles might be stored on a ``roles`` property, * and populated in any number of different ways when the user object * is created. * * @return (Role|string)[] The user roles */ public function getRoles() { return ['ROLE_ADMIN']; } /** * Returns the password used to authenticate the user. * * This should be the encoded password. On authentication, a plain-text * password will be salted, encoded, and then compared to this value. * * @return string The password */ public function getPassword() { return $this-&gt;UserAuth-&gt;getCredential(); } /** * Returns the salt that was originally used to encode the password. * * This can return null if the password was not encoded using a salt. * * @return string|null The salt */ public function getSalt() { } /** * Returns the username used to authenticate the user. * * @return string The username */ public function getUsername() { return $this-&gt;UserAuth-&gt;getIdentifier(); } /** * Removes sensitive data from the user. * * This is important if, at any given point, sensitive information like * the plain-text password is stored on this object. */ public function eraseCredentials() { }} The key named ‘ROLE_ADMIN’ in method getRoles() approved this Class can be accessed through the Symfony security roles. Then, update the configuration file app/config/security.yml and tell Symfony security component which encryption to be used. 1$ vim app/config/security.yml 1234security: ... encoders: AppBundle\\Security\\UserAuth: bcrypt After the visit http://localhost:8000/user/create, it will be added a user name ‘useraccount’ and password is ‘userpassword’ in database.Of course, the password is stored in encrypted mode. 123456789101112mysql&gt; select * from user;+----+-------------+| id | truename |+----+-------------+| 3 | useraccount |+----+-------------+mysql&gt; select * from user_auth;+----+-------------+--------------------------------------------------------------+--------+| id | identifier | credential | userId |+----+-------------+--------------------------------------------------------------+--------+| 3 | useraccount | $2y$13$dLcFHGIKXZ2gl8VbVchD3OpiSVw923DFcyVaiDVnzf9d0tx5nyUny | 3 |+----+-------------+--------------------------------------------------------------+--------+ Create a page that required login to access1$ vim src/AppBundle/Controller/UserController.php 123456789101112class UserController extends Controller{ ... /** * @Route(\"/user/list\",name=\"user_list\") */ public function listAction() { return new Response(\"user list ~~~\"); }} Authentication is required when accessing “/user/list”1$ vim app/config/security.yml 1234security: ... access_control: - { path: ^/user/list, roles: ROLE_ADMIN } Tell the firewall in Symfony security component to be verified by account using the way to submit a password form12345678910111213141516 providers:# in_memory:# memory: ~ webservice: id: app.webservice_user_provider firewalls: ... main: anonymous: ~# http_basic: ~ form_login: login_path: login check_path: login use_referer: true Pay attention to line 10, i’m using the “form_login” not “http_basic: ~”, you can customize the page for your login form in this way. “login_path”: controller for the route redirect to “/login” 1$ vim src/AppBundle/Controller/SecurityController.php 1234567891011121314151617181920212223242526272829303132333435namespace AppBundle\\Controller;use Sensio\\Bundle\\FrameworkExtraBundle\\Configuration\\Route;use Symfony\\Bundle\\FrameworkBundle\\Controller\\Controller;use Symfony\\Component\\HttpFoundation\\Request;class SecurityController extends Controller{ /** * @Route(\"/login\",name=\"login\") * @param Request $request * @return \\Symfony\\Component\\HttpFoundation\\Response */ public function loginAction(Request $request) { $authenticationUtils = $this-&gt;get('security.authentication_utils'); // get the login error if there is one $error = $authenticationUtils-&gt;getLastAuthenticationError(); // last username entered by the user $lastUsername = $authenticationUtils-&gt;getLastUsername(); return $this-&gt;render( 'security/login.html.twig', array( // last username entered by the user 'last_username' =&gt; $lastUsername, 'error' =&gt; $error, ) ); }} Just copy this contents in this file and you don’t need any of modification. Create a Login form12$ mkdir -p app/Resources/views/security/$ vim app/Resources/views/security/login.html.twig 12345678910111213141516171819{% if error %} &lt;div&gt;{{ error.messageKey|trans(error.messageData, 'security') }}&lt;/div&gt;{% endif %}&lt;form action=\"{{ path('login') }}\" method=\"post\"&gt; &lt;label for=\"username\"&gt;Username:&lt;/label&gt; &lt;input type=\"text\" id=\"username\" name=\"_username\" value=\"{{ last_username }}\" /&gt; &lt;label for=\"password\"&gt;Password:&lt;/label&gt; &lt;input type=\"password\" id=\"password\" name=\"_password\" /&gt; {# If you want to control the URL the user is redirected to on success (more details below) &lt;input type=\"hidden\" name=\"_target_path\" value=\"/account\" /&gt; #} &lt;button type=\"submit\"&gt;login&lt;/button&gt;&lt;/form&gt; Symfony needs “providers” to check the credential submitted in form. Create a user auth service for identifying use loginThe name of this service is the same as the id of providers in security.yml. Arguments in this service which offered Symfony doctrine entity manager is the only place missing in official documents. Through passing this parameter to “UserAuthProvider”, user can be checked exist or not in database in “loadUserByUsername” method 1$ vim app/config/services.yml 12345services: app.webservice_user_provider: class: AppBundle\\Security\\UserAuthProvider arguments: - &quot;@doctrine.orm.entity_manager&quot; Create identify provider for password checking1$ vim src/AppBundle/Security/UserAuthProvider.php 123456789101112131415161718192021222324252627282930313233343536373839404142434445namespace AppBundle\\Security;use AppBundle\\Entity\\UserAuth;use Doctrine\\ORM\\EntityManager;use Symfony\\Component\\Security\\Core\\Exception\\UnsupportedUserException;use Symfony\\Component\\Security\\Core\\Exception\\UsernameNotFoundException;use Symfony\\Component\\Security\\Core\\User\\UserInterface;use Symfony\\Component\\Security\\Core\\User\\UserProviderInterface;class UserAuthProvider implements UserProviderInterface{ /** * @var EntityManager */ protected $dm; public function __construct(EntityManager $dm) { $this-&gt;dm = $dm; } public function loadUserByUsername($username) { /** @var UserAuth $UserAuth */ $UserAuth = $this-&gt;dm-&gt;getRepository('AppBundle:UserAuth')-&gt;findOneBy(['identifier' =&gt; $username]); if (!$UserAuth) { throw new UsernameNotFoundException(\"Username \\\"{$username}\\\" does not exist.\"); } return new \\AppBundle\\Security\\UserAuth($UserAuth); } public function refreshUser(UserInterface $User) { if (!$User instanceof \\AppBundle\\Security\\UserAuth) { throw new UnsupportedUserException('Instances of \"' . get_class($User) . '\" are not supported.'); } return $this-&gt;loadUserByUsername($User-&gt;getUsername()); } public function supportsClass($class) { return $class === 'AppBundle\\Entity\\UserAuth'; }} OK, after input account and password in http://localhost:8000/user/list ,login success information will be right here!","link":"/2017/03/21/How-to-implement-login-form-by-Symfony3/"},{"title":"Mysql data optimization","text":"Server environment1234567891011121314$ cat /etc/issue Ubuntu 14.04.5 LTS $ free -h total used free shared buffers cachedMem: 7.8G 7.7G 132M 260K 276K 6.8M-/+ buffers/cache: 7.7G 139MSwap: 8.0G 2.3G 5.7G $ mysqld -Vmysqld Ver 5.5.58-0ubuntu0.14.04.1 for debian-linux-gnu on x86_64 ((Ubuntu))$ php -vPHP 7.1.12 Fake DataThere is no transaction operate, the table’s engine will use the myisam type. Create mysql table 123456789101112131415161718192021222324CREATE TABLE `users` ( `id` int(10) NOT NULL AUTO_INCREMENT, `name` varchar(191) COLLATE utf8mb4_unicode_ci NOT NULL, `email` varchar(50) COLLATE utf8mb4_unicode_ci NOT NULL, `emailPrefix` varchar(50) COLLATE utf8mb4_unicode_ci NOT NULL, `password` varchar(255) COLLATE utf8mb4_unicode_ci DEFAULT NULL, `age` int(11) NOT NULL, `account` varchar(50) COLLATE utf8mb4_unicode_ci NOT NULL, `address` varchar(50) COLLATE utf8mb4_unicode_ci NOT NULL, `country` varchar(50) COLLATE utf8mb4_unicode_ci NOT NULL, `city` varchar(50) COLLATE utf8mb4_unicode_ci NOT NULL, `streetName` varchar(50) COLLATE utf8mb4_unicode_ci NOT NULL, `streetAddress` varchar(50) COLLATE utf8mb4_unicode_ci NOT NULL, `sex` int(11) NOT NULL, `job` varchar(50) COLLATE utf8mb4_unicode_ci NOT NULL, `income` double(8,2) NOT NULL, `deposit` int(11) NOT NULL, `phone` varchar(50) COLLATE utf8mb4_unicode_ci NOT NULL, `company` varchar(50) COLLATE utf8mb4_unicode_ci NOT NULL, `about` text COLLATE utf8mb4_unicode_ci NOT NULL, `friendsNum` int(11) NOT NULL, `created_at` timestamp NULL DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=MyISAM AUTO_INCREMENT=1000 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci; Using php script to insert datas1234$ mkdir fakeData;cd fakeData$ composer init$ composer require catfan/Medoo$ composer require fzaninotto/faker 1$ vim main.php 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162&lt;?phprequire 'vendor/autoload.php';use Medoo\\Medoo;$faker = Faker\\Factory::create();$database = new Medoo([ 'database_type' =&gt; 'mysql', 'database_name' =&gt; 'app', 'server' =&gt; '127.0.0.1', 'username' =&gt; 'root', 'password' =&gt; 'password', 'port' =&gt; '3306',]);$emails = [ \"@gmail.com\",\"@qq.com\",\"@163.com\",\"@126.com\", \"@hotmail.com\",\"@outlook.com\",\"@foxmail.com\", \"@sina.com\",\"@sohu.com\",\"@yeah.net\", \"@189.com\",\"@139.com\",\"@21cn.com\",\"@189.com\",];for ($j = 0; $j &lt; 2050; $j++) { try { echo date('Y-m-d H:i:s') . PHP_EOL; $usersData = []; for ($i = 0; $i &lt; 5000; $i++) { $username = $faker-&gt;name . ' ' . $faker-&gt;userName . ' ' . $faker-&gt;firstName; $emailPrefix = $emails[$faker-&gt;numberBetween(0, count($emails) - 1)]; $email = str_replace([' ', '.'], '_', $username) . $emailPrefix; $data = [ 'name' =&gt; $username, 'email' =&gt; $email, 'emailPrefix' =&gt; str_replace('@','',$emailPrefix), 'password' =&gt; \"123456\", 'age' =&gt; $faker-&gt;numberBetween(10, 120), //年龄 'account' =&gt; $faker-&gt;firstName . '-' . $faker-&gt;name . '-' . $username, //账户 'address' =&gt; $faker-&gt;address, 'country' =&gt; $faker-&gt;country, 'city' =&gt; $faker-&gt;city, 'streetName' =&gt; $faker-&gt;streetName, 'streetAddress' =&gt; $faker-&gt;streetAddress, 'sex' =&gt; rand(0, 1), 'job' =&gt; $faker-&gt;jobTitle, 'income' =&gt; $faker-&gt;numberBetween(3000, 300000), 'deposit' =&gt; $faker-&gt;numberBetween(500, 9000000), 'phone' =&gt; $faker-&gt;phoneNumber, 'company' =&gt; $faker-&gt;companySuffix . ' ' . $faker-&gt;company, 'about' =&gt; $faker-&gt;text, 'friendsNum'=&gt;$faker-&gt;numberBetween(3, 1000), 'created_at'=&gt;$faker-&gt;dateTimeThisYear('+1 year')-&gt;format('Y-m-d H:i:s'), ]; $usersData[] = $data; } echo date('Y-m-d H:i:s') . PHP_EOL; $database-&gt;insert(\"users\", $usersData); echo date('Y-m-d H:i:s') . PHP_EOL; echo PHP_EOL; } catch (Exception $e) { echo $e-&gt;getMessage() . PHP_EOL; }}echo \"END \\n\"; Check slow logOpen slow query log123456789101112mysql&gt; set global slow_query_log=on;Query OK, 0 rows affected (0.00 sec)mysql&gt; show variables like &apos;slow%&apos;;+---------------------+--------------------------------------------------+| Variable_name | Value |+---------------------+--------------------------------------------------+| slow_launch_time | 2 || slow_query_log | ON || slow_query_log_file | /var/lib/mysql/vagrant-ubuntu-trusty-64-slow.log |+---------------------+--------------------------------------------------+3 rows in set (0.00 sec) Execute a query without data index1234567mysql&gt; select count(id) from users where age &lt; 35 and age &gt; 30 and sex = 1 and income &gt; 10000 and country = &apos;Hong Kong&apos;;+-----------+| count(id) |+-----------+| 744 |+-----------+1 row in set (10.06 sec) Tail the slow query log1234567891011$ tail -f /var/lib/mysql/vagrant-ubuntu-trusty-64-slow.logTime Id Command Argument/usr/sbin/mysqld, Version: 5.5.58-0ubuntu0.14.04.1 ((Ubuntu)). started with:Tcp port: 3306 Unix socket: /var/run/mysqld/mysqld.sockTime Id Command Argument# Time: 171217 6:16:14# User@Host: root[root] @ localhost [127.0.0.1]# Query_time: 10.053302 Lock_time: 0.000211 Rows_sent: 1 Rows_examined: 10150175use app;select count(id) from users where age &lt; 35 and age &gt; 30 and sex = 1 and income &gt; 10000 and country = &apos;Hong Kong&apos;; Notice this info ‘Lock_time: 0.000211 Rows_sent: 1 Rows_examined: 10150175’Full scanning the whole table to get one record but locked the table 0.000211 Explain slow SQL query1234567mysql&gt; explain select count(id) from users where age &lt; 35 and age &gt; 30 and sex = 1 and income &gt; 10000 and country = &apos;Hong Kong&apos;;+----+-------------+-------+------+---------------+------+---------+------+----------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+------+---------------+------+---------+------+----------+-------------+| 1 | SIMPLE | users | ALL | NULL | NULL | NULL | NULL | 10150175 | Using where |+----+-------------+-------+------+---------------+------+---------+------+----------+-------------+1 row in set (0.00 sec) Query CacheMysql query cache is opened default and the default size is 16M.123$ cat /etc/mysql/my.cnf | grep query_cache query_cache_limit = 1M query_cache_size = 16M 123456789101112mysql&gt; show variables like &apos;%query_cache%&apos;;+------------------------------+----------+| Variable_name | Value |+------------------------------+----------+| have_query_cache | YES || query_cache_limit | 1048576 || query_cache_min_res_unit | 4096 || query_cache_size | 16777216 || query_cache_type | ON || query_cache_wlock_invalidate | OFF |+------------------------------+----------+6 rows in set (0.00 sec) Suitable for very low update frequency and read only high query frequency. Otherwise it is not recommend to use.Query cache size recommend to maintain with 100M size.It is strict required two sql to be exactly the same. SQL statement, database protocol version, character set and other factors will affect the results123mysql&gt; set names latin1; SELECT * FROM table_name;mysql&gt; set names latin1; select * from table_name;mysql&gt; set names utf8; select * from table_name; The most important, query cache is controlled by a global lock in mysql, which will be locked if updating the query cache block.For example , a query result is 20kb, and current ‘query_cache_min_res_unit’ setting is 4kb(the default value, adjustable), the the query results need to be written in query cache in five times.It becomes a high costs. The query cache is better for the reading operations. Closing methods are simple, there are two way: set the options query_cache_type = 0 和 query_cache_size = 0； if compiled with source code MySQL, compiling with parameters –without-query-cache Create Index Type Desc Index Allow to have same index UNIQUE Index Not allow to have same index, value can be NULL PRIMARY KEY Not allow to have same index, value can not be NULL, only one primary key in each table FULLTEXT Index only support in myisam and english language 1mysql&gt; ALTER TABLE users ADD INDEX country_age_income (country,age,sex,income); Re-query above sql1234567mysql&gt; explain select count(id) from users where age &lt; 35 and age &gt; 30 and sex = 1 and income &gt; 10000 and country = &apos;Hong Kong&apos;;+----+-------------+-------+-------+--------------------+--------------------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+-------+--------------------+--------------------+---------+------+------+-------------+| 1 | SIMPLE | users | range | country_age_income | country_age_income | 770 | NULL | 1025 | Using where |+----+-------------+-------+-------+--------------------+--------------------+---------+------+------+-------------+1 row in set (0.02 sec) You can see the use of index “key=country_age_income” 1234567mysql&gt; select count(id) from users where age &lt; 35 and age &gt; 30 and sex = 1 and income &gt; 10000 and country = &apos;Hong Kong&apos;;+-----------+| count(id) |+-----------+| 744 |+-----------+1 row in set (0.52 sec) Time from 10.06s shortened to 0.52s by index Optimize SQL queryPlease google search base on your business background Splitting Read operation in slave Using Database Proxy Mysql-router Kingshard 360 Atlas … Separate in code PHP framework can deal with this like Laravel,Symfony etc. If you want to do this by yoursef ,the code logic looks like this: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657&lt;?phpclass DB{ private $writeConn; private $readConns = []; function __construct() { $this-&gt;initWriteConn(); $this-&gt;initReadConns(); } private function initWriteConn() { $conn = mysql_connect('192.168.0.101:3306', 'root', '111111'); mysql_select_db('test'); mysql_query('set names utf8'); $this-&gt;writeConn = $conn; } private function initReadConns() { $conn1 = mysql_connect('192.168.0.105:3306', 'root', '111111'); mysql_select_db('users'); mysql_query('set names utf8'); $conn2 = mysql_connect('192.168.0.106:3306', 'root', '111111'); mysql_select_db('users'); mysql_query('set names utf8'); $this-&gt;readConns[] = $conn1; $this-&gt;readConns[] = $conn2; } public function query($sql) { $queryStr = trim($sql); $queryType = strtolower(substr($queryStr, 0, 6)); if ($queryType == 'select') { $result = $this-&gt;readQuery($queryStr); } else { $result = $this-&gt;writeQuery($queryStr); } return $result; }}$sql1 = \"select * from users\";$sql2 = \"insert into users (name) values ('Alice')\";$sql3 = \"delete from users where id = 5\";$sql4 = \"update users set name='Jerry' where id = 4\";$db = new DB();$result1 = $db-&gt;query($sql1);$result2 = $db-&gt;query($sql2);$result3 = $db-&gt;query($sql3);$result4 = $db-&gt;query($sql4); PartitionPlease read another blog About Mysql Partition Vertical Split TableBased on the relational database column to split table with lots of columns.Split the filed which will not be used usually or with long length into a extended table. Horizontal Split TableAccording to rules, different data row in the table will be distributed to different database tablesPlease read another blog Mysql Table Horizontal Split By Mycat Vertical Split DatabaseIn accordance with the business module to separate different databases Meet some errors Cannot allocate memory1234567$ php artisan db:seedmmap() failed: [12] Cannot allocate memorymmap() failed: [12] Cannot allocate memoryPHP Fatal error: Out of memory (allocated 2160070656) (tried to allocate 2147483656 bytes) in /home/liang/learnlaravel5/vendor/laravel/framework/src/Illuminate/Database/Eloquent/FactoryBuilder.php on line 194In FactoryBuilder.php line 194: Out of memory (allocated 2160070656) (tried to allocate 2147483656 bytes) Increase your system swap, here i will add 8G swap, adjust according to your actual situation! 123$ dd if=/dev/zero of=/mnt/swapfile bs=1M count=8192$ mkswap /mnt/swapfile$ swapon /mnt/swapfile This problem still appears. I’m using Laravel to create a lot of mysql recode and these data are created in memory. To fix this problem, code in Laravel seed must be changed. Maximum retries of 10000 reached without finding a unique value When using faker mock the email data, it seems that the max unique email number is 10 million. Mysql Command Tipsshow columns:1mysql&gt; desc {TABLE_NAME}; update table name:1mysql&gt; alter table {OLD_TABLE_NAME} rename to {NEW_TABLE_NAME}; add column:1mysql&gt; alter table {TABLE_NAME} add column {COLUMN_NAME} varchar(30); drop column:1mysql&gt; alter table {TABLE_NAME} drop column {COLUMN_NAME}; update column property: 1mysql&gt; alter table {TABLE_NAME} modify name varchar(22); show table size:1mysql&gt; select concat(round(sum(DATA_LENGTH/1024/1024),2),&apos;MB&apos;) as data from information_schema.TABLES WHERE TABLE_SCHEMA=&apos;{DATABASE_NAME}&apos; and TABLE_NAME = &apos;{TABLE_NAME}&apos;; update database default character:1mysql&gt; ALTER DATABASE {DATABASE_NAME} CHARACTER SET = utf8mb4 COLLATE = utf8mb4_unicode_ci;","link":"/2017/08/27/Mysql-data-optimization/"},{"title":"Operate on Elasticsearch 2.2","text":"Server environment12345678$ cat /etc/issue Ubuntu 14.04.5 LTS$ curl localhost:9200{ &quot;version&quot; : { &quot;number&quot; : &quot;2.2.0&quot;,... Concept Elasticsearch Relational DB Indices Databases Types Tables Documents Rows Fields Columns API RequestRequest format： 1curl -X &lt;VERB &apos;&lt;PROTOCOL://&lt;HOST:&lt;PORT/&lt;PATH?&lt;QUERY_STRING&apos; -d &apos;&lt;BODY&apos; Parameters Description VERB HTTP Method : GET、 POST、 PUT、 HEAD or DELETE PROTOCOL http or https HOST node names in the cluster PORT Port. Default is 9200 PATH API Path QUERY_STRING (Optional) Query string parameters BODY (Optional) Request JSON body Index document123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263$ curl -X PUT &apos;http://localhost:9200/company/employee/1?pretty&apos; -d &apos; { &quot;first_name&quot; : &quot;John&quot;, &quot;last_name&quot; : &quot;Smith&quot;, &quot;age&quot; : 25, &quot;about&quot; : &quot;I love swimming&quot;, &quot;interests&quot;: [ &quot;sports&quot;, &quot;music&quot; ] }&apos;{ &quot;_index&quot; : &quot;company&quot;, &quot;_type&quot; : &quot;employee&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_version&quot; : 1, &quot;_shards&quot; : { &quot;total&quot; : 2, &quot;successful&quot; : 1, &quot;failed&quot; : 0 }, &quot;created&quot; : true}$ curl -X PUT &apos;http://localhost:9200/company/employee/2?pretty&apos; -d &apos; { &quot;first_name&quot; : &quot;Jane&quot;, &quot;last_name&quot; : &quot;Smith&quot;, &quot;age&quot; : 32, &quot;about&quot; : &quot;I like shopping&quot;, &quot;interests&quot;: [ &quot;music&quot; ] }&apos;{ &quot;_index&quot; : &quot;company&quot;, &quot;_type&quot; : &quot;employee&quot;, &quot;_id&quot; : &quot;2&quot;, &quot;_version&quot; : 1, &quot;_shards&quot; : { &quot;total&quot; : 2, &quot;successful&quot; : 1, &quot;failed&quot; : 0 }, &quot;created&quot; : true}$ curl -X PUT &apos;http://localhost:9200/company/employee/3?pretty&apos; -d &apos; { &quot;first_name&quot; : &quot;Douglas&quot;, &quot;last_name&quot; : &quot;Fir&quot;, &quot;age&quot; : 35, &quot;about&quot;: &quot;I like cooking and swimming&quot;, &quot;interests&quot;: [ &quot;cooking&quot; ,&quot;swimming&quot;] }&apos;{ &quot;_index&quot; : &quot;company&quot;, &quot;_type&quot; : &quot;employee&quot;, &quot;_id&quot; : &quot;3&quot;, &quot;_version&quot; : 1, &quot;_shards&quot; : { &quot;total&quot; : 2, &quot;successful&quot; : 1, &quot;failed&quot; : 0 }, &quot;created&quot; : true} Find documentGet ID 1 Document：123456789101112131415$ curl -X GET &apos;http://localhost:9200/company/employee/1?pretty&apos;{ &quot;_index&quot; : &quot;company&quot;, &quot;_type&quot; : &quot;employee&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_version&quot; : 1, &quot;found&quot; : true, &quot;_source&quot; : { &quot;first_name&quot; : &quot;John&quot;, &quot;last_name&quot; : &quot;Smith&quot;, &quot;age&quot; : 25, &quot;about&quot; : &quot;I love swimming&quot;, &quot;interests&quot; : [ &quot;sports&quot;, &quot;music&quot; ] }} Search employee information whose last name is ‘Smith’：123456789101112131415161718192021222324252627282930313233343536373839$ curl -X GET &apos;http://localhost:9200/company/employee/_search?q=last_name:Smith&amp;pretty&apos;{ &quot;took&quot; : 97, &quot;timed_out&quot; : false, &quot;_shards&quot; : { &quot;total&quot; : 5, &quot;successful&quot; : 5, &quot;failed&quot; : 0 }, &quot;hits&quot; : { &quot;total&quot; : 2, &quot;max_score&quot; : 0.30685282, &quot;hits&quot; : [ { &quot;_index&quot; : &quot;company&quot;, &quot;_type&quot; : &quot;employee&quot;, &quot;_id&quot; : &quot;2&quot;, &quot;_score&quot; : 0.30685282, &quot;_source&quot; : { &quot;first_name&quot; : &quot;Jane&quot;, &quot;last_name&quot; : &quot;Smith&quot;, &quot;age&quot; : 32, &quot;about&quot; : &quot;I like shopping&quot;, &quot;interests&quot; : [ &quot;music&quot; ] } }, { &quot;_index&quot; : &quot;company&quot;, &quot;_type&quot; : &quot;employee&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 0.30685282, &quot;_source&quot; : { &quot;first_name&quot; : &quot;John&quot;, &quot;last_name&quot; : &quot;Smith&quot;, &quot;age&quot; : 25, &quot;about&quot; : &quot;I love swimming&quot;, &quot;interests&quot; : [ &quot;sports&quot;, &quot;music&quot; ] } } ] }} The search value must match exactly except case Search employee information using query expressions whose last name is ‘Smith’：12345678910111213141516171819202122232425262728293031323334353637383940414243444546$ curl -X GET &apos;http://localhost:9200/company/employee/_search?pretty&apos; -d &apos; { &quot;query&quot; : { &quot;match&quot; : { &quot;last_name&quot; : &quot;smith&quot; } } }&apos;{ &quot;took&quot; : 2, &quot;timed_out&quot; : false, &quot;_shards&quot; : { &quot;total&quot; : 5, &quot;successful&quot; : 5, &quot;failed&quot; : 0 }, &quot;hits&quot; : { &quot;total&quot; : 2, &quot;max_score&quot; : 0.30685282, &quot;hits&quot; : [ { &quot;_index&quot; : &quot;company&quot;, &quot;_type&quot; : &quot;employee&quot;, &quot;_id&quot; : &quot;2&quot;, &quot;_score&quot; : 0.30685282, &quot;_source&quot; : { &quot;first_name&quot; : &quot;Jane&quot;, &quot;last_name&quot; : &quot;Smith&quot;, &quot;age&quot; : 32, &quot;about&quot; : &quot;I like shopping&quot;, &quot;interests&quot; : [ &quot;music&quot; ] } }, { &quot;_index&quot; : &quot;company&quot;, &quot;_type&quot; : &quot;employee&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 0.30685282, &quot;_source&quot; : { &quot;first_name&quot; : &quot;John&quot;, &quot;last_name&quot; : &quot;Smith&quot;, &quot;age&quot; : 25, &quot;about&quot; : &quot;I love swimming&quot;, &quot;interests&quot; : [ &quot;sports&quot;, &quot;music&quot; ] } } ] }} The search value must match exactly except case Search employee information whose last name is ‘Smith’ and order than 30：12345678910111213141516171819202122232425262728293031323334353637383940414243$ curl -X GET &apos;http://localhost:9200/company/employee/_search?pretty&apos; -d &apos; { &quot;query&quot; : { &quot;bool&quot; : { &quot;must&quot; : { &quot;match&quot; : { &quot;last_name&quot; : &quot;Smith&quot; } }, &quot;filter&quot;: { &quot;range&quot; : { &quot;age&quot; : { &quot;gt&quot; : 30 } } } } } }&apos;{ &quot;took&quot; : 6, &quot;timed_out&quot; : false, &quot;_shards&quot; : { &quot;total&quot; : 5, &quot;successful&quot; : 5, &quot;failed&quot; : 0 }, &quot;hits&quot; : { &quot;total&quot; : 1, &quot;max_score&quot; : 0.30685282, &quot;hits&quot; : [ { &quot;_index&quot; : &quot;company&quot;, &quot;_type&quot; : &quot;employee&quot;, &quot;_id&quot; : &quot;2&quot;, &quot;_score&quot; : 0.30685282, &quot;_source&quot; : { &quot;first_name&quot; : &quot;Jane&quot;, &quot;last_name&quot; : &quot;Smith&quot;, &quot;age&quot; : 32, &quot;about&quot; : &quot;I like shopping&quot;, &quot;interests&quot; : [ &quot;music&quot; ] } } ] }} The search value must match exactly except case Full Search employee information who likes swimming：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758$ curl -X GET &apos;http://localhost:9200/company/employee/_search?pretty&apos; -d &apos; { &quot;query&quot; : { &quot;match&quot; : { &quot;about&quot; : &quot;love swimming&quot; } }, &quot;highlight&quot; : { &quot;fields&quot; : { &quot;about&quot; : {} } } } }&apos;{ &quot;took&quot; : 42, &quot;timed_out&quot; : false, &quot;_shards&quot; : { &quot;total&quot; : 5, &quot;successful&quot; : 5, &quot;failed&quot; : 0 }, &quot;hits&quot; : { &quot;total&quot; : 2, &quot;max_score&quot; : 0.2169777, &quot;hits&quot; : [ { &quot;_index&quot; : &quot;company&quot;, &quot;_type&quot; : &quot;employee&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 0.2169777, &quot;_source&quot; : { &quot;first_name&quot; : &quot;John&quot;, &quot;last_name&quot; : &quot;Smith&quot;, &quot;age&quot; : 25, &quot;about&quot; : &quot;I love swimming&quot;, &quot;interests&quot; : [ &quot;sports&quot;, &quot;music&quot; ] }, &quot;highlight&quot; : { &quot;about&quot; : [ &quot;I &lt;em&gt;love&lt;/em&gt; &lt;em&gt;swimming&lt;/em&gt;&quot; ] } }, { &quot;_index&quot; : &quot;company&quot;, &quot;_type&quot; : &quot;employee&quot;, &quot;_id&quot; : &quot;3&quot;, &quot;_score&quot; : 0.019691018, &quot;_source&quot; : { &quot;first_name&quot; : &quot;Douglas&quot;, &quot;last_name&quot; : &quot;Fir&quot;, &quot;age&quot; : 35, &quot;about&quot; : &quot;I like cooking and swimming&quot;, &quot;interests&quot; : [ &quot;cooking&quot;, &quot;swimming&quot; ] }, &quot;highlight&quot; : { &quot;about&quot; : [ &quot;I like cooking and &lt;em&gt;swimming&lt;/em&gt;&quot; ] } } ] }} Replace “match” with “match_phrase” in above request can exactly matches the short word “swimming” Add the “highlight” parameter at the same level in “query” to tag matching keywords in the result with the &lt;/ em&gt; tag Aggregation analyze of employee interestsEnable the analysis of relate field first 1234567891011$ curl -X PUT &apos;http://localhost:9200/company/_mapping/employee?pretty&apos; -d &apos;{ &quot;properties&quot;: { &quot;interests&quot;: { &quot;type&quot;: &quot;string&quot; } }}&apos;{ &quot;acknowledged&quot; : true} Query the aggregation result12345678910111213141516171819202122232425262728293031323334353637383940414243$ curl -X GET &apos;http://localhost:9200/company/employee/_search?pretty&apos; -d &apos;{ &quot;aggs&quot;: { &quot;all_interests&quot;: { &quot;terms&quot;: { &quot;field&quot;: &quot;interests&quot; } } }}&apos;{ &quot;took&quot; : 95, &quot;timed_out&quot; : false, &quot;_shards&quot; : { &quot;total&quot; : 5, &quot;successful&quot; : 5, &quot;failed&quot; : 0 }, &quot;hits&quot; : { &quot;total&quot; : 3, &quot;max_score&quot; : 1.0, &quot;hits&quot; : [ ... ] }, &quot;aggregations&quot; : { &quot;all_interests&quot; : { &quot;doc_count_error_upper_bound&quot; : 0, &quot;sum_other_doc_count&quot; : 0, &quot;buckets&quot; : [ { &quot;key&quot; : &quot;music&quot;, &quot;doc_count&quot; : 2 }, { &quot;key&quot; : &quot;cooking&quot;, &quot;doc_count&quot; : 1 }, { &quot;key&quot; : &quot;sports&quot;, &quot;doc_count&quot; : 1 }, { &quot;key&quot; : &quot;swimming&quot;, &quot;doc_count&quot; : 1 } ] } }} Update documentUpdate the document which id is 2： 1234567891011121314151617181920$ curl -X PUT &apos;http://localhost:9200/company/employee/2?pretty&apos; -d &apos;{ &quot;first_name&quot; : &quot;Jane&quot;, &quot;last_name&quot; : &quot;Smith&quot;, &quot;age&quot; : 33, &quot;about&quot; : &quot;I like shopping ,dancing and music&quot;, &quot;interests&quot;: [ &quot;music&quot; ]}&apos;{ &quot;_index&quot; : &quot;company&quot;, &quot;_type&quot; : &quot;employee&quot;, &quot;_id&quot; : &quot;2&quot;, &quot;_version&quot; : 2, &quot;_shards&quot; : { &quot;total&quot; : 2, &quot;successful&quot; : 1, &quot;failed&quot; : 0 }, &quot;created&quot; : false} Delete document12345678910111213$ curl -X DELETE &apos;http://localhost:9200/company/employee/3?pretty&apos;{ &quot;found&quot; : true, &quot;_index&quot; : &quot;company&quot;, &quot;_type&quot; : &quot;employee&quot;, &quot;_id&quot; : &quot;3&quot;, &quot;_version&quot; : 3, &quot;_shards&quot; : { &quot;total&quot; : 2, &quot;successful&quot; : 1, &quot;failed&quot; : 0 }}","link":"/2017/04/05/Operate-on-Elasticsearch-2-2/"},{"title":"Operate on Mongodb 3.2","text":"Server environment12345$ mongoMongoDB shell version v3.4.10connecting to: mongodb://127.0.0.1:27017MongoDB server version: 3.2.17 Insert Document12&gt; db.blog.insert({&quot;title&quot;:&quot;mongo insert&quot;})WriteResult({ &quot;nInserted&quot; : 1 }) Batch Insert Document12345678910111213141516&gt; db.blog.insert([{&quot;title&quot;:&quot;mongo batch insert part 1&quot;},{&quot;title&quot;:&quot;mongo batch insert part 2&quot;},{&quot;title&quot;:&quot;mongo batch insert part 3&quot;}]) BulkWriteResult({ &quot;writeErrors&quot; : [ ], &quot;writeConcernErrors&quot; : [ ], &quot;nInserted&quot; : 3, &quot;nUpserted&quot; : 0, &quot;nMatched&quot; : 0, &quot;nModified&quot; : 0, &quot;nRemoved&quot; : 0, &quot;upserted&quot; : [ ] })&gt; db.blog.find(){ &quot;_id&quot; : ObjectId(&quot;5a202c00fb4c8a34cc1a4d81&quot;), &quot;title&quot; : &quot;mongo insert&quot; }{ &quot;_id&quot; : ObjectId(&quot;5a1fa36d4a00059e962af668&quot;), &quot;title&quot; : &quot;mongo batch insert part 1&quot; }{ &quot;_id&quot; : ObjectId(&quot;5a1fa36d4a00059e962af669&quot;), &quot;title&quot; : &quot;mongo batch insert part 2&quot; }{ &quot;_id&quot; : ObjectId(&quot;5a1fa36d4a00059e962af66a&quot;), &quot;title&quot; : &quot;mongo batch insert part 3&quot; } What about repeat insert?The record {“_id” : ObjectId(“5a1fa36d4a00059e962af668”} is exist, i’m going to insert a repeat id record now. 1234567891011121314151617181920212223242526&gt; db.blog.insert([{&quot;_id&quot; : ObjectId(&quot;5a1fa36d4a00059e962af668&quot;),&quot;title&quot;:&quot;mongo batch insert part 4&quot;},{&quot;title&quot;:&quot;mongo batch insert part 5&quot;}])BulkWriteResult({ &quot;writeErrors&quot; : [ { &quot;index&quot; : 0, &quot;code&quot; : 11000, &quot;errmsg&quot; : &quot;E11000 duplicate key error collection: test.blog index: _id_ dup key: { : ObjectId(&apos;5a1fa36d4a00059e962af668&apos;) }&quot;, &quot;op&quot; : { &quot;_id&quot; : ObjectId(&quot;5a1fa36d4a00059e962af668&quot;), &quot;title&quot; : &quot;mongo batch insert part 4&quot; } } ], &quot;writeConcernErrors&quot; : [ ], &quot;nInserted&quot; : 0, &quot;nUpserted&quot; : 0, &quot;nMatched&quot; : 0, &quot;nModified&quot; : 0, &quot;nRemoved&quot; : 0, &quot;upserted&quot; : [ ]})&gt; db.blog.find(){ &quot;_id&quot; : ObjectId(&quot;5a202c00fb4c8a34cc1a4d81&quot;), &quot;title&quot; : &quot;mongo insert&quot; }{ &quot;_id&quot; : ObjectId(&quot;5a1fa36d4a00059e962af668&quot;), &quot;title&quot; : &quot;mongo batch insert part 1&quot; }{ &quot;_id&quot; : ObjectId(&quot;5a1fa36d4a00059e962af669&quot;), &quot;title&quot; : &quot;mongo batch insert part 2&quot; }{ &quot;_id&quot; : ObjectId(&quot;5a1fa36d4a00059e962af66a&quot;), &quot;title&quot; : &quot;mongo batch insert part 3&quot; } Now we know the operate of batch insert will be break off if there is a duplicate key After mongodb version 3.2, the method batchInsert is deprecated InsertMany12345678910111213141516&gt; db.blog.insertMany([{&quot;title&quot;:&quot;mongo insertMany part 1&quot;},{&quot;title&quot;:&quot;mongo insertMany part 2&quot;},{&quot;title&quot;:&quot;mongo insertMany part 3&quot;}]){ &quot;acknowledged&quot; : true, &quot;insertedIds&quot; : [ ObjectId(&quot;5a1faab4fb4c8a34cc1a4d7e&quot;), ObjectId(&quot;5a1faab4fb4c8a34cc1a4d7f&quot;), ObjectId(&quot;5a1faab4fb4c8a34cc1a4d80&quot;) ]}&gt; db.blog.find(){ &quot;_id&quot; : ObjectId(&quot;5a1fa79efb4c8a34cc1a4d73&quot;), &quot;title&quot; : &quot;mongo bulk insert - Ordered Operations part 1&quot; }{ &quot;_id&quot; : ObjectId(&quot;5a1fa79efb4c8a34cc1a4d74&quot;), &quot;title&quot; : &quot;mongo bulk insert - Ordered Operations part 2&quot; }{ &quot;_id&quot; : ObjectId(&quot;5a1fa79efb4c8a34cc1a4d75&quot;), &quot;title&quot; : &quot;mongo bulk insert - Ordered Operations part 3&quot; }{ &quot;_id&quot; : ObjectId(&quot;5a1faab4fb4c8a34cc1a4d7e&quot;), &quot;title&quot; : &quot;mongo insertMany part 1&quot; }{ &quot;_id&quot; : ObjectId(&quot;5a1faab4fb4c8a34cc1a4d7f&quot;), &quot;title&quot; : &quot;mongo insertMany part 2&quot; }{ &quot;_id&quot; : ObjectId(&quot;5a1faab4fb4c8a34cc1a4d80&quot;), &quot;title&quot; : &quot;mongo insertMany part 3&quot; } Bulk InsertThe two basic usage are Unordered Operations和Ordered Operations. Ordered Operations, mongodb will stop and if one of the writing task list makes some errors when it is writing then other writing tasks wil not be continue.Operations which already done will not be rollback. 12345678910111213141516171819&gt; var bulk = db.blog.initializeOrderedBulkOp();&gt; bulk.insert({&quot;title&quot;:&quot;mongo bulk insert - Ordered Operations part 1&quot;});&gt; bulk.insert({&quot;title&quot;:&quot;mongo bulk insert - Ordered Operations part 2&quot;});&gt; bulk.insert({&quot;title&quot;:&quot;mongo bulk insert - Ordered Operations part 3&quot;});&gt; bulk.execute()BulkWriteResult({ &quot;writeErrors&quot; : [ ], &quot;writeConcernErrors&quot; : [ ], &quot;nInserted&quot; : 3, &quot;nUpserted&quot; : 0, &quot;nMatched&quot; : 0, &quot;nModified&quot; : 0, &quot;nRemoved&quot; : 0, &quot;&gt; db.blog.find()...{ &quot;_id&quot; : ObjectId(&quot;5a1fa79efb4c8a34cc1a4d73&quot;), &quot;title&quot; : &quot;mongo bulk insert - Ordered Operations part 1&quot; }{ &quot;_id&quot; : ObjectId(&quot;5a1fa79efb4c8a34cc1a4d74&quot;), &quot;title&quot; : &quot;mongo bulk insert - Ordered Operations part 2&quot; }{ &quot;_id&quot; : ObjectId(&quot;5a1fa79efb4c8a34cc1a4d75&quot;), &quot;title&quot; : &quot;mongo bulk insert - Ordered Operations part 3&quot; } Unordered Operations, it will not be stop if one of the writing task list makes errors then other task will continue their work.So this usage is much faster 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950&gt; var bulk = db.blog.initializeUnorderedBulkOp();&gt; bulk.insert({&quot;title&quot;:&quot;mongo bulk insert - Unordered Operations part 1&quot;});&gt; bulk.insert({&quot;title&quot;:&quot;mongo bulk insert - Unordered Operations part 2&quot;});&gt; bulk.insert({&quot;title&quot;:&quot;mongo bulk insert - Unordered Operations part 3&quot;});&gt; bulk.execute();BulkWriteResult({ &quot;writeErrors&quot; : [ ], &quot;writeConcernErrors&quot; : [ ], &quot;nInserted&quot; : 3, &quot;nUpserted&quot; : 0, &quot;nMatched&quot; : 0, &quot;nModified&quot; : 0, &quot;nRemoved&quot; : 0, &quot;upserted&quot; : [ ]})&gt; var bulk = db.blog.initializeUnorderedBulkOp();&gt; bulk.insert({&quot;title&quot;:&quot;mongo bulk insert - Unordered Operations part 4&quot;});&gt; bulk.insert({&quot;_id&quot; : ObjectId(&quot;5a1fa81efb4c8a34cc1a4d79&quot;),&quot;title&quot;:&quot;mongo bulk insert - Unordered Operations part 5&quot;});&gt; bulk.insert({&quot;title&quot;:&quot;mongo bulk insert - Unordered Operations part 6&quot;});&gt; bulk.execute();2017-03-22T14:43:51.928+0800 E QUERY [thread1] BulkWriteError: write error at item 1 in bulk operation :BulkWriteError({ &quot;writeErrors&quot; : [ { &quot;index&quot; : 1, &quot;code&quot; : 11000, &quot;errmsg&quot; : &quot;E11000 duplicate key error collection: test.blog index: _id_ dup key: { : ObjectId(&apos;5a1fa81efb4c8a34cc1a4d79&apos;) }&quot;, &quot;op&quot; : { &quot;_id&quot; : ObjectId(&quot;5a1fa81efb4c8a34cc1a4d79&quot;), &quot;title&quot; : &quot;mongo bulk insert - Unordered Operations part 5&quot; } } ], &quot;writeConcernErrors&quot; : [ ], &quot;nInserted&quot; : 2, &quot;nUpserted&quot; : 0, &quot;nMatched&quot; : 0, &quot;nModified&quot; : 0, &quot;nRemoved&quot; : 0, &quot;upserted&quot; : [ ]})&gt; db.blog.find()...{ &quot;_id&quot; : ObjectId(&quot;5a1fa81efb4c8a34cc1a4d79&quot;), &quot;title&quot; : &quot;mongo bulk insert - Unordered Operations part 1&quot; }{ &quot;_id&quot; : ObjectId(&quot;5a1fa81efb4c8a34cc1a4d7a&quot;), &quot;title&quot; : &quot;mongo bulk insert - Unordered Operations part 2&quot; }{ &quot;_id&quot; : ObjectId(&quot;5a1fa81efb4c8a34cc1a4d7b&quot;), &quot;title&quot; : &quot;mongo bulk insert - Unordered Operations part 3&quot; }{ &quot;_id&quot; : ObjectId(&quot;5a1fa8a7fb4c8a34cc1a4d7c&quot;), &quot;title&quot; : &quot;mongo bulk insert - Unordered Operations part 4&quot; }{ &quot;_id&quot; : ObjectId(&quot;5a1fa8a7fb4c8a34cc1a4d7d&quot;), &quot;title&quot; : &quot;mongo bulk insert - Unordered Operations part 6&quot; } The max document size must less than 16MB. Update12345678910111213&gt; db.blog.update({&quot;_id&quot; : ObjectId(&quot;5a202c00fb4c8a34cc1a4d81&quot;)},{&quot;$set&quot;:{&quot;subTitle&quot;:&quot;mongo update&quot;,&quot;tags&quot;:[&quot;mongo&quot;,&quot;mongo3.2&quot;,&quot;mongo update&quot;]}})WriteResult({ &quot;nMatched&quot; : 1, &quot;nUpserted&quot; : 0, &quot;nModified&quot; : 1 })&gt; db.blog.findOne({&quot;_id&quot; : ObjectId(&quot;5a202c00fb4c8a34cc1a4d81&quot;)}){ &quot;_id&quot; : ObjectId(&quot;5a202c00fb4c8a34cc1a4d81&quot;), &quot;title&quot; : &quot;mongo insert&quot;, &quot;subTitle&quot; : &quot;mongo update&quot;, &quot;tags&quot; : [ &quot;mongo&quot;, &quot;mongo3.2&quot;, &quot;mongo update&quot; ]} Append Update12345678910111213141516171819202122232425&gt; db.blog.update({&quot;_id&quot; : ObjectId(&quot;5a202c00fb4c8a34cc1a4d81&quot;)},{&quot;$push&quot;:{&quot;comments&quot;:[{&quot;name&quot;:&quot;liang&quot;,&quot;content&quot;:&quot;wow~ lucky!&quot;},{&quot;name&quot;:&quot;Jack&quot;,&quot;content&quot;:&quot;hihi~~&quot;}]}})WriteResult({ &quot;nMatched&quot; : 1, &quot;nUpserted&quot; : 0, &quot;nModified&quot; : 1 })&gt; db.blog.findOne({&quot;_id&quot; : ObjectId(&quot;5a202c00fb4c8a34cc1a4d81&quot;)}){ &quot;_id&quot; : ObjectId(&quot;5a202c00fb4c8a34cc1a4d81&quot;), &quot;title&quot; : &quot;mongo insert&quot;, &quot;subTitle&quot; : &quot;mongo update&quot;, &quot;tags&quot; : [ &quot;mongo&quot;, &quot;mongo3.2&quot;, &quot;mongo update&quot; ], &quot;comments&quot; : [ [ { &quot;name&quot; : &quot;liang&quot;, &quot;content&quot; : &quot;wow~ lucky!&quot; }, { &quot;name&quot; : &quot;Jack&quot;, &quot;content&quot; : &quot;hihi~~&quot; } ] ]} Update record does not exist12&gt; db.blog.update({&quot;_id&quot; : &quot;notexistid&quot;},{&quot;$set&quot;:{&quot;content&quot;:&quot;not exist record&quot;}},true)WriteResult({ &quot;nMatched&quot; : 0, &quot;nUpserted&quot; : 0, &quot;nModified&quot; : 0 }) Upsert12345&gt; db.blog.update({&quot;_id&quot; : &quot;notexistid&quot;},{&quot;$set&quot;:{&quot;content&quot;:&quot;not exist record&quot;}},true)WriteResult({ &quot;nMatched&quot; : 0, &quot;nUpserted&quot; : 1, &quot;nModified&quot; : 0, &quot;_id&quot; : &quot;notexistid&quot; })&gt; db.blog.find()...{ &quot;_id&quot; : &quot;notexistid&quot;, &quot;content&quot; : &quot;not exist record&quot; } The third parameter in update method indicates that if the record does not exist, the operation will become an insert operation Multi Update12345678910111213141516&gt; db.blog.update({&quot;title&quot;:{&quot;$ne&quot;:&quot;&quot;}},{&quot;$set&quot;:{&quot;author&quot;:&quot;liang&quot;}},false,true)WriteResult({ &quot;nMatched&quot; : 13, &quot;nUpserted&quot; : 0, &quot;nModified&quot; : 13 })&gt; db.blog.find(){ &quot;_id&quot; : ObjectId(&quot;5a1fa79efb4c8a34cc1a4d73&quot;), &quot;title&quot; : &quot;mongo bulk insert - Ordered Operations part 1&quot;, &quot;author&quot; : &quot;liang&quot; }{ &quot;_id&quot; : ObjectId(&quot;5a1fa79efb4c8a34cc1a4d74&quot;), &quot;title&quot; : &quot;mongo bulk insert - Ordered Operations part 2&quot;, &quot;author&quot; : &quot;liang&quot; }{ &quot;_id&quot; : ObjectId(&quot;5a1fa79efb4c8a34cc1a4d75&quot;), &quot;title&quot; : &quot;mongo bulk insert - Ordered Operations part 3&quot;, &quot;author&quot; : &quot;liang&quot; }{ &quot;_id&quot; : ObjectId(&quot;5a1fa81efb4c8a34cc1a4d79&quot;), &quot;title&quot; : &quot;mongo bulk insert - Unordered Operations part 1&quot;, &quot;author&quot; : &quot;liang&quot; }{ &quot;_id&quot; : ObjectId(&quot;5a1fa81efb4c8a34cc1a4d7a&quot;), &quot;title&quot; : &quot;mongo bulk insert - Unordered Operations part 2&quot;, &quot;author&quot; : &quot;liang&quot; }{ &quot;_id&quot; : ObjectId(&quot;5a1fa81efb4c8a34cc1a4d7b&quot;), &quot;title&quot; : &quot;mongo bulk insert - Unordered Operations part 3&quot;, &quot;author&quot; : &quot;liang&quot; }{ &quot;_id&quot; : ObjectId(&quot;5a1fa8a7fb4c8a34cc1a4d7c&quot;), &quot;title&quot; : &quot;mongo bulk insert - Unordered Operations part 4&quot;, &quot;author&quot; : &quot;liang&quot; }{ &quot;_id&quot; : ObjectId(&quot;5a1fa8a7fb4c8a34cc1a4d7d&quot;), &quot;title&quot; : &quot;mongo bulk insert - Unordered Operations part 6&quot;, &quot;author&quot; : &quot;liang&quot; }{ &quot;_id&quot; : ObjectId(&quot;5a1faab4fb4c8a34cc1a4d7e&quot;), &quot;title&quot; : &quot;mongo insertMany part 1&quot;, &quot;author&quot; : &quot;liang&quot; }{ &quot;_id&quot; : ObjectId(&quot;5a1faab4fb4c8a34cc1a4d7f&quot;), &quot;title&quot; : &quot;mongo insertMany part 2&quot;, &quot;author&quot; : &quot;liang&quot; }{ &quot;_id&quot; : ObjectId(&quot;5a1faab4fb4c8a34cc1a4d80&quot;), &quot;title&quot; : &quot;mongo insertMany part 3&quot;, &quot;author&quot; : &quot;liang&quot; }{ &quot;_id&quot; : ObjectId(&quot;5a202c00fb4c8a34cc1a4d81&quot;), &quot;title&quot; : &quot;mongo insert&quot;, &quot;subTitle&quot; : &quot;mongo update&quot;, &quot;tags&quot; : [ &quot;mongo&quot;, &quot;mongo3.2&quot;, &quot;mongo update&quot; ], &quot;comments&quot; : [ [ { &quot;name&quot; : &quot;liang&quot;, &quot;content&quot; : &quot;wow~ lucky!&quot; }, { &quot;name&quot; : &quot;Jack&quot;, &quot;content&quot; : &quot;hihi~~&quot; } ] ], &quot;author&quot; : &quot;liang&quot; }{ &quot;_id&quot; : &quot;notexistid&quot;, &quot;content&quot; : &quot;not exist record&quot;, &quot;author&quot; : &quot;liang&quot; } The fourth parameter in update method indicates that all matching documents will be updated findAndModifyNote that the findAndModify operation is atomic ! 1234567891011121314151617181920212223242526272829303132333435&gt; db.runCommand({&quot;findAndModify&quot;:&quot;blog&quot;... ,&quot;query&quot;:{&quot;_id&quot; : ObjectId(&quot;5a202c00fb4c8a34cc1a4d81&quot;)},... &quot;sort&quot;:{&quot;_id&quot;:&quot;-1&quot;},... &quot;update&quot;:{&quot;$set&quot;:{&quot;price&quot;:100}}... }){ &quot;lastErrorObject&quot; : { &quot;updatedExisting&quot; : true, &quot;n&quot; : 1 }, &quot;value&quot; : { &quot;_id&quot; : ObjectId(&quot;5a202c00fb4c8a34cc1a4d81&quot;), &quot;title&quot; : &quot;mongo insert&quot;, &quot;subTitle&quot; : &quot;mongo update&quot;, &quot;tags&quot; : [ &quot;mongo&quot;, &quot;mongo3.2&quot;, &quot;mongo update&quot; ], &quot;comments&quot; : [ [ { &quot;name&quot; : &quot;liang&quot;, &quot;content&quot; : &quot;wow~ lucky!&quot; }, { &quot;name&quot; : &quot;Jack&quot;, &quot;content&quot; : &quot;hihi~~&quot; } ] ], &quot;author&quot; : &quot;liang&quot; }, &quot;ok&quot; : 1} FindInsert a record.12&gt; db.blog.insert({&quot;title&quot;:&quot;mongo find&quot;,&quot;author&quot;:&quot;liang&quot;,&quot;language&quot;:&quot;english&quot;,&quot;price&quot;:{&quot;max&quot;:99,&quot;min&quot;:60}})WriteResult({ &quot;nInserted&quot; : 1 }) 12&gt; db.blog.find({&quot;price.max&quot;:{&quot;$lte&quot;:99,&quot;$gte&quot;:60},&quot;author&quot;:&quot;liang&quot;},{&quot;title&quot;:1,&quot;author&quot;:1,&quot;_id&quot;:0}){ &quot;title&quot; : &quot;mongo find&quot;, &quot;author&quot; : &quot;liang&quot; } The first param is query condition, the second param is query result(it will not return if this value is zero) Cursor123456789&gt; for(i=0;i&lt;100;i++){... db.blog.insert({&quot;author&quot;:&quot;liang&quot;,&quot;title&quot;:&quot;cursor insert &quot;+i});... }WriteResult({ &quot;nInserted&quot; : 1 })&gt; var cursor = db.blog.find();&gt; while(cursor.hasNext()){... obj = cursor.next();... } If the number of query result is huge, cursor will meet a repeat documentIf document becomes bigger when data from database increased after business process, this document ‘s memory address will be reassign.When the cursor is moved to right, it will re-meet the same document with a new address again","link":"/2017/03/22/Operate-on-Mongodb-3-2/"},{"title":"Mysql Table Horizontal Split By Mycat","text":"Server environment12345678$ cat /etc/issueUbuntu 14.04.5 LTS$ mysqld -Vmysqld Ver 5.5.58-0ubuntu0.14.04.1 for debian-linux-gnu on x86_64 ((Ubuntu))$ java -versionjava version &quot;1.7.0_151&quot; Install Java7+1$ sudo apt-get install -y openjdk-7-jre Install MycatDownload the zip file from github 1234567891011$ wget http://dl.mycat.io/1.6-RELEASE/Mycat-server-1.6-RELEASE-20161028204710-linux.tar.gz$ tar zxvf Mycat-server-1.6-RELEASE-20161028204710-linux.tar.gz$ cd mycat/$ tree -L 1 ..├── bin├── catlet├── conf├── lib├── logs└── version.txt Set up Mysql account1$ vim conf/server.xml 123456789101112... &lt;user name=\"user1\"&gt; &lt;property name=\"password\"&gt;password1&lt;/property&gt; &lt;property name=\"schemas\"&gt;virtual_database&lt;/property&gt; &lt;/user&gt; &lt;user name=\"user2\"&gt; &lt;property name=\"password\"&gt;password2&lt;/property&gt; &lt;property name=\"schemas\"&gt;virtual_database&lt;/property&gt; &lt;property name=\"readOnly\"&gt;true&lt;/property&gt; &lt;/user&gt;... There are 2 users, user1 and user2, passwords are password1 and password2, user2 users are read-only Set up Mysql nodes1$ vim conf/schema.xml 1234567891011121314151617181920212223242526272829303132333435363738394041424344&lt;?xml version=\"1.0\"?&gt;&lt;!DOCTYPE mycat:schema SYSTEM \"schema.dtd\"&gt;&lt;mycat:schema xmlns:mycat=\"http://io.mycat/\"&gt; &lt;schema name=\"virtual_database\" checkSQLschema=\"false\" sqlMaxLimit=\"100\"&gt; &lt;!-- auto sharding by id (long) --&gt; &lt;table name=\"travelrecord\" dataNode=\"dn1,dn2,dn3\" rule=\"auto-sharding-long\" /&gt; &lt;!-- global table is auto cloned to all defined data nodes ,so can join with any table whose sharding node is in the same data node --&gt; &lt;table name=\"company\" primaryKey=\"ID\" type=\"global\" dataNode=\"dn1,dn2,dn3\" /&gt; &lt;table name=\"goods\" primaryKey=\"ID\" type=\"global\" dataNode=\"dn1,dn2\" /&gt; &lt;!-- random sharding using mod sharind rule --&gt; &lt;table name=\"hotnews\" primaryKey=\"ID\" autoIncrement=\"true\" dataNode=\"dn1,dn2,dn3\" rule=\"mod-long\" /&gt; &lt;table name=\"employee\" primaryKey=\"ID\" dataNode=\"dn1,dn2\" rule=\"sharding-by-intfile\" /&gt; &lt;table name=\"customer\" primaryKey=\"ID\" dataNode=\"dn1,dn2\" rule=\"sharding-by-intfile\"&gt; &lt;childTable name=\"orders\" primaryKey=\"ID\" joinKey=\"customer_id\" parentKey=\"id\"&gt; &lt;childTable name=\"order_items\" joinKey=\"order_id\" parentKey=\"id\" /&gt; &lt;/childTable&gt; &lt;childTable name=\"customer_addr\" primaryKey=\"ID\" joinKey=\"customer_id\" parentKey=\"id\" /&gt; &lt;/table&gt; &lt;/schema&gt; &lt;dataNode name=\"dn1\" dataHost=\"devdb\" database=\"app1\" /&gt; &lt;dataNode name=\"dn2\" dataHost=\"devdb\" database=\"app2\" /&gt; &lt;dataNode name=\"dn3\" dataHost=\"devdb\" database=\"app3\" /&gt; &lt;dataHost name=\"devdb\" maxCon=\"1000\" minCon=\"10\" balance=\"0\" writeType=\"0\" dbType=\"mysql\" dbDriver=\"native\" switchType=\"1\" slaveThreshold=\"100\"&gt; &lt;heartbeat&gt;select user()&lt;/heartbeat&gt; &lt;!-- can have multi write hosts --&gt; &lt;writeHost host=\"hostM1\" url=\"localhost:3306\" user=\"user1\" password=\"password1\"&gt; &lt;!-- can have multi read hosts --&gt; &lt;!--&lt;readHost host=\"hostS2\" url=\"127.0.0.1:3306\" user=\"user1\" password=\"password1\" /&gt;--&gt; &lt;/writeHost&gt; &lt;writeHost host=\"hostS1\" url=\"localhost:3306\" user=\"user2\" password=\"password2\" /&gt; &lt;/dataHost&gt;&lt;/mycat:schema&gt; There are three main parts:The first is schema block, which describe the tables in schema of virtual database. Which nodes or algorithms are using on these table. For example It means that travelrecord table is distributed in dn1,dn2,dn3 these three nodes and used auto-sharding-long algorithm. The second is the data nodes. It tells Mycat Which nodes are included in database and mapping the actual data server and database.I am going to configure three nodes dn1, dn2, dn3 which are in local server and databases ‘s names are app1, app2, app3. The third is data host. This part is the actual server configuration.I will config two Mysql database, hostM1 and hostS1, address is “localhost:3306” and username are “user1”,”user1” and password are “password1”,”password2”. Then specify the heart beat method “select user()”. Change the log level from info to debug1$ vim conf/log4j2.xml 123456&lt;asyncRoot level=\"debug\" includeLocation=\"true\"&gt; &lt;AppenderRef ref=\"Console\" /&gt; &lt;AppenderRef ref=\"RollingFile\"/&gt;&lt;/asyncRoot&gt; Create three databaseIn this tutorial, just one database, so hostM1 and hostS1 are the same machine hostM1:123mysql&gt; create database app1;mysql&gt; create database app2;mysql&gt; create database app3; hostS1:123mysql&gt; create database app1;mysql&gt; create database app2;mysql&gt; create database app3; Create two database userhostM1:123mysql&gt; CREATE USER user1 IDENTIFIED BY &apos;password1&apos;;mysql&gt; GRANT ALL PRIVILEGES ON *.* TO &apos;user1&apos;@&apos;%&apos; ;mysql&gt; FLUSH PRIVILEGES; hostS1:123mysql&gt; CREATE USER user2 IDENTIFIED BY &apos;password2&apos;;mysql&gt; GRANT ALL PRIVILEGES ON *.* TO &apos;user2&apos;@&apos;%&apos; ;mysql&gt; FLUSH PRIVILEGES; Start Mycat12$ ./bin/mycat startStarting Mycat-server... Track the log for connecting info1$ tail -f logs/mycat.log Login Mycat Mycat default port is 8066 1$ mysql -uuser1 -ppassword1 -h127.0.0.1 -P8066 -Dvirtual_database Check databases1234567mysql&gt; show databases;+------------------+| DATABASE |+------------------+| virtual_database |+------------------+1 row in set (0.00 sec) Create a table in schema rule123456789101112mysql&gt; explain create table travelrecord (id bigint not null primary key,user_id varchar(100),traveldate DATE, fee decimal,days int);+-----------+-----------------------------------------------------------------------------------------------------------------------+| DATA_NODE | SQL |+-----------+-----------------------------------------------------------------------------------------------------------------------+| dn1 | create table travelrecord (id bigint not null primary key,user_id varchar(100),traveldate DATE, fee decimal,days int) || dn2 | create table travelrecord (id bigint not null primary key,user_id varchar(100),traveldate DATE, fee decimal,days int) || dn3 | create table travelrecord (id bigint not null primary key,user_id varchar(100),traveldate DATE, fee decimal,days int) |+-----------+-----------------------------------------------------------------------------------------------------------------------+3 rows in set (0.00 sec)mysql&gt; create table travelrecord (id bigint not null primary key,user_id varchar(100),traveldate DATE, fee decimal,days int);Query OK, 0 rows affected (0.02 sec) Ok, Mycat is working. Table will be create in three different node. Create a table that not include in schema rule12mysql&gt; create table my_table_not_in_rule (id bigint not null primary key, name varchar(100));ERROR 1064 (HY000): op table not in schema----MY_TABLE_NOT_IN_RULE Add three records which id are 1000000, 5000001, 100000011234567891011121314151617181920212223mysql&gt; explain insert into travelrecord (id,user_id,traveldate,fee,days) values(1000000,&apos;liang&apos;,&apos;2017-08-01&apos;,96.05,3);+-----------+-------------------------------------------------------------------------------------------------------+| DATA_NODE | SQL |+-----------+-------------------------------------------------------------------------------------------------------+| dn1 | insert into travelrecord (id,user_id,traveldate,fee,days) values(1000000,&apos;liang&apos;,&apos;2017-08-01&apos;,96.05,3) |+-----------+-------------------------------------------------------------------------------------------------------+1 row in set (0.08 sec)mysql&gt; explain insert into travelrecord (id,user_id,traveldate,fee,days) values(5000001,&apos;liang&apos;,&apos;2017-08-01&apos;,96.05,3);+-----------+-------------------------------------------------------------------------------------------------------+| DATA_NODE | SQL |+-----------+-------------------------------------------------------------------------------------------------------+| dn2 | insert into travelrecord (id,user_id,traveldate,fee,days) values(5000001,&apos;liang&apos;,&apos;2017-08-01&apos;,96.05,3) |+-----------+-------------------------------------------------------------------------------------------------------+1 row in set (0.01 sec)mysql&gt; explain insert into travelrecord (id,user_id,traveldate,fee,days) values(10000001,&apos;liang&apos;,&apos;2017-08-01&apos;,96.05,3);+-----------+---------------------------------------------------------------------------------------------------------+| DATA_NODE | SQL |+-----------+---------------------------------------------------------------------------------------------------------+| dn3 | insert into travelrecord (id,user_id,traveldate,fee,days) values(10000001,&apos;liang&apos;,&apos;2017-08-01&apos;,96.05,3) |+-----------+---------------------------------------------------------------------------------------------------------+1 row in set (0.00 sec) Auto sharding into different node by id 12345678mysql&gt; insert into travelrecord (id,user_id,traveldate,fee,days) values(1000000,&apos;liang&apos;,&apos;2017-08-01&apos;,96.05,3);Query OK, 1 row affected, 1 warning (0.00 sec)mysql&gt; insert into travelrecord (id,user_id,traveldate,fee,days) values(5000001,&apos;liang&apos;,&apos;2017-08-01&apos;,96.05,3);Query OK, 1 row affected, 1 warning (0.00 sec)mysql&gt; insert into travelrecord (id,user_id,traveldate,fee,days) values(10000001,&apos;liang&apos;,&apos;2017-08-01&apos;,96.05,3);Query OK, 1 row affected, 1 warning (0.00 sec) Find the data123456789mysql&gt; explain select * from travelrecord where id &gt;= 2000000;+-----------+----------------------------------------------------------+| DATA_NODE | SQL |+-----------+----------------------------------------------------------+| dn1 | SELECT * FROM travelrecord WHERE id &gt;= 2000000 LIMIT 100 || dn2 | SELECT * FROM travelrecord WHERE id &gt;= 2000000 LIMIT 100 || dn3 | SELECT * FROM travelrecord WHERE id &gt;= 2000000 LIMIT 100 |+-----------+----------------------------------------------------------+3 rows in set (0.00 sec) It’s easy to know that it is querying on each database and limit 100 default Add New Table RuleMyCAT ‘s schema.xml can be loaded dynamically 1$ vim conf/schema.xml 1234&lt;schema name=\"virtual_database\" checkSQLschema=\"false\" sqlMaxLimit=\"100\"&gt; ... &lt;table name=\"t_order\" primaryKey=\"id\" autoIncrement=\"true\" dataNode=\"dn1,dn2,dn3\" rule=\"sharding-by-month\" /&gt;&lt;/schema&gt; Add new table t_order and use “sharding-by-month” rule. It is Mycat configured rule for the field “create_time” to splitting monthly which in file “conf/rule.xml”. Use Mycat manage port for real-time refresh rules Mycat admin port default is 9066 1$ mysql -uuser1 -ppassword1 -h127.0.0.1 -P9066 -Dvirtual_database Refresh Mycat config in admin123mysql&gt; reload @@config;Query OK, 1 row affected (0.04 sec)Reload config success Access Mycat, you can see a new t_order table which is just a virtual table for mapping management 123456789101112131415161718$ mysql -uuser1 -ppassword1 -h127.0.0.1 -P8066 -Dvirtual_database...mysql&gt; show tables;+----------------------------+| Tables in virtual_database |+----------------------------+| company || customer || customer_addr || employee || goods || hotnews || orders || order_items || travelrecord || t_order |+----------------------------+10 rows in set (0.00 sec) Create table t_order 12345CREATE TABLE `t_order` ( `id` varchar(32) NOT NULL, `order_id` varchar(64) DEFAULT NULL, `create_time` datetime) ENGINE=InnoDB DEFAULT CHARSET=utf8; 12345678mysql&gt; insert into t_order(id,order_id,create_time) values(&apos;11&apos;,&apos;order_1001&apos;,&apos;2017-01-01 10:00:00&apos;);ERROR 1064 (HY000): Can&apos;t find a valid data node for specified node index :T_ORDER -&gt; CREATE_TIME -&gt; 2017-01-01 10:00:00 -&gt; Index : 24mysql&gt; insert into t_order(id,order_id,create_time) values(&apos;12&apos;,&apos;order_1002&apos;,&apos;2017-02-01 10:00:00&apos;);ERROR 1064 (HY000): Can&apos;t find a valid data node for specified node index :T_ORDER -&gt; CREATE_TIME -&gt; 2017-02-01 10:00:00 -&gt; Index : 25mysql&gt; insert into t_order(id,order_id,create_time) values(&apos;13&apos;,&apos;order_1003&apos;,&apos;2017-03-01 00:00:00&apos;);ERROR 1064 (HY000): Can&apos;t find a valid data node for specified node index :T_ORDER -&gt; CREATE_TIME -&gt; 2017-03-01 00:00:00 -&gt; Index : 26 Because the rule “sharding-by-month” in file conf/rule.xml, the start time is defined as 2015-01-1, need to be modified to 2017-01-01 12345678910111213&lt;tableRule name=\"sharding-by-month\"&gt; &lt;rule&gt; &lt;columns&gt;create_time&lt;/columns&gt; &lt;algorithm&gt;partbymonth&lt;/algorithm&gt; &lt;/rule&gt;&lt;/tableRule&gt;...&lt;function name=\"partbymonth\" class=\"io.mycat.route.function.PartitionByMonth\"&gt; &lt;property name=\"dateFormat\"&gt;yyyy-MM-dd&lt;/property&gt; &lt;!-- &lt;property name=\"sBeginDate\"&gt;2015-01-01&lt;/property&gt; --&gt; &lt;property name=\"sBeginDate\"&gt;2017-01-01&lt;/property&gt;&lt;/function&gt; Increase the number of nodes to 121$ vim conf/schema.xml 123456789101112131415161718192021&lt;schema name=\"virtual_database\" checkSQLschema=\"false\" sqlMaxLimit=\"100\"&gt; ... &lt;table name=\"t_order\" primaryKey=\"id\" autoIncrement=\"true\" dataNode=\"dn$1-12\" rule=\"sharding-by-month\" /&gt;&lt;/schema&gt;...&lt;dataNode name=\"dn1\" dataHost=\"devdb\" database=\"app1\" /&gt;&lt;dataNode name=\"dn2\" dataHost=\"devdb\" database=\"app2\" /&gt;&lt;dataNode name=\"dn3\" dataHost=\"devdb\" database=\"app3\" /&gt;&lt;dataNode name=\"dn4\" dataHost=\"devdb\" database=\"app1\" /&gt; &lt;!-- new --&gt;&lt;dataNode name=\"dn5\" dataHost=\"devdb\" database=\"app2\" /&gt; &lt;!-- new --&gt;&lt;dataNode name=\"dn6\" dataHost=\"devdb\" database=\"app3\" /&gt; &lt;!-- new --&gt;&lt;dataNode name=\"dn7\" dataHost=\"devdb\" database=\"app1\" /&gt; &lt;!-- new --&gt;&lt;dataNode name=\"dn8\" dataHost=\"devdb\" database=\"app2\" /&gt; &lt;!-- new --&gt;&lt;dataNode name=\"dn9\" dataHost=\"devdb\" database=\"app3\" /&gt; &lt;!-- new --&gt;&lt;dataNode name=\"dn10\" dataHost=\"devdb\" database=\"app1\" /&gt; &lt;!-- new --&gt;&lt;dataNode name=\"dn11\" dataHost=\"devdb\" database=\"app2\" /&gt; &lt;!-- new --&gt;&lt;dataNode name=\"dn12\" dataHost=\"devdb\" database=\"app3\" /&gt; &lt;!-- new --&gt;... Refresh Mycat config123mysql&gt; reload @@config_all;Query OK, 1 row affected (0.03 sec)Reload config success Add New records123456mysql&gt; insert into t_order(id,order_id,create_time) values(&apos;11&apos;,&apos;order_1001&apos;,&apos;2017-01-01 10:00:00&apos;);mysql&gt; insert into t_order(id,order_id,create_time) values(&apos;12&apos;,&apos;order_1002&apos;,&apos;2017-02-01 10:00:00&apos;);mysql&gt; insert into t_order(id,order_id,create_time) values(&apos;13&apos;,&apos;order_1003&apos;,&apos;2017-03-01 00:00:00&apos;);mysql&gt; insert into t_order(id,order_id,create_time) values(&apos;14&apos;,&apos;order_1003&apos;,&apos;2017-04-01 00:00:00&apos;);mysql&gt; insert into t_order(id,order_id,create_time) values(&apos;15&apos;,&apos;order_1003&apos;,&apos;2017-05-01 00:00:00&apos;);mysql&gt; insert into t_order(id,order_id,create_time) values(&apos;16&apos;,&apos;order_1003&apos;,&apos;2017-06-01 00:00:00&apos;); Mycat monthly division can only be scheduled within a year!!!!!I do not know if there is any other way Meet some errors Unknown database 12$ mysql -uuser1 -ppassword1 -h127.0.0.1 -P8066 -Dvirtual_databaseERROR 1049 (42000): Unknown database &apos;virtual_database&apos; The solution is to specify the ip address… 1234567$ mysql -uuser1 -ppassword1 -h127.0.0.1 -P8066 -Dvirtual_databaseWelcome to the MySQL monitor. Commands end with ; or \\g.Server version: 5.6.29-mycat-1.6-RELEASE-20161028204710 MyCat Server (OpenCloundDB)Type &apos;help;&apos; or &apos;\\h&apos; for help. Type &apos;\\c&apos; to clear the current input statement.mysql&gt; After dynamically loading scheme, NullPointerException occurs 1ERROR 3009 (HY000): java.lang.NullPointerException adjusting nodes, you should use config_all 123mysql&gt; reload @@config_all;Query OK, 1 row affected (0.03 sec)Reload config success Can’t find table define in schema 1java.sql.SQLNonTransientException: can&apos;t find table define in schema &quot;USERS&quot; schema:virtual_database Many sql syntax errors will be reported this error… I’m using this library catfan/Medoo then show this error please make sure the primaryKey’s config is not null in schemal.xml Make sure the schema.xml ‘s configuration is right: primaryKey=”ID” autoIncrement=”true” 1&lt;table name=&quot;users&quot; primaryKey=&quot;ID&quot; autoIncrement=&quot;true&quot; dataNode=&quot;dn1,dn2,dn3&quot; rule=&quot;auto-sharding-long&quot; /&gt; can’t find any valid datanode :USERS -&gt; ID -&gt; 947870356817973248 If sharding rule is auto-sharding-long, it will limit the id range. Datanode1: 1~5000001 Datanode2: 5000001~10000001 Datanode3: 10000001~15000001 When id is more than 15000001, it will show this error: [Err] 1064 - can’t find any valid datanode :TB_ITEM -&gt; ID -&gt; 15000002 At this point you need to add a new node.","link":"/2017/08/26/Mysql-Table-Horizontal-Split-By-Mycat/"}],"tags":[{"name":"golang","slug":"golang","link":"/tags/golang/"},{"name":"Glide","slug":"Glide","link":"/tags/Glide/"},{"name":"git","slug":"git","link":"/tags/git/"},{"name":"fork","slug":"fork","link":"/tags/fork/"},{"name":"PHP","slug":"PHP","link":"/tags/PHP/"},{"name":"Ubuntu","slug":"Ubuntu","link":"/tags/Ubuntu/"},{"name":"docker","slug":"docker","link":"/tags/docker/"},{"name":"iptable","slug":"iptable","link":"/tags/iptable/"},{"name":"nginx","slug":"nginx","link":"/tags/nginx/"},{"name":"proxy","slug":"proxy","link":"/tags/proxy/"},{"name":"Mysql","slug":"Mysql","link":"/tags/Mysql/"},{"name":"Slave","slug":"Slave","link":"/tags/Slave/"},{"name":"composer","slug":"composer","link":"/tags/composer/"},{"name":"domain","slug":"domain","link":"/tags/domain/"},{"name":"cookie","slug":"cookie","link":"/tags/cookie/"},{"name":"Apache","slug":"Apache","link":"/tags/Apache/"},{"name":"web","slug":"web","link":"/tags/web/"},{"name":"auth","slug":"auth","link":"/tags/auth/"},{"name":"Symfony","slug":"Symfony","link":"/tags/Symfony/"},{"name":"Mac","slug":"Mac","link":"/tags/Mac/"},{"name":"Iterm","slug":"Iterm","link":"/tags/Iterm/"},{"name":"tmux","slug":"tmux","link":"/tags/tmux/"},{"name":"Redis","slug":"Redis","link":"/tags/Redis/"},{"name":"Linux","slug":"Linux","link":"/tags/Linux/"},{"name":"redis-cluster","slug":"redis-cluster","link":"/tags/redis-cluster/"},{"name":"redis-sentinel","slug":"redis-sentinel","link":"/tags/redis-sentinel/"},{"name":"github","slug":"github","link":"/tags/github/"},{"name":"blog","slug":"blog","link":"/tags/blog/"},{"name":"Twig","slug":"Twig","link":"/tags/Twig/"},{"name":"Elasticsearch","slug":"Elasticsearch","link":"/tags/Elasticsearch/"},{"name":"Logstash","slug":"Logstash","link":"/tags/Logstash/"},{"name":"Kibana","slug":"Kibana","link":"/tags/Kibana/"},{"name":"Filebeat","slug":"Filebeat","link":"/tags/Filebeat/"},{"name":"brew","slug":"brew","link":"/tags/brew/"},{"name":"vagrant","slug":"vagrant","link":"/tags/vagrant/"},{"name":"homestead","slug":"homestead","link":"/tags/homestead/"},{"name":"laravel","slug":"laravel","link":"/tags/laravel/"},{"name":"dante","slug":"dante","link":"/tags/dante/"},{"name":"sock5","slug":"sock5","link":"/tags/sock5/"},{"name":"Nodejs","slug":"Nodejs","link":"/tags/Nodejs/"},{"name":"beanstalkd","slug":"beanstalkd","link":"/tags/beanstalkd/"},{"name":"JQuery","slug":"JQuery","link":"/tags/JQuery/"},{"name":"tips","slug":"tips","link":"/tags/tips/"},{"name":"supervisor","slug":"supervisor","link":"/tags/supervisor/"},{"name":"Daemon","slug":"Daemon","link":"/tags/Daemon/"},{"name":"Baidu","slug":"Baidu","link":"/tags/Baidu/"},{"name":"CloudDisk","slug":"CloudDisk","link":"/tags/CloudDisk/"},{"name":"gitlab","slug":"gitlab","link":"/tags/gitlab/"},{"name":"backup","slug":"backup","link":"/tags/backup/"},{"name":"log","slug":"log","link":"/tags/log/"},{"name":"Keepalived","slug":"Keepalived","link":"/tags/Keepalived/"},{"name":"file","slug":"file","link":"/tags/file/"},{"name":"download","slug":"download","link":"/tags/download/"},{"name":"NFS","slug":"NFS","link":"/tags/NFS/"},{"name":"SlowLog","slug":"SlowLog","link":"/tags/SlowLog/"},{"name":"Alipay","slug":"Alipay","link":"/tags/Alipay/"},{"name":"Virtualbox","slug":"Virtualbox","link":"/tags/Virtualbox/"},{"name":"usb","slug":"usb","link":"/tags/usb/"},{"name":"TCP","slug":"TCP","link":"/tags/TCP/"},{"name":"css","slug":"css","link":"/tags/css/"},{"name":"Partition","slug":"Partition","link":"/tags/Partition/"},{"name":"Canal","slug":"Canal","link":"/tags/Canal/"},{"name":"binlog","slug":"binlog","link":"/tags/binlog/"},{"name":"sync","slug":"sync","link":"/tags/sync/"},{"name":"Mongodb","slug":"Mongodb","link":"/tags/Mongodb/"},{"name":"Replica","slug":"Replica","link":"/tags/Replica/"},{"name":"Login","slug":"Login","link":"/tags/Login/"},{"name":"Security","slug":"Security","link":"/tags/Security/"},{"name":"Mycat","slug":"Mycat","link":"/tags/Mycat/"}],"categories":[]}